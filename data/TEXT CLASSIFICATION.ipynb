{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based by \"https://github.com/google/eng-edu/tree/master/ml/guides/text_classification\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=321):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_texts, train_labels), (test_texts, test_labels)) = load_imdb_sentiment_analysis_dataset(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes\n",
    "\n",
    "\n",
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "\n",
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution(labels):\n",
    "    \"\"\"Plots the class distribution.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    \"\"\"\n",
    "    num_classes = get_num_classes(labels)\n",
    "    count_map = Counter(labels)\n",
    "    counts = [count_map[i] for i in range(num_classes)]\n",
    "    idx = np.arange(num_classes)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Class distribution')\n",
    "    plt.xticks(idx, idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the training sample: 174.0\n",
      "Number of the test sample: 172.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of the training sample:\",get_num_words_per_sample(train_texts))\n",
    "print(\"Number of the test sample:\",get_num_words_per_sample(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e0407a400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e06212dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e040c7a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequency_distribution_of_ngrams(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train)\n",
    "    x_val = selector.transform(x_val)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    return x_train, x_val\n",
    "\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.67816091954023\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts)/get_num_words_per_sample(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val = ngram_vectorize(train_texts, train_labels, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0dc1759a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.spy(x_train, precision=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model\n",
    "\n",
    "\n",
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 21s - loss: 0.4715 - acc: 0.8538 - val_loss: 0.3228 - val_acc: 0.8874\n",
      "Epoch 2/1000\n",
      " - 19s - loss: 0.2330 - acc: 0.9192 - val_loss: 0.2515 - val_acc: 0.9021\n",
      "Epoch 3/1000\n",
      " - 19s - loss: 0.1688 - acc: 0.9412 - val_loss: 0.2325 - val_acc: 0.9063\n",
      "Epoch 4/1000\n",
      " - 20s - loss: 0.1357 - acc: 0.9537 - val_loss: 0.2310 - val_acc: 0.9052\n",
      "Epoch 5/1000\n",
      " - 19s - loss: 0.1108 - acc: 0.9638 - val_loss: 0.2326 - val_acc: 0.9047\n",
      "Epoch 6/1000\n",
      " - 20s - loss: 0.0929 - acc: 0.9698 - val_loss: 0.2395 - val_acc: 0.9018\n",
      "Validation accuracy: 0.90176, loss: 0.23947998819351196\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val = ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('imdb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the IMDb movie reviews dataset to demonstrate training n-gram model\n",
    "    data = load_imdb_sentiment_analysis_dataset(FLAGS.data_dir)\n",
    "    train_ngram_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def train_sequence_model(data,\n",
    "                         learning_rate=1e-3,\n",
    "                         epochs=1000,\n",
    "                         batch_size=128,\n",
    "                         blocks=2,\n",
    "                         filters=64,\n",
    "                         dropout_rate=0.2,\n",
    "                         embedding_dim=200,\n",
    "                         kernel_size=3,\n",
    "                         pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('rotten_tomatoes_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the Rotten tomatoes movie reviews dataset to demonstrate\n",
    "    # training sequence model.\n",
    "    data = load_rotten_tomatoes_sentiment_analysis_dataset(\n",
    "            FLAGS.data_dir)\n",
    "    train_sequence_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train fine tuned sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def _get_embedding_matrix(word_index, embedding_data_dir, embedding_dim):\n",
    "    \"\"\"Gets embedding matrix from the embedding index data.\n",
    "    # Arguments\n",
    "        word_index: dict, word to index map that was generated from the data.\n",
    "        embedding_data_dir: string, path to the pre-training embeddings.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "    # Returns\n",
    "        dict, word vectors for words in word_index from pre-trained embedding.\n",
    "    # References:\n",
    "        https://nlp.stanford.edu/projects/glove/\n",
    "        Download and uncompress archive from:\n",
    "        http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "\n",
    "    # We are using 200d GloVe embeddings.\n",
    "    fname = os.path.join(embedding_data_dir, 'glove.6B.200d.txt')\n",
    "    with open(fname) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def train_fine_tuned_sequence_model(data,\n",
    "                                    embedding_data_dir,\n",
    "                                    learning_rate=1e-3,\n",
    "                                    epochs=1000,\n",
    "                                    batch_size=128,\n",
    "                                    blocks=2,\n",
    "                                    filters=64,\n",
    "                                    dropout_rate=0.2,\n",
    "                                    embedding_dim=200,\n",
    "                                    kernel_size=3,\n",
    "                                    pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        embedding_data_dir: string, path to the pre-training embeddings.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    embedding_matrix = _get_embedding_matrix(\n",
    "        word_index, embedding_data_dir, embedding_dim)\n",
    "\n",
    "    # Create model instance. First time we will train rest of network while\n",
    "    # keeping embedding layer weights frozen. So, we set\n",
    "    # is_embedding_trainable as False.\n",
    "    model = build_model.sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features,\n",
    "                                     use_pretrained_embedding=True,\n",
    "                                     is_embedding_trainable=False,\n",
    "                                     embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    model.fit(x_train,\n",
    "              train_labels,\n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_val, val_labels),\n",
    "              verbose=2,  # Logs once per epoch.\n",
    "              batch_size=batch_size)\n",
    "\n",
    "    # Save the model.\n",
    "    model.save_weights('sequence_model_with_pre_trained_embedding.h5')\n",
    "\n",
    "    # Create another model instance. This time we will unfreeze the embedding\n",
    "    # layer and let it fine-tune to the given dataset.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features,\n",
    "                                     use_pretrained_embedding=True,\n",
    "                                     is_embedding_trainable=True,\n",
    "                                     embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Load the weights that we had saved into this new model.\n",
    "    model.load_weights('sequence_model_with_pre_trained_embedding.h5')\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(x_train,\n",
    "                        train_labels,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(x_val, val_labels),\n",
    "                        verbose=2,  # Logs once per epoch.\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('tweet_weather_sepcnn_fine_tuned_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='./data',\n",
    "                        help='input data directory')\n",
    "    parser.add_argument('--embedding_data_dir', type=str, default='./data',\n",
    "                        help='embedding input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the tweet weather topic classification dataset to demonstrate\n",
    "    # training sequence model with fine-tuned pre-trained embedding.\n",
    "    data = load_tweet_weather_topic_classification_dataset(\n",
    "            FLAGS.data_dir)\n",
    "    train_fine_tuned_sequence_model(data, FLAGS.embedding_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch train sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def _data_generator(x, y, num_features, batch_size):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "    # Arguments\n",
    "        x: np.matrix, feature matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        num_features: int, number of features.\n",
    "        batch_size: int, number of samples per batch.\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            x_batch = x[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def batch_train_sequence_model(data,\n",
    "                               learning_rate=1e-3,\n",
    "                               epochs=1000,\n",
    "                               batch_size=128,\n",
    "                               blocks=2,\n",
    "                               filters=64,\n",
    "                               dropout_rate=0.2,\n",
    "                               embedding_dim=200,\n",
    "                               kernel_size=3,\n",
    "                               pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = build_model.sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Create training and validation generators.\n",
    "    training_generator = _data_generator(\n",
    "        x_train, train_labels, num_features, batch_size)\n",
    "    validation_generator = _data_generator(\n",
    "        x_val, val_labels, num_features, batch_size)\n",
    "\n",
    "    # Get number of training steps. This indicated the number of steps it takes\n",
    "    # to cover all samples in one epoch.\n",
    "    steps_per_epoch = x_train.shape[0] // batch_size\n",
    "    if x_train.shape[0] % batch_size:\n",
    "        steps_per_epoch += 1\n",
    "\n",
    "    # Get number of validation steps.\n",
    "    validation_steps = x_val.shape[0] // batch_size\n",
    "    if x_val.shape[0] % batch_size:\n",
    "        validation_steps += 1\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit_generator(\n",
    "            generator=training_generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            epochs=epochs,\n",
    "            verbose=2)  # Logs once per epoch.\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('amazon_reviews_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the Amazon reviews dataset to demonstrate training of\n",
    "    # sequence model with batches of data.\n",
    "    data = load_amazon_reviews_sentiment_analysis_dataset(\n",
    "            FLAGS.data_dir)\n",
    "    batch_train_sequence_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 13s - loss: 0.6688 - acc: 0.8324 - val_loss: 0.6483 - val_acc: 0.8429\n",
      "Epoch 2/1000\n",
      " - 12s - loss: 0.6224 - acc: 0.8758 - val_loss: 0.6100 - val_acc: 0.8517\n",
      "Epoch 3/1000\n",
      " - 13s - loss: 0.5826 - acc: 0.8791 - val_loss: 0.5771 - val_acc: 0.8560\n",
      "Epoch 4/1000\n",
      " - 12s - loss: 0.5484 - acc: 0.8835 - val_loss: 0.5488 - val_acc: 0.8593\n",
      "Epoch 5/1000\n",
      " - 14s - loss: 0.5187 - acc: 0.8858 - val_loss: 0.5242 - val_acc: 0.8614\n",
      "Epoch 6/1000\n",
      " - 13s - loss: 0.4923 - acc: 0.8873 - val_loss: 0.5026 - val_acc: 0.8639\n",
      "Epoch 7/1000\n",
      " - 13s - loss: 0.4699 - acc: 0.8901 - val_loss: 0.4836 - val_acc: 0.8658\n",
      "Epoch 8/1000\n",
      " - 13s - loss: 0.4500 - acc: 0.8917 - val_loss: 0.4668 - val_acc: 0.8674\n",
      "Epoch 9/1000\n",
      " - 15s - loss: 0.4318 - acc: 0.8964 - val_loss: 0.4517 - val_acc: 0.8697\n",
      "Epoch 10/1000\n",
      " - 15s - loss: 0.4154 - acc: 0.8989 - val_loss: 0.4381 - val_acc: 0.8712\n",
      "Epoch 11/1000\n",
      " - 14s - loss: 0.4015 - acc: 0.8992 - val_loss: 0.4259 - val_acc: 0.8728\n",
      "Epoch 12/1000\n",
      " - 15s - loss: 0.3880 - acc: 0.9012 - val_loss: 0.4147 - val_acc: 0.8752\n",
      "Epoch 13/1000\n",
      " - 13s - loss: 0.3757 - acc: 0.9043 - val_loss: 0.4046 - val_acc: 0.8767\n",
      "Epoch 14/1000\n",
      " - 14s - loss: 0.3637 - acc: 0.9078 - val_loss: 0.3952 - val_acc: 0.8780\n",
      "Epoch 15/1000\n",
      " - 15s - loss: 0.3534 - acc: 0.9076 - val_loss: 0.3867 - val_acc: 0.8797\n",
      "Epoch 16/1000\n",
      " - 14s - loss: 0.3438 - acc: 0.9077 - val_loss: 0.3787 - val_acc: 0.8815\n",
      "Epoch 17/1000\n",
      " - 14s - loss: 0.3355 - acc: 0.9096 - val_loss: 0.3714 - val_acc: 0.8826\n",
      "Epoch 18/1000\n",
      " - 14s - loss: 0.3265 - acc: 0.9129 - val_loss: 0.3644 - val_acc: 0.8835\n",
      "Epoch 19/1000\n",
      " - 13s - loss: 0.3191 - acc: 0.9116 - val_loss: 0.3582 - val_acc: 0.8851\n",
      "Epoch 20/1000\n",
      " - 15s - loss: 0.3115 - acc: 0.9164 - val_loss: 0.3521 - val_acc: 0.8863\n",
      "Epoch 21/1000\n",
      " - 15s - loss: 0.3056 - acc: 0.9155 - val_loss: 0.3466 - val_acc: 0.8875\n",
      "Epoch 22/1000\n",
      " - 15s - loss: 0.2984 - acc: 0.9170 - val_loss: 0.3414 - val_acc: 0.8880\n",
      "Epoch 23/1000\n",
      " - 14s - loss: 0.2923 - acc: 0.9185 - val_loss: 0.3366 - val_acc: 0.8890\n",
      "Epoch 24/1000\n",
      " - 14s - loss: 0.2862 - acc: 0.9205 - val_loss: 0.3318 - val_acc: 0.8900\n",
      "Epoch 25/1000\n",
      " - 13s - loss: 0.2811 - acc: 0.9220 - val_loss: 0.3275 - val_acc: 0.8908\n",
      "Epoch 26/1000\n",
      " - 14s - loss: 0.2753 - acc: 0.9226 - val_loss: 0.3236 - val_acc: 0.8914\n",
      "Epoch 27/1000\n",
      " - 13s - loss: 0.2714 - acc: 0.9237 - val_loss: 0.3195 - val_acc: 0.8920\n",
      "Epoch 28/1000\n",
      " - 14s - loss: 0.2660 - acc: 0.9252 - val_loss: 0.3160 - val_acc: 0.8926\n",
      "Epoch 29/1000\n",
      " - 14s - loss: 0.2620 - acc: 0.9248 - val_loss: 0.3125 - val_acc: 0.8930\n",
      "Epoch 30/1000\n",
      " - 13s - loss: 0.2580 - acc: 0.9250 - val_loss: 0.3093 - val_acc: 0.8938\n",
      "Epoch 31/1000\n",
      " - 15s - loss: 0.2533 - acc: 0.9285 - val_loss: 0.3061 - val_acc: 0.8943\n",
      "Epoch 32/1000\n",
      " - 14s - loss: 0.2496 - acc: 0.9286 - val_loss: 0.3032 - val_acc: 0.8948\n",
      "Epoch 33/1000\n",
      " - 15s - loss: 0.2466 - acc: 0.9291 - val_loss: 0.3004 - val_acc: 0.8954\n",
      "Epoch 34/1000\n",
      " - 13s - loss: 0.2429 - acc: 0.9300 - val_loss: 0.2978 - val_acc: 0.8962\n",
      "Epoch 35/1000\n",
      " - 18s - loss: 0.2384 - acc: 0.9316 - val_loss: 0.2954 - val_acc: 0.8958\n",
      "Epoch 36/1000\n",
      " - 14s - loss: 0.2357 - acc: 0.9329 - val_loss: 0.2930 - val_acc: 0.8965\n",
      "Epoch 37/1000\n",
      " - 17s - loss: 0.2337 - acc: 0.9322 - val_loss: 0.2907 - val_acc: 0.8974\n",
      "Epoch 38/1000\n",
      " - 14s - loss: 0.2294 - acc: 0.9337 - val_loss: 0.2884 - val_acc: 0.8980\n",
      "Epoch 39/1000\n",
      " - 13s - loss: 0.2266 - acc: 0.9343 - val_loss: 0.2863 - val_acc: 0.8982\n",
      "Epoch 40/1000\n",
      " - 12s - loss: 0.2242 - acc: 0.9350 - val_loss: 0.2842 - val_acc: 0.8982\n",
      "Epoch 41/1000\n",
      " - 12s - loss: 0.2219 - acc: 0.9348 - val_loss: 0.2824 - val_acc: 0.8985\n",
      "Epoch 42/1000\n",
      " - 13s - loss: 0.2189 - acc: 0.9360 - val_loss: 0.2805 - val_acc: 0.8990\n",
      "Epoch 43/1000\n",
      " - 15s - loss: 0.2161 - acc: 0.9376 - val_loss: 0.2788 - val_acc: 0.8993\n",
      "Epoch 44/1000\n",
      " - 15s - loss: 0.2140 - acc: 0.9366 - val_loss: 0.2771 - val_acc: 0.8996\n",
      "Epoch 45/1000\n",
      " - 13s - loss: 0.2113 - acc: 0.9384 - val_loss: 0.2754 - val_acc: 0.8998\n",
      "Epoch 46/1000\n",
      " - 14s - loss: 0.2094 - acc: 0.9388 - val_loss: 0.2740 - val_acc: 0.9002\n",
      "Epoch 47/1000\n",
      " - 13s - loss: 0.2061 - acc: 0.9398 - val_loss: 0.2726 - val_acc: 0.9005\n",
      "Epoch 48/1000\n",
      " - 12s - loss: 0.2047 - acc: 0.9399 - val_loss: 0.2712 - val_acc: 0.9009\n",
      "Epoch 49/1000\n",
      " - 12s - loss: 0.2010 - acc: 0.9422 - val_loss: 0.2698 - val_acc: 0.9012\n",
      "Epoch 50/1000\n",
      " - 12s - loss: 0.1995 - acc: 0.9420 - val_loss: 0.2685 - val_acc: 0.9014\n",
      "Epoch 51/1000\n",
      " - 11s - loss: 0.1974 - acc: 0.9430 - val_loss: 0.2673 - val_acc: 0.9012\n",
      "Epoch 52/1000\n",
      " - 11s - loss: 0.1952 - acc: 0.9428 - val_loss: 0.2661 - val_acc: 0.9017\n",
      "Epoch 53/1000\n",
      " - 12s - loss: 0.1952 - acc: 0.9430 - val_loss: 0.2649 - val_acc: 0.9018\n",
      "Epoch 54/1000\n",
      " - 11s - loss: 0.1922 - acc: 0.9436 - val_loss: 0.2637 - val_acc: 0.9023\n",
      "Epoch 55/1000\n",
      " - 11s - loss: 0.1907 - acc: 0.9434 - val_loss: 0.2628 - val_acc: 0.9026\n",
      "Epoch 56/1000\n",
      " - 11s - loss: 0.1881 - acc: 0.9448 - val_loss: 0.2618 - val_acc: 0.9027\n",
      "Epoch 57/1000\n",
      " - 12s - loss: 0.1870 - acc: 0.9456 - val_loss: 0.2607 - val_acc: 0.9031\n",
      "Epoch 58/1000\n",
      " - 12s - loss: 0.1851 - acc: 0.9461 - val_loss: 0.2597 - val_acc: 0.9032\n",
      "Epoch 59/1000\n",
      " - 11s - loss: 0.1838 - acc: 0.9456 - val_loss: 0.2590 - val_acc: 0.9031\n",
      "Epoch 60/1000\n",
      " - 11s - loss: 0.1821 - acc: 0.9473 - val_loss: 0.2580 - val_acc: 0.9034\n",
      "Epoch 61/1000\n",
      " - 12s - loss: 0.1808 - acc: 0.9475 - val_loss: 0.2571 - val_acc: 0.9035\n",
      "Epoch 62/1000\n",
      " - 12s - loss: 0.1789 - acc: 0.9474 - val_loss: 0.2563 - val_acc: 0.9036\n",
      "Epoch 63/1000\n",
      " - 12s - loss: 0.1772 - acc: 0.9486 - val_loss: 0.2555 - val_acc: 0.9040\n",
      "Epoch 64/1000\n",
      " - 12s - loss: 0.1764 - acc: 0.9479 - val_loss: 0.2546 - val_acc: 0.9042\n",
      "Epoch 65/1000\n",
      " - 11s - loss: 0.1755 - acc: 0.9489 - val_loss: 0.2538 - val_acc: 0.9043\n",
      "Epoch 66/1000\n",
      " - 12s - loss: 0.1728 - acc: 0.9508 - val_loss: 0.2532 - val_acc: 0.9043\n",
      "Epoch 67/1000\n",
      " - 12s - loss: 0.1725 - acc: 0.9498 - val_loss: 0.2525 - val_acc: 0.9046\n",
      "Epoch 68/1000\n",
      " - 12s - loss: 0.1709 - acc: 0.9495 - val_loss: 0.2518 - val_acc: 0.9049\n",
      "Epoch 69/1000\n",
      " - 11s - loss: 0.1691 - acc: 0.9498 - val_loss: 0.2514 - val_acc: 0.9043\n",
      "Epoch 70/1000\n",
      " - 12s - loss: 0.1679 - acc: 0.9516 - val_loss: 0.2506 - val_acc: 0.9046\n",
      "Epoch 71/1000\n",
      " - 12s - loss: 0.1669 - acc: 0.9520 - val_loss: 0.2500 - val_acc: 0.9049\n",
      "Epoch 72/1000\n",
      " - 12s - loss: 0.1665 - acc: 0.9515 - val_loss: 0.2496 - val_acc: 0.9048\n",
      "Epoch 73/1000\n",
      " - 11s - loss: 0.1646 - acc: 0.9526 - val_loss: 0.2488 - val_acc: 0.9054\n",
      "Epoch 74/1000\n",
      " - 12s - loss: 0.1632 - acc: 0.9528 - val_loss: 0.2484 - val_acc: 0.9054\n",
      "Epoch 75/1000\n",
      " - 12s - loss: 0.1619 - acc: 0.9529 - val_loss: 0.2478 - val_acc: 0.9056\n",
      "Epoch 76/1000\n",
      " - 12s - loss: 0.1606 - acc: 0.9556 - val_loss: 0.2471 - val_acc: 0.9057\n",
      "Epoch 77/1000\n",
      " - 12s - loss: 0.1605 - acc: 0.9532 - val_loss: 0.2468 - val_acc: 0.9058\n",
      "Epoch 78/1000\n",
      " - 12s - loss: 0.1587 - acc: 0.9551 - val_loss: 0.2464 - val_acc: 0.9055\n",
      "Epoch 79/1000\n",
      " - 12s - loss: 0.1581 - acc: 0.9547 - val_loss: 0.2458 - val_acc: 0.9058\n",
      "Epoch 80/1000\n",
      " - 11s - loss: 0.1566 - acc: 0.9560 - val_loss: 0.2454 - val_acc: 0.9061\n",
      "Epoch 81/1000\n",
      " - 11s - loss: 0.1552 - acc: 0.9546 - val_loss: 0.2449 - val_acc: 0.9063\n",
      "Epoch 82/1000\n",
      " - 11s - loss: 0.1543 - acc: 0.9551 - val_loss: 0.2446 - val_acc: 0.9063\n",
      "Epoch 83/1000\n",
      " - 11s - loss: 0.1540 - acc: 0.9557 - val_loss: 0.2441 - val_acc: 0.9064\n",
      "Epoch 84/1000\n",
      " - 12s - loss: 0.1526 - acc: 0.9565 - val_loss: 0.2438 - val_acc: 0.9062\n",
      "Epoch 85/1000\n",
      " - 11s - loss: 0.1513 - acc: 0.9568 - val_loss: 0.2434 - val_acc: 0.9062\n",
      "Epoch 86/1000\n",
      " - 11s - loss: 0.1507 - acc: 0.9574 - val_loss: 0.2429 - val_acc: 0.9064\n",
      "Epoch 87/1000\n",
      " - 11s - loss: 0.1501 - acc: 0.9576 - val_loss: 0.2426 - val_acc: 0.9061\n",
      "Epoch 88/1000\n",
      " - 13s - loss: 0.1485 - acc: 0.9576 - val_loss: 0.2423 - val_acc: 0.9059\n",
      "Epoch 89/1000\n",
      " - 11s - loss: 0.1481 - acc: 0.9573 - val_loss: 0.2421 - val_acc: 0.9059\n",
      "Epoch 90/1000\n",
      " - 11s - loss: 0.1475 - acc: 0.9587 - val_loss: 0.2418 - val_acc: 0.9059\n",
      "Epoch 91/1000\n",
      " - 11s - loss: 0.1456 - acc: 0.9584 - val_loss: 0.2414 - val_acc: 0.9060\n",
      "Epoch 92/1000\n",
      " - 11s - loss: 0.1449 - acc: 0.9599 - val_loss: 0.2410 - val_acc: 0.9061\n",
      "Epoch 93/1000\n",
      " - 11s - loss: 0.1443 - acc: 0.9594 - val_loss: 0.2408 - val_acc: 0.9061\n",
      "Epoch 94/1000\n",
      " - 11s - loss: 0.1438 - acc: 0.9587 - val_loss: 0.2405 - val_acc: 0.9061\n",
      "Epoch 95/1000\n",
      " - 12s - loss: 0.1424 - acc: 0.9596 - val_loss: 0.2402 - val_acc: 0.9064\n",
      "Epoch 96/1000\n",
      " - 11s - loss: 0.1412 - acc: 0.9597 - val_loss: 0.2401 - val_acc: 0.9057\n",
      "Epoch 97/1000\n",
      " - 11s - loss: 0.1400 - acc: 0.9610 - val_loss: 0.2397 - val_acc: 0.9062\n",
      "Epoch 98/1000\n",
      " - 12s - loss: 0.1408 - acc: 0.9598 - val_loss: 0.2394 - val_acc: 0.9065\n",
      "Epoch 99/1000\n",
      " - 12s - loss: 0.1390 - acc: 0.9609 - val_loss: 0.2393 - val_acc: 0.9060\n",
      "Epoch 100/1000\n",
      " - 12s - loss: 0.1387 - acc: 0.9610 - val_loss: 0.2390 - val_acc: 0.9062\n",
      "Epoch 101/1000\n",
      " - 11s - loss: 0.1376 - acc: 0.9608 - val_loss: 0.2386 - val_acc: 0.9066\n",
      "Epoch 102/1000\n",
      " - 11s - loss: 0.1376 - acc: 0.9619 - val_loss: 0.2385 - val_acc: 0.9064\n",
      "Epoch 103/1000\n",
      " - 12s - loss: 0.1361 - acc: 0.9623 - val_loss: 0.2383 - val_acc: 0.9064\n",
      "Epoch 104/1000\n",
      " - 12s - loss: 0.1355 - acc: 0.9625 - val_loss: 0.2381 - val_acc: 0.9065\n",
      "Epoch 105/1000\n",
      " - 12s - loss: 0.1339 - acc: 0.9625 - val_loss: 0.2380 - val_acc: 0.9064\n",
      "Epoch 106/1000\n",
      " - 12s - loss: 0.1334 - acc: 0.9621 - val_loss: 0.2377 - val_acc: 0.9066\n",
      "Epoch 107/1000\n",
      " - 12s - loss: 0.1328 - acc: 0.9630 - val_loss: 0.2375 - val_acc: 0.9068\n",
      "Epoch 108/1000\n",
      " - 12s - loss: 0.1311 - acc: 0.9631 - val_loss: 0.2374 - val_acc: 0.9068\n",
      "Epoch 109/1000\n",
      " - 12s - loss: 0.1322 - acc: 0.9630 - val_loss: 0.2373 - val_acc: 0.9069\n",
      "Epoch 110/1000\n",
      " - 12s - loss: 0.1304 - acc: 0.9629 - val_loss: 0.2369 - val_acc: 0.9066\n",
      "Epoch 111/1000\n",
      " - 12s - loss: 0.1296 - acc: 0.9644 - val_loss: 0.2368 - val_acc: 0.9066\n",
      "Epoch 112/1000\n",
      " - 11s - loss: 0.1287 - acc: 0.9654 - val_loss: 0.2366 - val_acc: 0.9067\n",
      "Epoch 113/1000\n",
      " - 11s - loss: 0.1295 - acc: 0.9646 - val_loss: 0.2365 - val_acc: 0.9066\n",
      "Epoch 114/1000\n",
      " - 12s - loss: 0.1282 - acc: 0.9652 - val_loss: 0.2364 - val_acc: 0.9068\n",
      "Epoch 115/1000\n",
      " - 12s - loss: 0.1275 - acc: 0.9645 - val_loss: 0.2363 - val_acc: 0.9066\n",
      "Epoch 116/1000\n",
      " - 12s - loss: 0.1272 - acc: 0.9643 - val_loss: 0.2362 - val_acc: 0.9065\n",
      "Epoch 117/1000\n",
      " - 12s - loss: 0.1266 - acc: 0.9649 - val_loss: 0.2361 - val_acc: 0.9067\n",
      "Epoch 118/1000\n",
      " - 12s - loss: 0.1257 - acc: 0.9650 - val_loss: 0.2361 - val_acc: 0.9065\n",
      "Epoch 119/1000\n",
      " - 11s - loss: 0.1249 - acc: 0.9655 - val_loss: 0.2358 - val_acc: 0.9062\n",
      "Epoch 120/1000\n",
      " - 11s - loss: 0.1235 - acc: 0.9673 - val_loss: 0.2357 - val_acc: 0.9062\n",
      "Epoch 121/1000\n",
      " - 11s - loss: 0.1228 - acc: 0.9659 - val_loss: 0.2357 - val_acc: 0.9060\n",
      "Epoch 122/1000\n",
      " - 12s - loss: 0.1242 - acc: 0.9654 - val_loss: 0.2355 - val_acc: 0.9062\n",
      "Epoch 123/1000\n",
      " - 12s - loss: 0.1222 - acc: 0.9660 - val_loss: 0.2355 - val_acc: 0.9059\n",
      "Epoch 124/1000\n",
      " - 12s - loss: 0.1221 - acc: 0.9668 - val_loss: 0.2355 - val_acc: 0.9060\n",
      "Epoch 125/1000\n",
      " - 11s - loss: 0.1203 - acc: 0.9670 - val_loss: 0.2353 - val_acc: 0.9060\n",
      "Epoch 126/1000\n",
      " - 12s - loss: 0.1204 - acc: 0.9679 - val_loss: 0.2351 - val_acc: 0.9058\n",
      "Epoch 127/1000\n",
      " - 12s - loss: 0.1198 - acc: 0.9671 - val_loss: 0.2350 - val_acc: 0.9058\n",
      "Epoch 128/1000\n",
      " - 12s - loss: 0.1190 - acc: 0.9680 - val_loss: 0.2350 - val_acc: 0.9055\n",
      "Epoch 129/1000\n",
      " - 12s - loss: 0.1186 - acc: 0.9683 - val_loss: 0.2349 - val_acc: 0.9055\n",
      "Epoch 130/1000\n",
      " - 13s - loss: 0.1178 - acc: 0.9681 - val_loss: 0.2348 - val_acc: 0.9056\n",
      "Epoch 131/1000\n",
      " - 12s - loss: 0.1170 - acc: 0.9679 - val_loss: 0.2347 - val_acc: 0.9054\n",
      "Epoch 132/1000\n",
      " - 14s - loss: 0.1178 - acc: 0.9688 - val_loss: 0.2348 - val_acc: 0.9057\n",
      "Epoch 133/1000\n",
      " - 12s - loss: 0.1176 - acc: 0.9678 - val_loss: 0.2347 - val_acc: 0.9055\n",
      "Validation accuracy: 0.9055199999809265, loss: 0.23472495356559753\n",
      "Accuracy: 0.9055199999809265, Parameters: (layers=1, units=8)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 16s - loss: 0.6690 - acc: 0.7939 - val_loss: 0.6485 - val_acc: 0.8454\n",
      "Epoch 2/1000\n",
      " - 14s - loss: 0.6226 - acc: 0.8730 - val_loss: 0.6101 - val_acc: 0.8517\n",
      "Epoch 3/1000\n",
      " - 13s - loss: 0.5828 - acc: 0.8780 - val_loss: 0.5772 - val_acc: 0.8568\n",
      "Epoch 4/1000\n",
      " - 12s - loss: 0.5485 - acc: 0.8818 - val_loss: 0.5489 - val_acc: 0.8596\n",
      "Epoch 5/1000\n",
      " - 14s - loss: 0.5190 - acc: 0.8850 - val_loss: 0.5242 - val_acc: 0.8622\n",
      "Epoch 6/1000\n",
      " - 18s - loss: 0.4928 - acc: 0.8899 - val_loss: 0.5026 - val_acc: 0.8646\n",
      "Epoch 7/1000\n",
      " - 12s - loss: 0.4699 - acc: 0.8918 - val_loss: 0.4837 - val_acc: 0.8666\n",
      "Epoch 8/1000\n",
      " - 12s - loss: 0.4497 - acc: 0.8938 - val_loss: 0.4668 - val_acc: 0.8685\n",
      "Epoch 9/1000\n",
      " - 12s - loss: 0.4320 - acc: 0.8950 - val_loss: 0.4517 - val_acc: 0.8704\n",
      "Epoch 10/1000\n",
      " - 13s - loss: 0.4157 - acc: 0.8968 - val_loss: 0.4381 - val_acc: 0.8716\n",
      "Epoch 11/1000\n",
      " - 12s - loss: 0.4013 - acc: 0.8995 - val_loss: 0.4258 - val_acc: 0.8738\n",
      "Epoch 12/1000\n",
      " - 13s - loss: 0.3880 - acc: 0.9011 - val_loss: 0.4148 - val_acc: 0.8748\n",
      "Epoch 13/1000\n",
      " - 12s - loss: 0.3753 - acc: 0.9054 - val_loss: 0.4046 - val_acc: 0.8770\n",
      "Epoch 14/1000\n",
      " - 14s - loss: 0.3638 - acc: 0.9044 - val_loss: 0.3952 - val_acc: 0.8782\n",
      "Epoch 15/1000\n",
      " - 14s - loss: 0.3544 - acc: 0.9059 - val_loss: 0.3866 - val_acc: 0.8800\n",
      "Epoch 16/1000\n",
      " - 13s - loss: 0.3441 - acc: 0.9093 - val_loss: 0.3786 - val_acc: 0.8814\n",
      "Epoch 17/1000\n",
      " - 12s - loss: 0.3345 - acc: 0.9111 - val_loss: 0.3713 - val_acc: 0.8828\n",
      "Epoch 18/1000\n",
      " - 12s - loss: 0.3265 - acc: 0.9111 - val_loss: 0.3643 - val_acc: 0.8840\n",
      "Epoch 19/1000\n",
      " - 12s - loss: 0.3193 - acc: 0.9141 - val_loss: 0.3579 - val_acc: 0.8852\n",
      "Epoch 20/1000\n",
      " - 12s - loss: 0.3123 - acc: 0.9145 - val_loss: 0.3521 - val_acc: 0.8866\n",
      "Epoch 21/1000\n",
      " - 13s - loss: 0.3053 - acc: 0.9154 - val_loss: 0.3466 - val_acc: 0.8872\n",
      "Epoch 22/1000\n",
      " - 12s - loss: 0.2988 - acc: 0.9175 - val_loss: 0.3413 - val_acc: 0.8881\n",
      "Epoch 23/1000\n",
      " - 12s - loss: 0.2924 - acc: 0.9190 - val_loss: 0.3364 - val_acc: 0.8888\n",
      "Epoch 24/1000\n",
      " - 15s - loss: 0.2864 - acc: 0.9206 - val_loss: 0.3319 - val_acc: 0.8894\n",
      "Epoch 25/1000\n",
      " - 14s - loss: 0.2816 - acc: 0.9215 - val_loss: 0.3275 - val_acc: 0.8902\n",
      "Epoch 26/1000\n",
      " - 14s - loss: 0.2767 - acc: 0.9213 - val_loss: 0.3234 - val_acc: 0.8910\n",
      "Epoch 27/1000\n",
      " - 14s - loss: 0.2714 - acc: 0.9215 - val_loss: 0.3194 - val_acc: 0.8920\n",
      "Epoch 28/1000\n",
      " - 15s - loss: 0.2667 - acc: 0.9240 - val_loss: 0.3159 - val_acc: 0.8923\n",
      "Epoch 29/1000\n",
      " - 14s - loss: 0.2621 - acc: 0.9262 - val_loss: 0.3124 - val_acc: 0.8932\n",
      "Epoch 30/1000\n",
      " - 14s - loss: 0.2576 - acc: 0.9269 - val_loss: 0.3091 - val_acc: 0.8942\n",
      "Epoch 31/1000\n",
      " - 13s - loss: 0.2536 - acc: 0.9280 - val_loss: 0.3061 - val_acc: 0.8945\n",
      "Epoch 32/1000\n",
      " - 12s - loss: 0.2503 - acc: 0.9278 - val_loss: 0.3032 - val_acc: 0.8952\n",
      "Epoch 33/1000\n",
      " - 13s - loss: 0.2462 - acc: 0.9288 - val_loss: 0.3003 - val_acc: 0.8957\n",
      "Epoch 34/1000\n",
      " - 13s - loss: 0.2418 - acc: 0.9306 - val_loss: 0.2976 - val_acc: 0.8963\n",
      "Epoch 35/1000\n",
      " - 12s - loss: 0.2393 - acc: 0.9303 - val_loss: 0.2952 - val_acc: 0.8963\n",
      "Epoch 36/1000\n",
      " - 14s - loss: 0.2355 - acc: 0.9313 - val_loss: 0.2927 - val_acc: 0.8974\n",
      "Epoch 37/1000\n",
      " - 13s - loss: 0.2330 - acc: 0.9328 - val_loss: 0.2905 - val_acc: 0.8973\n",
      "Epoch 38/1000\n",
      " - 13s - loss: 0.2291 - acc: 0.9342 - val_loss: 0.2883 - val_acc: 0.8975\n",
      "Epoch 39/1000\n",
      " - 13s - loss: 0.2256 - acc: 0.9352 - val_loss: 0.2862 - val_acc: 0.8979\n",
      "Epoch 40/1000\n",
      " - 12s - loss: 0.2236 - acc: 0.9352 - val_loss: 0.2843 - val_acc: 0.8981\n",
      "Epoch 41/1000\n",
      " - 14s - loss: 0.2209 - acc: 0.9363 - val_loss: 0.2822 - val_acc: 0.8986\n",
      "Epoch 42/1000\n",
      " - 13s - loss: 0.2180 - acc: 0.9366 - val_loss: 0.2804 - val_acc: 0.8989\n",
      "Epoch 43/1000\n",
      " - 13s - loss: 0.2163 - acc: 0.9376 - val_loss: 0.2788 - val_acc: 0.8993\n",
      "Epoch 44/1000\n",
      " - 14s - loss: 0.2136 - acc: 0.9381 - val_loss: 0.2771 - val_acc: 0.8994\n",
      "Epoch 45/1000\n",
      " - 12s - loss: 0.2108 - acc: 0.9383 - val_loss: 0.2754 - val_acc: 0.8997\n",
      "Epoch 46/1000\n",
      " - 12s - loss: 0.2085 - acc: 0.9383 - val_loss: 0.2740 - val_acc: 0.9002\n",
      "Epoch 47/1000\n",
      " - 12s - loss: 0.2063 - acc: 0.9396 - val_loss: 0.2724 - val_acc: 0.9006\n",
      "Epoch 48/1000\n",
      " - 11s - loss: 0.2039 - acc: 0.9414 - val_loss: 0.2711 - val_acc: 0.9008\n",
      "Epoch 49/1000\n",
      " - 11s - loss: 0.2024 - acc: 0.9418 - val_loss: 0.2697 - val_acc: 0.9009\n",
      "Epoch 50/1000\n",
      " - 12s - loss: 0.2006 - acc: 0.9414 - val_loss: 0.2684 - val_acc: 0.9013\n",
      "Epoch 51/1000\n",
      " - 12s - loss: 0.1973 - acc: 0.9433 - val_loss: 0.2670 - val_acc: 0.9015\n",
      "Epoch 52/1000\n",
      " - 12s - loss: 0.1959 - acc: 0.9414 - val_loss: 0.2659 - val_acc: 0.9018\n",
      "Epoch 53/1000\n",
      " - 12s - loss: 0.1943 - acc: 0.9425 - val_loss: 0.2649 - val_acc: 0.9020\n",
      "Epoch 54/1000\n",
      " - 12s - loss: 0.1923 - acc: 0.9441 - val_loss: 0.2637 - val_acc: 0.9026\n",
      "Epoch 55/1000\n",
      " - 12s - loss: 0.1905 - acc: 0.9443 - val_loss: 0.2626 - val_acc: 0.9026\n",
      "Epoch 56/1000\n",
      " - 12s - loss: 0.1895 - acc: 0.9438 - val_loss: 0.2616 - val_acc: 0.9030\n",
      "Epoch 57/1000\n",
      " - 12s - loss: 0.1874 - acc: 0.9449 - val_loss: 0.2605 - val_acc: 0.9030\n",
      "Epoch 58/1000\n",
      " - 12s - loss: 0.1852 - acc: 0.9476 - val_loss: 0.2596 - val_acc: 0.9031\n",
      "Epoch 59/1000\n",
      " - 12s - loss: 0.1842 - acc: 0.9467 - val_loss: 0.2587 - val_acc: 0.9032\n",
      "Epoch 60/1000\n",
      " - 12s - loss: 0.1822 - acc: 0.9460 - val_loss: 0.2580 - val_acc: 0.9035\n",
      "Epoch 61/1000\n",
      " - 12s - loss: 0.1803 - acc: 0.9481 - val_loss: 0.2570 - val_acc: 0.9034\n",
      "Epoch 62/1000\n",
      " - 11s - loss: 0.1789 - acc: 0.9493 - val_loss: 0.2562 - val_acc: 0.9038\n",
      "Epoch 63/1000\n",
      " - 12s - loss: 0.1785 - acc: 0.9477 - val_loss: 0.2554 - val_acc: 0.9041\n",
      "Epoch 64/1000\n",
      " - 12s - loss: 0.1769 - acc: 0.9490 - val_loss: 0.2548 - val_acc: 0.9038\n",
      "Epoch 65/1000\n",
      " - 11s - loss: 0.1750 - acc: 0.9494 - val_loss: 0.2538 - val_acc: 0.9044\n",
      "Epoch 66/1000\n",
      " - 12s - loss: 0.1733 - acc: 0.9494 - val_loss: 0.2530 - val_acc: 0.9045\n",
      "Epoch 67/1000\n",
      " - 11s - loss: 0.1716 - acc: 0.9502 - val_loss: 0.2526 - val_acc: 0.9044\n",
      "Epoch 68/1000\n",
      " - 11s - loss: 0.1706 - acc: 0.9506 - val_loss: 0.2518 - val_acc: 0.9046\n",
      "Epoch 69/1000\n",
      " - 12s - loss: 0.1692 - acc: 0.9518 - val_loss: 0.2511 - val_acc: 0.9047\n",
      "Epoch 70/1000\n",
      " - 12s - loss: 0.1681 - acc: 0.9520 - val_loss: 0.2504 - val_acc: 0.9046\n",
      "Epoch 71/1000\n",
      " - 12s - loss: 0.1678 - acc: 0.9510 - val_loss: 0.2498 - val_acc: 0.9046\n",
      "Epoch 72/1000\n",
      " - 12s - loss: 0.1657 - acc: 0.9516 - val_loss: 0.2494 - val_acc: 0.9049\n",
      "Epoch 73/1000\n",
      " - 12s - loss: 0.1644 - acc: 0.9520 - val_loss: 0.2488 - val_acc: 0.9050\n",
      "Epoch 74/1000\n",
      " - 12s - loss: 0.1634 - acc: 0.9544 - val_loss: 0.2482 - val_acc: 0.9053\n",
      "Epoch 75/1000\n",
      " - 12s - loss: 0.1621 - acc: 0.9529 - val_loss: 0.2476 - val_acc: 0.9052\n",
      "Epoch 76/1000\n",
      " - 11s - loss: 0.1617 - acc: 0.9538 - val_loss: 0.2472 - val_acc: 0.9052\n",
      "Epoch 77/1000\n",
      " - 12s - loss: 0.1595 - acc: 0.9544 - val_loss: 0.2467 - val_acc: 0.9053\n",
      "Epoch 78/1000\n",
      " - 12s - loss: 0.1599 - acc: 0.9530 - val_loss: 0.2461 - val_acc: 0.9056\n",
      "Epoch 79/1000\n",
      " - 11s - loss: 0.1574 - acc: 0.9544 - val_loss: 0.2457 - val_acc: 0.9055\n",
      "Epoch 80/1000\n",
      " - 12s - loss: 0.1560 - acc: 0.9550 - val_loss: 0.2454 - val_acc: 0.9054\n",
      "Epoch 81/1000\n",
      " - 12s - loss: 0.1565 - acc: 0.9548 - val_loss: 0.2449 - val_acc: 0.9053\n",
      "Epoch 82/1000\n",
      " - 12s - loss: 0.1541 - acc: 0.9564 - val_loss: 0.2445 - val_acc: 0.9054\n",
      "Epoch 83/1000\n",
      " - 13s - loss: 0.1542 - acc: 0.9558 - val_loss: 0.2441 - val_acc: 0.9059\n",
      "Epoch 84/1000\n",
      " - 12s - loss: 0.1522 - acc: 0.9568 - val_loss: 0.2437 - val_acc: 0.9056\n",
      "Epoch 85/1000\n",
      " - 15s - loss: 0.1520 - acc: 0.9579 - val_loss: 0.2433 - val_acc: 0.9058\n",
      "Epoch 86/1000\n",
      " - 13s - loss: 0.1505 - acc: 0.9574 - val_loss: 0.2429 - val_acc: 0.9060\n",
      "Epoch 87/1000\n",
      " - 12s - loss: 0.1502 - acc: 0.9568 - val_loss: 0.2427 - val_acc: 0.9062\n",
      "Epoch 88/1000\n",
      " - 12s - loss: 0.1481 - acc: 0.9588 - val_loss: 0.2422 - val_acc: 0.9061\n",
      "Epoch 89/1000\n",
      " - 12s - loss: 0.1483 - acc: 0.9570 - val_loss: 0.2420 - val_acc: 0.9060\n",
      "Epoch 90/1000\n",
      " - 12s - loss: 0.1473 - acc: 0.9582 - val_loss: 0.2415 - val_acc: 0.9058\n",
      "Epoch 91/1000\n",
      " - 11s - loss: 0.1469 - acc: 0.9576 - val_loss: 0.2412 - val_acc: 0.9060\n",
      "Epoch 92/1000\n",
      " - 11s - loss: 0.1447 - acc: 0.9593 - val_loss: 0.2409 - val_acc: 0.9059\n",
      "Epoch 93/1000\n",
      " - 12s - loss: 0.1435 - acc: 0.9600 - val_loss: 0.2407 - val_acc: 0.9060\n",
      "Epoch 94/1000\n",
      " - 12s - loss: 0.1430 - acc: 0.9592 - val_loss: 0.2404 - val_acc: 0.9060\n",
      "Epoch 95/1000\n",
      " - 12s - loss: 0.1421 - acc: 0.9612 - val_loss: 0.2401 - val_acc: 0.9060\n",
      "Epoch 96/1000\n",
      " - 12s - loss: 0.1426 - acc: 0.9594 - val_loss: 0.2399 - val_acc: 0.9059\n",
      "Epoch 97/1000\n",
      " - 12s - loss: 0.1406 - acc: 0.9602 - val_loss: 0.2395 - val_acc: 0.9062\n",
      "Epoch 98/1000\n",
      " - 12s - loss: 0.1406 - acc: 0.9605 - val_loss: 0.2394 - val_acc: 0.9060\n",
      "Epoch 99/1000\n",
      " - 12s - loss: 0.1395 - acc: 0.9609 - val_loss: 0.2391 - val_acc: 0.9060\n",
      "Epoch 100/1000\n",
      " - 12s - loss: 0.1375 - acc: 0.9612 - val_loss: 0.2389 - val_acc: 0.9060\n",
      "Epoch 101/1000\n",
      " - 12s - loss: 0.1373 - acc: 0.9628 - val_loss: 0.2385 - val_acc: 0.9061\n",
      "Epoch 102/1000\n",
      " - 12s - loss: 0.1367 - acc: 0.9608 - val_loss: 0.2384 - val_acc: 0.9061\n",
      "Epoch 103/1000\n",
      " - 14s - loss: 0.1358 - acc: 0.9628 - val_loss: 0.2382 - val_acc: 0.9062\n",
      "Epoch 104/1000\n",
      " - 16s - loss: 0.1358 - acc: 0.9614 - val_loss: 0.2380 - val_acc: 0.9063\n",
      "Epoch 105/1000\n",
      " - 16s - loss: 0.1340 - acc: 0.9635 - val_loss: 0.2378 - val_acc: 0.9064\n",
      "Epoch 106/1000\n",
      " - 13s - loss: 0.1346 - acc: 0.9618 - val_loss: 0.2375 - val_acc: 0.9067\n",
      "Epoch 107/1000\n",
      " - 14s - loss: 0.1336 - acc: 0.9627 - val_loss: 0.2376 - val_acc: 0.9063\n",
      "Epoch 108/1000\n",
      " - 13s - loss: 0.1324 - acc: 0.9644 - val_loss: 0.2373 - val_acc: 0.9063\n",
      "Epoch 109/1000\n",
      " - 14s - loss: 0.1310 - acc: 0.9634 - val_loss: 0.2371 - val_acc: 0.9065\n",
      "Epoch 110/1000\n",
      " - 13s - loss: 0.1315 - acc: 0.9633 - val_loss: 0.2372 - val_acc: 0.9062\n",
      "Epoch 111/1000\n",
      " - 15s - loss: 0.1305 - acc: 0.9632 - val_loss: 0.2369 - val_acc: 0.9065\n",
      "Epoch 112/1000\n",
      " - 12s - loss: 0.1293 - acc: 0.9644 - val_loss: 0.2367 - val_acc: 0.9064\n",
      "Epoch 113/1000\n",
      " - 12s - loss: 0.1295 - acc: 0.9633 - val_loss: 0.2366 - val_acc: 0.9062\n",
      "Epoch 114/1000\n",
      " - 12s - loss: 0.1288 - acc: 0.9644 - val_loss: 0.2365 - val_acc: 0.9061\n",
      "Epoch 115/1000\n",
      " - 12s - loss: 0.1280 - acc: 0.9643 - val_loss: 0.2363 - val_acc: 0.9062\n",
      "Epoch 116/1000\n",
      " - 12s - loss: 0.1273 - acc: 0.9657 - val_loss: 0.2362 - val_acc: 0.9060\n",
      "Epoch 117/1000\n",
      " - 12s - loss: 0.1269 - acc: 0.9646 - val_loss: 0.2359 - val_acc: 0.9062\n",
      "Epoch 118/1000\n",
      " - 13s - loss: 0.1257 - acc: 0.9648 - val_loss: 0.2359 - val_acc: 0.9058\n",
      "Epoch 119/1000\n",
      " - 11s - loss: 0.1250 - acc: 0.9659 - val_loss: 0.2357 - val_acc: 0.9059\n",
      "Epoch 120/1000\n",
      " - 12s - loss: 0.1242 - acc: 0.9650 - val_loss: 0.2357 - val_acc: 0.9060\n",
      "Epoch 121/1000\n",
      " - 12s - loss: 0.1257 - acc: 0.9648 - val_loss: 0.2355 - val_acc: 0.9058\n",
      "Epoch 122/1000\n",
      " - 12s - loss: 0.1233 - acc: 0.9670 - val_loss: 0.2355 - val_acc: 0.9058\n",
      "Epoch 123/1000\n",
      " - 12s - loss: 0.1228 - acc: 0.9674 - val_loss: 0.2353 - val_acc: 0.9056\n",
      "Epoch 124/1000\n",
      " - 12s - loss: 0.1215 - acc: 0.9672 - val_loss: 0.2353 - val_acc: 0.9056\n",
      "Epoch 125/1000\n",
      " - 11s - loss: 0.1210 - acc: 0.9677 - val_loss: 0.2352 - val_acc: 0.9056\n",
      "Epoch 126/1000\n",
      " - 13s - loss: 0.1204 - acc: 0.9671 - val_loss: 0.2351 - val_acc: 0.9054\n",
      "Epoch 127/1000\n",
      " - 13s - loss: 0.1201 - acc: 0.9668 - val_loss: 0.2350 - val_acc: 0.9054\n",
      "Epoch 128/1000\n",
      " - 13s - loss: 0.1202 - acc: 0.9669 - val_loss: 0.2350 - val_acc: 0.9054\n",
      "Epoch 129/1000\n",
      " - 14s - loss: 0.1199 - acc: 0.9674 - val_loss: 0.2350 - val_acc: 0.9054\n",
      "Epoch 130/1000\n",
      " - 14s - loss: 0.1179 - acc: 0.9685 - val_loss: 0.2348 - val_acc: 0.9054\n",
      "Epoch 131/1000\n",
      " - 12s - loss: 0.1174 - acc: 0.9701 - val_loss: 0.2348 - val_acc: 0.9053\n",
      "Epoch 132/1000\n",
      " - 15s - loss: 0.1163 - acc: 0.9692 - val_loss: 0.2347 - val_acc: 0.9054\n",
      "Epoch 133/1000\n",
      " - 13s - loss: 0.1164 - acc: 0.9687 - val_loss: 0.2345 - val_acc: 0.9054\n",
      "Epoch 134/1000\n",
      " - 14s - loss: 0.1167 - acc: 0.9682 - val_loss: 0.2345 - val_acc: 0.9054\n",
      "Epoch 135/1000\n",
      " - 16s - loss: 0.1160 - acc: 0.9686 - val_loss: 0.2345 - val_acc: 0.9055\n",
      "Epoch 136/1000\n",
      " - 13s - loss: 0.1151 - acc: 0.9691 - val_loss: 0.2344 - val_acc: 0.9054\n",
      "Epoch 137/1000\n",
      " - 16s - loss: 0.1150 - acc: 0.9692 - val_loss: 0.2344 - val_acc: 0.9054\n",
      "Epoch 138/1000\n",
      " - 16s - loss: 0.1138 - acc: 0.9696 - val_loss: 0.2343 - val_acc: 0.9056\n",
      "Epoch 139/1000\n",
      " - 16s - loss: 0.1127 - acc: 0.9700 - val_loss: 0.2343 - val_acc: 0.9057\n",
      "Epoch 140/1000\n",
      " - 14s - loss: 0.1129 - acc: 0.9699 - val_loss: 0.2342 - val_acc: 0.9056\n",
      "Epoch 141/1000\n",
      " - 15s - loss: 0.1121 - acc: 0.9713 - val_loss: 0.2342 - val_acc: 0.9056\n",
      "Epoch 142/1000\n",
      " - 15s - loss: 0.1120 - acc: 0.9708 - val_loss: 0.2342 - val_acc: 0.9054\n",
      "Epoch 143/1000\n",
      " - 13s - loss: 0.1109 - acc: 0.9704 - val_loss: 0.2342 - val_acc: 0.9055\n",
      "Validation accuracy: 0.9055199999809265, loss: 0.23421534759521484\n",
      "Accuracy: 0.9055199999809265, Parameters: (layers=1, units=16)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 14s - loss: 0.6686 - acc: 0.8021 - val_loss: 0.6481 - val_acc: 0.8500\n",
      "Epoch 2/1000\n",
      " - 12s - loss: 0.6222 - acc: 0.8751 - val_loss: 0.6099 - val_acc: 0.8527\n",
      "Epoch 3/1000\n",
      " - 13s - loss: 0.5827 - acc: 0.8778 - val_loss: 0.5770 - val_acc: 0.8572\n",
      "Epoch 4/1000\n",
      " - 13s - loss: 0.5484 - acc: 0.8837 - val_loss: 0.5487 - val_acc: 0.8602\n",
      "Epoch 5/1000\n",
      " - 12s - loss: 0.5191 - acc: 0.8869 - val_loss: 0.5241 - val_acc: 0.8626\n",
      "Epoch 6/1000\n",
      " - 12s - loss: 0.4928 - acc: 0.8880 - val_loss: 0.5025 - val_acc: 0.8645\n",
      "Epoch 7/1000\n",
      " - 12s - loss: 0.4699 - acc: 0.8916 - val_loss: 0.4836 - val_acc: 0.8666\n",
      "Epoch 8/1000\n",
      " - 12s - loss: 0.4500 - acc: 0.8930 - val_loss: 0.4667 - val_acc: 0.8682\n",
      "Epoch 9/1000\n",
      " - 12s - loss: 0.4324 - acc: 0.8953 - val_loss: 0.4517 - val_acc: 0.8702\n",
      "Epoch 10/1000\n",
      " - 12s - loss: 0.4153 - acc: 0.8984 - val_loss: 0.4381 - val_acc: 0.8722\n",
      "Epoch 11/1000\n",
      " - 13s - loss: 0.4016 - acc: 0.8985 - val_loss: 0.4258 - val_acc: 0.8735\n",
      "Epoch 12/1000\n",
      " - 12s - loss: 0.3881 - acc: 0.9000 - val_loss: 0.4147 - val_acc: 0.8759\n",
      "Epoch 13/1000\n",
      " - 12s - loss: 0.3753 - acc: 0.9048 - val_loss: 0.4045 - val_acc: 0.8774\n",
      "Epoch 14/1000\n",
      " - 12s - loss: 0.3645 - acc: 0.9040 - val_loss: 0.3952 - val_acc: 0.8785\n",
      "Epoch 15/1000\n",
      " - 12s - loss: 0.3536 - acc: 0.9082 - val_loss: 0.3866 - val_acc: 0.8802\n",
      "Epoch 16/1000\n",
      " - 12s - loss: 0.3434 - acc: 0.9108 - val_loss: 0.3786 - val_acc: 0.8813\n",
      "Epoch 17/1000\n",
      " - 12s - loss: 0.3355 - acc: 0.9098 - val_loss: 0.3712 - val_acc: 0.8828\n",
      "Epoch 18/1000\n",
      " - 12s - loss: 0.3268 - acc: 0.9125 - val_loss: 0.3643 - val_acc: 0.8841\n",
      "Epoch 19/1000\n",
      " - 12s - loss: 0.3189 - acc: 0.9138 - val_loss: 0.3579 - val_acc: 0.8848\n",
      "Epoch 20/1000\n",
      " - 12s - loss: 0.3123 - acc: 0.9139 - val_loss: 0.3520 - val_acc: 0.8862\n",
      "Epoch 21/1000\n",
      " - 12s - loss: 0.3049 - acc: 0.9172 - val_loss: 0.3464 - val_acc: 0.8871\n",
      "Epoch 22/1000\n",
      " - 12s - loss: 0.2982 - acc: 0.9173 - val_loss: 0.3414 - val_acc: 0.8880\n",
      "Epoch 23/1000\n",
      " - 12s - loss: 0.2919 - acc: 0.9181 - val_loss: 0.3364 - val_acc: 0.8888\n",
      "Epoch 24/1000\n",
      " - 12s - loss: 0.2867 - acc: 0.9197 - val_loss: 0.3318 - val_acc: 0.8898\n",
      "Epoch 25/1000\n",
      " - 12s - loss: 0.2807 - acc: 0.9204 - val_loss: 0.3274 - val_acc: 0.8908\n",
      "Epoch 26/1000\n",
      " - 12s - loss: 0.2763 - acc: 0.9217 - val_loss: 0.3234 - val_acc: 0.8915\n",
      "Epoch 27/1000\n",
      " - 12s - loss: 0.2710 - acc: 0.9226 - val_loss: 0.3194 - val_acc: 0.8924\n",
      "Epoch 28/1000\n",
      " - 12s - loss: 0.2664 - acc: 0.9242 - val_loss: 0.3158 - val_acc: 0.8930\n",
      "Epoch 29/1000\n",
      " - 12s - loss: 0.2621 - acc: 0.9260 - val_loss: 0.3124 - val_acc: 0.8934\n",
      "Epoch 30/1000\n",
      " - 12s - loss: 0.2580 - acc: 0.9256 - val_loss: 0.3094 - val_acc: 0.8938\n",
      "Epoch 31/1000\n",
      " - 13s - loss: 0.2534 - acc: 0.9272 - val_loss: 0.3061 - val_acc: 0.8945\n",
      "Epoch 32/1000\n",
      " - 12s - loss: 0.2489 - acc: 0.9300 - val_loss: 0.3034 - val_acc: 0.8945\n",
      "Epoch 33/1000\n",
      " - 12s - loss: 0.2470 - acc: 0.9288 - val_loss: 0.3004 - val_acc: 0.8955\n",
      "Epoch 34/1000\n",
      " - 12s - loss: 0.2430 - acc: 0.9281 - val_loss: 0.2978 - val_acc: 0.8958\n",
      "Epoch 35/1000\n",
      " - 12s - loss: 0.2386 - acc: 0.9318 - val_loss: 0.2953 - val_acc: 0.8967\n",
      "Epoch 36/1000\n",
      " - 12s - loss: 0.2361 - acc: 0.9318 - val_loss: 0.2928 - val_acc: 0.8971\n",
      "Epoch 37/1000\n",
      " - 12s - loss: 0.2327 - acc: 0.9325 - val_loss: 0.2905 - val_acc: 0.8976\n",
      "Epoch 38/1000\n",
      " - 12s - loss: 0.2297 - acc: 0.9328 - val_loss: 0.2884 - val_acc: 0.8977\n",
      "Epoch 39/1000\n",
      " - 12s - loss: 0.2274 - acc: 0.9334 - val_loss: 0.2864 - val_acc: 0.8980\n",
      "Epoch 40/1000\n",
      " - 12s - loss: 0.2238 - acc: 0.9348 - val_loss: 0.2842 - val_acc: 0.8985\n",
      "Epoch 41/1000\n",
      " - 12s - loss: 0.2220 - acc: 0.9342 - val_loss: 0.2824 - val_acc: 0.8989\n",
      "Epoch 42/1000\n",
      " - 12s - loss: 0.2188 - acc: 0.9361 - val_loss: 0.2805 - val_acc: 0.8991\n",
      "Epoch 43/1000\n",
      " - 12s - loss: 0.2159 - acc: 0.9367 - val_loss: 0.2788 - val_acc: 0.8992\n",
      "Epoch 44/1000\n",
      " - 12s - loss: 0.2129 - acc: 0.9383 - val_loss: 0.2771 - val_acc: 0.8999\n",
      "Epoch 45/1000\n",
      " - 12s - loss: 0.2114 - acc: 0.9374 - val_loss: 0.2755 - val_acc: 0.9006\n",
      "Epoch 46/1000\n",
      " - 12s - loss: 0.2082 - acc: 0.9397 - val_loss: 0.2739 - val_acc: 0.9009\n",
      "Epoch 47/1000\n",
      " - 12s - loss: 0.2070 - acc: 0.9400 - val_loss: 0.2725 - val_acc: 0.9011\n",
      "Epoch 48/1000\n",
      " - 12s - loss: 0.2042 - acc: 0.9414 - val_loss: 0.2712 - val_acc: 0.9013\n",
      "Epoch 49/1000\n",
      " - 12s - loss: 0.2022 - acc: 0.9408 - val_loss: 0.2699 - val_acc: 0.9013\n",
      "Epoch 50/1000\n",
      " - 12s - loss: 0.1999 - acc: 0.9417 - val_loss: 0.2685 - val_acc: 0.9015\n",
      "Epoch 51/1000\n",
      " - 14s - loss: 0.1981 - acc: 0.9436 - val_loss: 0.2674 - val_acc: 0.9014\n",
      "Epoch 52/1000\n",
      " - 12s - loss: 0.1967 - acc: 0.9423 - val_loss: 0.2660 - val_acc: 0.9021\n",
      "Epoch 53/1000\n",
      " - 12s - loss: 0.1936 - acc: 0.9431 - val_loss: 0.2649 - val_acc: 0.9021\n",
      "Epoch 54/1000\n",
      " - 13s - loss: 0.1934 - acc: 0.9425 - val_loss: 0.2638 - val_acc: 0.9024\n",
      "Epoch 55/1000\n",
      " - 13s - loss: 0.1916 - acc: 0.9442 - val_loss: 0.2628 - val_acc: 0.9024\n",
      "Epoch 56/1000\n",
      " - 13s - loss: 0.1887 - acc: 0.9446 - val_loss: 0.2617 - val_acc: 0.9029\n",
      "Epoch 57/1000\n",
      " - 12s - loss: 0.1871 - acc: 0.9440 - val_loss: 0.2608 - val_acc: 0.9031\n",
      "Epoch 58/1000\n",
      " - 12s - loss: 0.1847 - acc: 0.9476 - val_loss: 0.2597 - val_acc: 0.9035\n",
      "Epoch 59/1000\n",
      " - 12s - loss: 0.1827 - acc: 0.9467 - val_loss: 0.2587 - val_acc: 0.9035\n",
      "Epoch 60/1000\n",
      " - 12s - loss: 0.1824 - acc: 0.9467 - val_loss: 0.2580 - val_acc: 0.9035\n",
      "Epoch 61/1000\n",
      " - 12s - loss: 0.1805 - acc: 0.9476 - val_loss: 0.2570 - val_acc: 0.9036\n",
      "Epoch 62/1000\n",
      " - 12s - loss: 0.1789 - acc: 0.9471 - val_loss: 0.2563 - val_acc: 0.9037\n",
      "Epoch 63/1000\n",
      " - 12s - loss: 0.1780 - acc: 0.9472 - val_loss: 0.2554 - val_acc: 0.9039\n",
      "Epoch 64/1000\n",
      " - 12s - loss: 0.1761 - acc: 0.9494 - val_loss: 0.2548 - val_acc: 0.9038\n",
      "Epoch 65/1000\n",
      " - 12s - loss: 0.1748 - acc: 0.9498 - val_loss: 0.2539 - val_acc: 0.9040\n",
      "Epoch 66/1000\n",
      " - 12s - loss: 0.1729 - acc: 0.9492 - val_loss: 0.2533 - val_acc: 0.9044\n",
      "Epoch 67/1000\n",
      " - 12s - loss: 0.1722 - acc: 0.9509 - val_loss: 0.2526 - val_acc: 0.9046\n",
      "Epoch 68/1000\n",
      " - 12s - loss: 0.1715 - acc: 0.9496 - val_loss: 0.2519 - val_acc: 0.9044\n",
      "Epoch 69/1000\n",
      " - 12s - loss: 0.1697 - acc: 0.9508 - val_loss: 0.2511 - val_acc: 0.9051\n",
      "Epoch 70/1000\n",
      " - 12s - loss: 0.1676 - acc: 0.9526 - val_loss: 0.2506 - val_acc: 0.9051\n",
      "Epoch 71/1000\n",
      " - 12s - loss: 0.1654 - acc: 0.9518 - val_loss: 0.2500 - val_acc: 0.9050\n",
      "Epoch 72/1000\n",
      " - 12s - loss: 0.1659 - acc: 0.9501 - val_loss: 0.2495 - val_acc: 0.9050\n",
      "Epoch 73/1000\n",
      " - 12s - loss: 0.1652 - acc: 0.9514 - val_loss: 0.2489 - val_acc: 0.9052\n",
      "Epoch 74/1000\n",
      " - 12s - loss: 0.1629 - acc: 0.9533 - val_loss: 0.2482 - val_acc: 0.9056\n",
      "Epoch 75/1000\n",
      " - 12s - loss: 0.1619 - acc: 0.9544 - val_loss: 0.2478 - val_acc: 0.9053\n",
      "Epoch 76/1000\n",
      " - 15s - loss: 0.1604 - acc: 0.9544 - val_loss: 0.2474 - val_acc: 0.9055\n",
      "Epoch 77/1000\n",
      " - 12s - loss: 0.1591 - acc: 0.9548 - val_loss: 0.2468 - val_acc: 0.9056\n",
      "Epoch 78/1000\n",
      " - 13s - loss: 0.1578 - acc: 0.9548 - val_loss: 0.2463 - val_acc: 0.9059\n",
      "Epoch 79/1000\n",
      " - 12s - loss: 0.1574 - acc: 0.9542 - val_loss: 0.2457 - val_acc: 0.9060\n",
      "Epoch 80/1000\n",
      " - 12s - loss: 0.1565 - acc: 0.9544 - val_loss: 0.2453 - val_acc: 0.9062\n",
      "Epoch 81/1000\n",
      " - 12s - loss: 0.1549 - acc: 0.9546 - val_loss: 0.2448 - val_acc: 0.9063\n",
      "Epoch 82/1000\n",
      " - 12s - loss: 0.1543 - acc: 0.9560 - val_loss: 0.2445 - val_acc: 0.9061\n",
      "Epoch 83/1000\n",
      " - 12s - loss: 0.1543 - acc: 0.9558 - val_loss: 0.2442 - val_acc: 0.9059\n",
      "Epoch 84/1000\n",
      " - 12s - loss: 0.1517 - acc: 0.9568 - val_loss: 0.2437 - val_acc: 0.9061\n",
      "Epoch 85/1000\n",
      " - 12s - loss: 0.1507 - acc: 0.9571 - val_loss: 0.2433 - val_acc: 0.9060\n",
      "Epoch 86/1000\n",
      " - 12s - loss: 0.1502 - acc: 0.9572 - val_loss: 0.2429 - val_acc: 0.9062\n",
      "Epoch 87/1000\n",
      " - 12s - loss: 0.1493 - acc: 0.9565 - val_loss: 0.2426 - val_acc: 0.9061\n",
      "Epoch 88/1000\n",
      " - 12s - loss: 0.1490 - acc: 0.9571 - val_loss: 0.2422 - val_acc: 0.9062\n",
      "Epoch 89/1000\n",
      " - 12s - loss: 0.1476 - acc: 0.9575 - val_loss: 0.2419 - val_acc: 0.9061\n",
      "Epoch 90/1000\n",
      " - 13s - loss: 0.1460 - acc: 0.9573 - val_loss: 0.2416 - val_acc: 0.9062\n",
      "Epoch 91/1000\n",
      " - 12s - loss: 0.1450 - acc: 0.9592 - val_loss: 0.2413 - val_acc: 0.9066\n",
      "Epoch 92/1000\n",
      " - 12s - loss: 0.1445 - acc: 0.9589 - val_loss: 0.2410 - val_acc: 0.9063\n",
      "Epoch 93/1000\n",
      " - 12s - loss: 0.1441 - acc: 0.9590 - val_loss: 0.2406 - val_acc: 0.9062\n",
      "Epoch 94/1000\n",
      " - 12s - loss: 0.1427 - acc: 0.9599 - val_loss: 0.2403 - val_acc: 0.9064\n",
      "Epoch 95/1000\n",
      " - 12s - loss: 0.1421 - acc: 0.9599 - val_loss: 0.2401 - val_acc: 0.9064\n",
      "Epoch 96/1000\n",
      " - 12s - loss: 0.1420 - acc: 0.9594 - val_loss: 0.2398 - val_acc: 0.9067\n",
      "Epoch 97/1000\n",
      " - 12s - loss: 0.1401 - acc: 0.9611 - val_loss: 0.2395 - val_acc: 0.9065\n",
      "Epoch 98/1000\n",
      " - 12s - loss: 0.1401 - acc: 0.9601 - val_loss: 0.2395 - val_acc: 0.9063\n",
      "Epoch 99/1000\n",
      " - 12s - loss: 0.1399 - acc: 0.9604 - val_loss: 0.2391 - val_acc: 0.9066\n",
      "Epoch 100/1000\n",
      " - 12s - loss: 0.1377 - acc: 0.9608 - val_loss: 0.2389 - val_acc: 0.9065\n",
      "Epoch 101/1000\n",
      " - 12s - loss: 0.1378 - acc: 0.9611 - val_loss: 0.2386 - val_acc: 0.9064\n",
      "Epoch 102/1000\n",
      " - 12s - loss: 0.1363 - acc: 0.9617 - val_loss: 0.2384 - val_acc: 0.9064\n",
      "Epoch 103/1000\n",
      " - 12s - loss: 0.1368 - acc: 0.9612 - val_loss: 0.2382 - val_acc: 0.9065\n",
      "Epoch 104/1000\n",
      " - 12s - loss: 0.1351 - acc: 0.9616 - val_loss: 0.2381 - val_acc: 0.9066\n",
      "Epoch 105/1000\n",
      " - 12s - loss: 0.1342 - acc: 0.9626 - val_loss: 0.2380 - val_acc: 0.9063\n",
      "Epoch 106/1000\n",
      " - 12s - loss: 0.1332 - acc: 0.9638 - val_loss: 0.2377 - val_acc: 0.9066\n",
      "Epoch 107/1000\n",
      " - 13s - loss: 0.1329 - acc: 0.9632 - val_loss: 0.2376 - val_acc: 0.9066\n",
      "Epoch 108/1000\n",
      " - 12s - loss: 0.1323 - acc: 0.9635 - val_loss: 0.2373 - val_acc: 0.9065\n",
      "Epoch 109/1000\n",
      " - 12s - loss: 0.1317 - acc: 0.9632 - val_loss: 0.2372 - val_acc: 0.9065\n",
      "Epoch 110/1000\n",
      " - 12s - loss: 0.1319 - acc: 0.9629 - val_loss: 0.2370 - val_acc: 0.9067\n",
      "Epoch 111/1000\n",
      " - 12s - loss: 0.1297 - acc: 0.9635 - val_loss: 0.2368 - val_acc: 0.9066\n",
      "Epoch 112/1000\n",
      " - 12s - loss: 0.1293 - acc: 0.9637 - val_loss: 0.2366 - val_acc: 0.9068\n",
      "Epoch 113/1000\n",
      " - 12s - loss: 0.1292 - acc: 0.9639 - val_loss: 0.2367 - val_acc: 0.9065\n",
      "Epoch 114/1000\n",
      " - 12s - loss: 0.1283 - acc: 0.9647 - val_loss: 0.2364 - val_acc: 0.9066\n",
      "Epoch 115/1000\n",
      " - 12s - loss: 0.1270 - acc: 0.9650 - val_loss: 0.2362 - val_acc: 0.9066\n",
      "Epoch 116/1000\n",
      " - 12s - loss: 0.1266 - acc: 0.9653 - val_loss: 0.2361 - val_acc: 0.9066\n",
      "Epoch 117/1000\n",
      " - 12s - loss: 0.1267 - acc: 0.9649 - val_loss: 0.2359 - val_acc: 0.9063\n",
      "Epoch 118/1000\n",
      " - 12s - loss: 0.1246 - acc: 0.9658 - val_loss: 0.2359 - val_acc: 0.9066\n",
      "Epoch 119/1000\n",
      " - 12s - loss: 0.1254 - acc: 0.9656 - val_loss: 0.2357 - val_acc: 0.9065\n",
      "Epoch 120/1000\n",
      " - 12s - loss: 0.1241 - acc: 0.9656 - val_loss: 0.2358 - val_acc: 0.9062\n",
      "Epoch 121/1000\n",
      " - 15s - loss: 0.1233 - acc: 0.9666 - val_loss: 0.2355 - val_acc: 0.9065\n",
      "Epoch 122/1000\n",
      " - 12s - loss: 0.1222 - acc: 0.9682 - val_loss: 0.2354 - val_acc: 0.9064\n",
      "Epoch 123/1000\n",
      " - 13s - loss: 0.1229 - acc: 0.9671 - val_loss: 0.2354 - val_acc: 0.9061\n",
      "Epoch 124/1000\n",
      " - 15s - loss: 0.1223 - acc: 0.9666 - val_loss: 0.2352 - val_acc: 0.9061\n",
      "Epoch 125/1000\n",
      " - 14s - loss: 0.1208 - acc: 0.9674 - val_loss: 0.2351 - val_acc: 0.9058\n",
      "Epoch 126/1000\n",
      " - 14s - loss: 0.1210 - acc: 0.9671 - val_loss: 0.2351 - val_acc: 0.9056\n",
      "Epoch 127/1000\n",
      " - 14s - loss: 0.1200 - acc: 0.9680 - val_loss: 0.2351 - val_acc: 0.9058\n",
      "Epoch 128/1000\n",
      " - 12s - loss: 0.1202 - acc: 0.9678 - val_loss: 0.2349 - val_acc: 0.9057\n",
      "Epoch 129/1000\n",
      " - 12s - loss: 0.1186 - acc: 0.9677 - val_loss: 0.2349 - val_acc: 0.9056\n",
      "Epoch 130/1000\n",
      " - 13s - loss: 0.1190 - acc: 0.9678 - val_loss: 0.2348 - val_acc: 0.9057\n",
      "Epoch 131/1000\n",
      " - 12s - loss: 0.1162 - acc: 0.9706 - val_loss: 0.2347 - val_acc: 0.9057\n",
      "Epoch 132/1000\n",
      " - 12s - loss: 0.1171 - acc: 0.9692 - val_loss: 0.2346 - val_acc: 0.9058\n",
      "Epoch 133/1000\n",
      " - 12s - loss: 0.1164 - acc: 0.9695 - val_loss: 0.2346 - val_acc: 0.9058\n",
      "Epoch 134/1000\n",
      " - 12s - loss: 0.1163 - acc: 0.9684 - val_loss: 0.2346 - val_acc: 0.9056\n",
      "Epoch 135/1000\n",
      " - 12s - loss: 0.1160 - acc: 0.9688 - val_loss: 0.2345 - val_acc: 0.9057\n",
      "Epoch 136/1000\n",
      " - 12s - loss: 0.1157 - acc: 0.9694 - val_loss: 0.2344 - val_acc: 0.9058\n",
      "Epoch 137/1000\n",
      " - 14s - loss: 0.1145 - acc: 0.9692 - val_loss: 0.2345 - val_acc: 0.9056\n",
      "Epoch 138/1000\n",
      " - 14s - loss: 0.1146 - acc: 0.9697 - val_loss: 0.2345 - val_acc: 0.9057\n",
      "Validation accuracy: 0.9057199999809266, loss: 0.23448476146697997\n",
      "Accuracy: 0.9057199999809266, Parameters: (layers=1, units=32)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 15s - loss: 0.6689 - acc: 0.8087 - val_loss: 0.6485 - val_acc: 0.8426\n",
      "Epoch 2/1000\n",
      " - 13s - loss: 0.6224 - acc: 0.8737 - val_loss: 0.6101 - val_acc: 0.8528\n",
      "Epoch 3/1000\n",
      " - 13s - loss: 0.5825 - acc: 0.8794 - val_loss: 0.5771 - val_acc: 0.8556\n",
      "Epoch 4/1000\n",
      " - 12s - loss: 0.5483 - acc: 0.8828 - val_loss: 0.5488 - val_acc: 0.8592\n",
      "Epoch 5/1000\n",
      " - 13s - loss: 0.5187 - acc: 0.8857 - val_loss: 0.5242 - val_acc: 0.8614\n",
      "Epoch 6/1000\n",
      " - 14s - loss: 0.4928 - acc: 0.8886 - val_loss: 0.5027 - val_acc: 0.8635\n",
      "Epoch 7/1000\n",
      " - 14s - loss: 0.4700 - acc: 0.8900 - val_loss: 0.4836 - val_acc: 0.8662\n",
      "Epoch 8/1000\n",
      " - 13s - loss: 0.4496 - acc: 0.8949 - val_loss: 0.4667 - val_acc: 0.8681\n",
      "Epoch 9/1000\n",
      " - 15s - loss: 0.4319 - acc: 0.8948 - val_loss: 0.4517 - val_acc: 0.8693\n",
      "Epoch 10/1000\n",
      " - 12s - loss: 0.4156 - acc: 0.8984 - val_loss: 0.4382 - val_acc: 0.8708\n",
      "Epoch 11/1000\n",
      " - 12s - loss: 0.4009 - acc: 0.8998 - val_loss: 0.4258 - val_acc: 0.8735\n",
      "Epoch 12/1000\n",
      " - 12s - loss: 0.3876 - acc: 0.9014 - val_loss: 0.4147 - val_acc: 0.8747\n",
      "Epoch 13/1000\n",
      " - 12s - loss: 0.3755 - acc: 0.9036 - val_loss: 0.4045 - val_acc: 0.8768\n",
      "Epoch 14/1000\n",
      " - 12s - loss: 0.3648 - acc: 0.9048 - val_loss: 0.3953 - val_acc: 0.8783\n",
      "Epoch 15/1000\n",
      " - 12s - loss: 0.3535 - acc: 0.9085 - val_loss: 0.3864 - val_acc: 0.8802\n",
      "Epoch 16/1000\n",
      " - 12s - loss: 0.3450 - acc: 0.9069 - val_loss: 0.3785 - val_acc: 0.8812\n",
      "Epoch 17/1000\n",
      " - 12s - loss: 0.3356 - acc: 0.9112 - val_loss: 0.3713 - val_acc: 0.8829\n",
      "Epoch 18/1000\n",
      " - 12s - loss: 0.3273 - acc: 0.9111 - val_loss: 0.3645 - val_acc: 0.8832\n",
      "Epoch 19/1000\n",
      " - 12s - loss: 0.3186 - acc: 0.9135 - val_loss: 0.3581 - val_acc: 0.8850\n",
      "Epoch 20/1000\n",
      " - 12s - loss: 0.3123 - acc: 0.9140 - val_loss: 0.3520 - val_acc: 0.8864\n",
      "Epoch 21/1000\n",
      " - 12s - loss: 0.3054 - acc: 0.9160 - val_loss: 0.3466 - val_acc: 0.8874\n",
      "Epoch 22/1000\n",
      " - 12s - loss: 0.2993 - acc: 0.9166 - val_loss: 0.3415 - val_acc: 0.8883\n",
      "Epoch 23/1000\n",
      " - 12s - loss: 0.2936 - acc: 0.9183 - val_loss: 0.3365 - val_acc: 0.8888\n",
      "Epoch 24/1000\n",
      " - 12s - loss: 0.2872 - acc: 0.9192 - val_loss: 0.3320 - val_acc: 0.8897\n",
      "Epoch 25/1000\n",
      " - 11s - loss: 0.2810 - acc: 0.9218 - val_loss: 0.3276 - val_acc: 0.8905\n",
      "Epoch 26/1000\n",
      " - 12s - loss: 0.2755 - acc: 0.9238 - val_loss: 0.3235 - val_acc: 0.8912\n",
      "Epoch 27/1000\n",
      " - 12s - loss: 0.2708 - acc: 0.9234 - val_loss: 0.3195 - val_acc: 0.8920\n",
      "Epoch 28/1000\n",
      " - 12s - loss: 0.2667 - acc: 0.9244 - val_loss: 0.3161 - val_acc: 0.8924\n",
      "Epoch 29/1000\n",
      " - 12s - loss: 0.2627 - acc: 0.9249 - val_loss: 0.3126 - val_acc: 0.8933\n",
      "Epoch 30/1000\n",
      " - 12s - loss: 0.2575 - acc: 0.9262 - val_loss: 0.3092 - val_acc: 0.8944\n",
      "Epoch 31/1000\n",
      " - 12s - loss: 0.2536 - acc: 0.9275 - val_loss: 0.3062 - val_acc: 0.8941\n",
      "Epoch 32/1000\n",
      " - 12s - loss: 0.2496 - acc: 0.9283 - val_loss: 0.3033 - val_acc: 0.8948\n",
      "Epoch 33/1000\n",
      " - 12s - loss: 0.2465 - acc: 0.9286 - val_loss: 0.3005 - val_acc: 0.8952\n",
      "Epoch 34/1000\n",
      " - 12s - loss: 0.2426 - acc: 0.9314 - val_loss: 0.2978 - val_acc: 0.8960\n",
      "Epoch 35/1000\n",
      " - 12s - loss: 0.2390 - acc: 0.9316 - val_loss: 0.2953 - val_acc: 0.8963\n",
      "Epoch 36/1000\n",
      " - 12s - loss: 0.2354 - acc: 0.9320 - val_loss: 0.2930 - val_acc: 0.8968\n",
      "Epoch 37/1000\n",
      " - 12s - loss: 0.2323 - acc: 0.9330 - val_loss: 0.2906 - val_acc: 0.8970\n",
      "Epoch 38/1000\n",
      " - 11s - loss: 0.2289 - acc: 0.9334 - val_loss: 0.2882 - val_acc: 0.8982\n",
      "Epoch 39/1000\n",
      " - 12s - loss: 0.2272 - acc: 0.9342 - val_loss: 0.2862 - val_acc: 0.8980\n",
      "Epoch 40/1000\n",
      " - 12s - loss: 0.2242 - acc: 0.9338 - val_loss: 0.2843 - val_acc: 0.8982\n",
      "Epoch 41/1000\n",
      " - 13s - loss: 0.2210 - acc: 0.9369 - val_loss: 0.2823 - val_acc: 0.8984\n",
      "Epoch 42/1000\n",
      " - 11s - loss: 0.2191 - acc: 0.9355 - val_loss: 0.2806 - val_acc: 0.8989\n",
      "Epoch 43/1000\n",
      " - 12s - loss: 0.2162 - acc: 0.9380 - val_loss: 0.2788 - val_acc: 0.8995\n",
      "Epoch 44/1000\n",
      " - 12s - loss: 0.2131 - acc: 0.9375 - val_loss: 0.2772 - val_acc: 0.8998\n",
      "Epoch 45/1000\n",
      " - 12s - loss: 0.2108 - acc: 0.9392 - val_loss: 0.2755 - val_acc: 0.9004\n",
      "Epoch 46/1000\n",
      " - 12s - loss: 0.2089 - acc: 0.9402 - val_loss: 0.2739 - val_acc: 0.9008\n",
      "Epoch 47/1000\n",
      " - 12s - loss: 0.2068 - acc: 0.9383 - val_loss: 0.2725 - val_acc: 0.9008\n",
      "Epoch 48/1000\n",
      " - 12s - loss: 0.2038 - acc: 0.9414 - val_loss: 0.2712 - val_acc: 0.9009\n",
      "Epoch 49/1000\n",
      " - 11s - loss: 0.2017 - acc: 0.9399 - val_loss: 0.2698 - val_acc: 0.9011\n",
      "Epoch 50/1000\n",
      " - 12s - loss: 0.2010 - acc: 0.9414 - val_loss: 0.2685 - val_acc: 0.9015\n",
      "Epoch 51/1000\n",
      " - 12s - loss: 0.1985 - acc: 0.9430 - val_loss: 0.2673 - val_acc: 0.9018\n",
      "Epoch 52/1000\n",
      " - 11s - loss: 0.1965 - acc: 0.9438 - val_loss: 0.2660 - val_acc: 0.9022\n",
      "Epoch 53/1000\n",
      " - 11s - loss: 0.1944 - acc: 0.9430 - val_loss: 0.2649 - val_acc: 0.9022\n",
      "Epoch 54/1000\n",
      " - 12s - loss: 0.1915 - acc: 0.9452 - val_loss: 0.2638 - val_acc: 0.9024\n",
      "Epoch 55/1000\n",
      " - 12s - loss: 0.1900 - acc: 0.9452 - val_loss: 0.2627 - val_acc: 0.9026\n",
      "Epoch 56/1000\n",
      " - 12s - loss: 0.1885 - acc: 0.9454 - val_loss: 0.2617 - val_acc: 0.9032\n",
      "Epoch 57/1000\n",
      " - 11s - loss: 0.1872 - acc: 0.9452 - val_loss: 0.2607 - val_acc: 0.9032\n",
      "Epoch 58/1000\n",
      " - 11s - loss: 0.1852 - acc: 0.9460 - val_loss: 0.2598 - val_acc: 0.9034\n",
      "Epoch 59/1000\n",
      " - 13s - loss: 0.1832 - acc: 0.9464 - val_loss: 0.2588 - val_acc: 0.9037\n",
      "Epoch 60/1000\n",
      " - 12s - loss: 0.1823 - acc: 0.9455 - val_loss: 0.2579 - val_acc: 0.9040\n",
      "Epoch 61/1000\n",
      " - 12s - loss: 0.1811 - acc: 0.9473 - val_loss: 0.2571 - val_acc: 0.9040\n",
      "Epoch 62/1000\n",
      " - 12s - loss: 0.1782 - acc: 0.9494 - val_loss: 0.2563 - val_acc: 0.9039\n",
      "Epoch 63/1000\n",
      " - 12s - loss: 0.1784 - acc: 0.9487 - val_loss: 0.2556 - val_acc: 0.9040\n",
      "Epoch 64/1000\n",
      " - 12s - loss: 0.1762 - acc: 0.9497 - val_loss: 0.2547 - val_acc: 0.9040\n",
      "Epoch 65/1000\n",
      " - 12s - loss: 0.1750 - acc: 0.9494 - val_loss: 0.2540 - val_acc: 0.9042\n",
      "Epoch 66/1000\n",
      " - 11s - loss: 0.1733 - acc: 0.9486 - val_loss: 0.2533 - val_acc: 0.9042\n",
      "Epoch 67/1000\n",
      " - 11s - loss: 0.1719 - acc: 0.9507 - val_loss: 0.2524 - val_acc: 0.9046\n",
      "Epoch 68/1000\n",
      " - 11s - loss: 0.1709 - acc: 0.9510 - val_loss: 0.2519 - val_acc: 0.9044\n",
      "Epoch 69/1000\n",
      " - 12s - loss: 0.1681 - acc: 0.9514 - val_loss: 0.2514 - val_acc: 0.9044\n",
      "Epoch 70/1000\n",
      " - 12s - loss: 0.1687 - acc: 0.9505 - val_loss: 0.2505 - val_acc: 0.9050\n",
      "Epoch 71/1000\n",
      " - 12s - loss: 0.1669 - acc: 0.9502 - val_loss: 0.2499 - val_acc: 0.9050\n",
      "Epoch 72/1000\n",
      " - 12s - loss: 0.1654 - acc: 0.9529 - val_loss: 0.2493 - val_acc: 0.9051\n",
      "Epoch 73/1000\n",
      " - 12s - loss: 0.1643 - acc: 0.9518 - val_loss: 0.2487 - val_acc: 0.9053\n",
      "Epoch 74/1000\n",
      " - 12s - loss: 0.1627 - acc: 0.9531 - val_loss: 0.2483 - val_acc: 0.9050\n",
      "Epoch 75/1000\n",
      " - 12s - loss: 0.1622 - acc: 0.9538 - val_loss: 0.2477 - val_acc: 0.9053\n",
      "Epoch 76/1000\n",
      " - 12s - loss: 0.1611 - acc: 0.9542 - val_loss: 0.2472 - val_acc: 0.9052\n",
      "Epoch 77/1000\n",
      " - 12s - loss: 0.1603 - acc: 0.9526 - val_loss: 0.2468 - val_acc: 0.9055\n",
      "Epoch 78/1000\n",
      " - 12s - loss: 0.1592 - acc: 0.9537 - val_loss: 0.2461 - val_acc: 0.9056\n",
      "Epoch 79/1000\n",
      " - 11s - loss: 0.1580 - acc: 0.9532 - val_loss: 0.2458 - val_acc: 0.9058\n",
      "Epoch 80/1000\n",
      " - 11s - loss: 0.1557 - acc: 0.9559 - val_loss: 0.2454 - val_acc: 0.9057\n",
      "Epoch 81/1000\n",
      " - 11s - loss: 0.1556 - acc: 0.9553 - val_loss: 0.2448 - val_acc: 0.9058\n",
      "Epoch 82/1000\n",
      " - 12s - loss: 0.1545 - acc: 0.9558 - val_loss: 0.2444 - val_acc: 0.9061\n",
      "Epoch 83/1000\n",
      " - 12s - loss: 0.1539 - acc: 0.9555 - val_loss: 0.2442 - val_acc: 0.9060\n",
      "Epoch 84/1000\n",
      " - 12s - loss: 0.1522 - acc: 0.9562 - val_loss: 0.2437 - val_acc: 0.9062\n",
      "Epoch 85/1000\n",
      " - 12s - loss: 0.1511 - acc: 0.9571 - val_loss: 0.2432 - val_acc: 0.9060\n",
      "Epoch 86/1000\n",
      " - 12s - loss: 0.1501 - acc: 0.9580 - val_loss: 0.2430 - val_acc: 0.9061\n",
      "Epoch 87/1000\n",
      " - 12s - loss: 0.1494 - acc: 0.9576 - val_loss: 0.2425 - val_acc: 0.9061\n",
      "Epoch 88/1000\n",
      " - 12s - loss: 0.1488 - acc: 0.9572 - val_loss: 0.2422 - val_acc: 0.9060\n",
      "Epoch 89/1000\n",
      " - 12s - loss: 0.1472 - acc: 0.9580 - val_loss: 0.2420 - val_acc: 0.9059\n",
      "Epoch 90/1000\n",
      " - 11s - loss: 0.1465 - acc: 0.9584 - val_loss: 0.2416 - val_acc: 0.9059\n",
      "Epoch 91/1000\n",
      " - 12s - loss: 0.1460 - acc: 0.9579 - val_loss: 0.2413 - val_acc: 0.9060\n",
      "Epoch 92/1000\n",
      " - 12s - loss: 0.1450 - acc: 0.9590 - val_loss: 0.2410 - val_acc: 0.9059\n",
      "Epoch 93/1000\n",
      " - 11s - loss: 0.1438 - acc: 0.9601 - val_loss: 0.2407 - val_acc: 0.9060\n",
      "Epoch 94/1000\n",
      " - 12s - loss: 0.1426 - acc: 0.9601 - val_loss: 0.2404 - val_acc: 0.9061\n",
      "Epoch 95/1000\n",
      " - 12s - loss: 0.1420 - acc: 0.9598 - val_loss: 0.2402 - val_acc: 0.9063\n",
      "Epoch 96/1000\n",
      " - 12s - loss: 0.1410 - acc: 0.9608 - val_loss: 0.2399 - val_acc: 0.9064\n",
      "Epoch 97/1000\n",
      " - 12s - loss: 0.1400 - acc: 0.9605 - val_loss: 0.2397 - val_acc: 0.9063\n",
      "Epoch 98/1000\n",
      " - 12s - loss: 0.1402 - acc: 0.9608 - val_loss: 0.2394 - val_acc: 0.9063\n",
      "Epoch 99/1000\n",
      " - 12s - loss: 0.1393 - acc: 0.9602 - val_loss: 0.2391 - val_acc: 0.9068\n",
      "Epoch 100/1000\n",
      " - 12s - loss: 0.1376 - acc: 0.9622 - val_loss: 0.2388 - val_acc: 0.9067\n",
      "Epoch 101/1000\n",
      " - 12s - loss: 0.1372 - acc: 0.9621 - val_loss: 0.2386 - val_acc: 0.9070\n",
      "Epoch 102/1000\n",
      " - 12s - loss: 0.1365 - acc: 0.9621 - val_loss: 0.2384 - val_acc: 0.9069\n",
      "Epoch 103/1000\n",
      " - 12s - loss: 0.1360 - acc: 0.9612 - val_loss: 0.2382 - val_acc: 0.9066\n",
      "Epoch 104/1000\n",
      " - 12s - loss: 0.1354 - acc: 0.9609 - val_loss: 0.2380 - val_acc: 0.9068\n",
      "Epoch 105/1000\n",
      " - 12s - loss: 0.1340 - acc: 0.9633 - val_loss: 0.2378 - val_acc: 0.9069\n",
      "Epoch 106/1000\n",
      " - 12s - loss: 0.1333 - acc: 0.9634 - val_loss: 0.2376 - val_acc: 0.9067\n",
      "Epoch 107/1000\n",
      " - 12s - loss: 0.1330 - acc: 0.9622 - val_loss: 0.2375 - val_acc: 0.9066\n",
      "Epoch 108/1000\n",
      " - 11s - loss: 0.1317 - acc: 0.9637 - val_loss: 0.2373 - val_acc: 0.9068\n",
      "Epoch 109/1000\n",
      " - 11s - loss: 0.1321 - acc: 0.9633 - val_loss: 0.2372 - val_acc: 0.9067\n",
      "Epoch 110/1000\n",
      " - 12s - loss: 0.1301 - acc: 0.9640 - val_loss: 0.2370 - val_acc: 0.9068\n",
      "Epoch 111/1000\n",
      " - 12s - loss: 0.1298 - acc: 0.9646 - val_loss: 0.2367 - val_acc: 0.9067\n",
      "Epoch 112/1000\n",
      " - 11s - loss: 0.1291 - acc: 0.9634 - val_loss: 0.2367 - val_acc: 0.9068\n",
      "Epoch 113/1000\n",
      " - 12s - loss: 0.1286 - acc: 0.9653 - val_loss: 0.2365 - val_acc: 0.9068\n",
      "Epoch 114/1000\n",
      " - 12s - loss: 0.1285 - acc: 0.9646 - val_loss: 0.2364 - val_acc: 0.9067\n",
      "Epoch 115/1000\n",
      " - 12s - loss: 0.1272 - acc: 0.9644 - val_loss: 0.2363 - val_acc: 0.9067\n",
      "Epoch 116/1000\n",
      " - 12s - loss: 0.1275 - acc: 0.9646 - val_loss: 0.2361 - val_acc: 0.9068\n",
      "Epoch 117/1000\n",
      " - 12s - loss: 0.1266 - acc: 0.9654 - val_loss: 0.2360 - val_acc: 0.9068\n",
      "Epoch 118/1000\n",
      " - 12s - loss: 0.1254 - acc: 0.9657 - val_loss: 0.2360 - val_acc: 0.9067\n",
      "Epoch 119/1000\n",
      " - 12s - loss: 0.1245 - acc: 0.9648 - val_loss: 0.2358 - val_acc: 0.9065\n",
      "Epoch 120/1000\n",
      " - 12s - loss: 0.1241 - acc: 0.9649 - val_loss: 0.2356 - val_acc: 0.9066\n",
      "Epoch 121/1000\n",
      " - 11s - loss: 0.1239 - acc: 0.9662 - val_loss: 0.2355 - val_acc: 0.9064\n",
      "Epoch 122/1000\n",
      " - 12s - loss: 0.1224 - acc: 0.9679 - val_loss: 0.2354 - val_acc: 0.9064\n",
      "Epoch 123/1000\n",
      " - 11s - loss: 0.1232 - acc: 0.9664 - val_loss: 0.2354 - val_acc: 0.9063\n",
      "Epoch 124/1000\n",
      " - 12s - loss: 0.1215 - acc: 0.9661 - val_loss: 0.2353 - val_acc: 0.9060\n",
      "Epoch 125/1000\n",
      " - 11s - loss: 0.1202 - acc: 0.9671 - val_loss: 0.2350 - val_acc: 0.9059\n",
      "Epoch 126/1000\n",
      " - 12s - loss: 0.1205 - acc: 0.9674 - val_loss: 0.2350 - val_acc: 0.9057\n",
      "Epoch 127/1000\n",
      " - 12s - loss: 0.1196 - acc: 0.9676 - val_loss: 0.2349 - val_acc: 0.9058\n",
      "Epoch 128/1000\n",
      " - 11s - loss: 0.1194 - acc: 0.9684 - val_loss: 0.2349 - val_acc: 0.9056\n",
      "Epoch 129/1000\n",
      " - 11s - loss: 0.1185 - acc: 0.9680 - val_loss: 0.2349 - val_acc: 0.9056\n",
      "Epoch 130/1000\n",
      " - 12s - loss: 0.1186 - acc: 0.9679 - val_loss: 0.2348 - val_acc: 0.9058\n",
      "Epoch 131/1000\n",
      " - 13s - loss: 0.1182 - acc: 0.9676 - val_loss: 0.2347 - val_acc: 0.9057\n",
      "Epoch 132/1000\n",
      " - 12s - loss: 0.1171 - acc: 0.9692 - val_loss: 0.2345 - val_acc: 0.9056\n",
      "Epoch 133/1000\n",
      " - 11s - loss: 0.1166 - acc: 0.9683 - val_loss: 0.2345 - val_acc: 0.9055\n",
      "Epoch 134/1000\n",
      " - 11s - loss: 0.1154 - acc: 0.9691 - val_loss: 0.2344 - val_acc: 0.9056\n",
      "Epoch 135/1000\n",
      " - 11s - loss: 0.1157 - acc: 0.9688 - val_loss: 0.2345 - val_acc: 0.9052\n",
      "Epoch 136/1000\n",
      " - 12s - loss: 0.1153 - acc: 0.9698 - val_loss: 0.2344 - val_acc: 0.9053\n",
      "Epoch 137/1000\n",
      " - 12s - loss: 0.1152 - acc: 0.9692 - val_loss: 0.2343 - val_acc: 0.9055\n",
      "Epoch 138/1000\n",
      " - 12s - loss: 0.1146 - acc: 0.9692 - val_loss: 0.2341 - val_acc: 0.9057\n",
      "Epoch 139/1000\n",
      " - 11s - loss: 0.1135 - acc: 0.9698 - val_loss: 0.2342 - val_acc: 0.9056\n",
      "Epoch 140/1000\n",
      " - 12s - loss: 0.1126 - acc: 0.9693 - val_loss: 0.2343 - val_acc: 0.9054\n",
      "Validation accuracy: 0.9053599999809265, loss: 0.23425194484710693\n",
      "Accuracy: 0.9053599999809265, Parameters: (layers=1, units=64)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 17s - loss: 0.6688 - acc: 0.8141 - val_loss: 0.6484 - val_acc: 0.8480\n",
      "Epoch 2/1000\n",
      " - 12s - loss: 0.6222 - acc: 0.8761 - val_loss: 0.6101 - val_acc: 0.8524\n",
      "Epoch 3/1000\n",
      " - 12s - loss: 0.5827 - acc: 0.8796 - val_loss: 0.5773 - val_acc: 0.8563\n",
      "Epoch 4/1000\n",
      " - 12s - loss: 0.5486 - acc: 0.8834 - val_loss: 0.5489 - val_acc: 0.8592\n",
      "Epoch 5/1000\n",
      " - 12s - loss: 0.5187 - acc: 0.8860 - val_loss: 0.5243 - val_acc: 0.8615\n",
      "Epoch 6/1000\n",
      " - 12s - loss: 0.4933 - acc: 0.8874 - val_loss: 0.5027 - val_acc: 0.8642\n",
      "Epoch 7/1000\n",
      " - 12s - loss: 0.4704 - acc: 0.8908 - val_loss: 0.4838 - val_acc: 0.8656\n",
      "Epoch 8/1000\n",
      " - 12s - loss: 0.4500 - acc: 0.8928 - val_loss: 0.4668 - val_acc: 0.8687\n",
      "Epoch 9/1000\n",
      " - 12s - loss: 0.4320 - acc: 0.8946 - val_loss: 0.4517 - val_acc: 0.8698\n",
      "Epoch 10/1000\n",
      " - 12s - loss: 0.4153 - acc: 0.8982 - val_loss: 0.4382 - val_acc: 0.8716\n",
      "Epoch 11/1000\n",
      " - 13s - loss: 0.4013 - acc: 0.8990 - val_loss: 0.4258 - val_acc: 0.8739\n",
      "Epoch 12/1000\n",
      " - 12s - loss: 0.3876 - acc: 0.9002 - val_loss: 0.4147 - val_acc: 0.8753\n",
      "Epoch 13/1000\n",
      " - 13s - loss: 0.3753 - acc: 0.9049 - val_loss: 0.4045 - val_acc: 0.8769\n",
      "Epoch 14/1000\n",
      " - 12s - loss: 0.3637 - acc: 0.9056 - val_loss: 0.3951 - val_acc: 0.8788\n",
      "Epoch 15/1000\n",
      " - 13s - loss: 0.3537 - acc: 0.9077 - val_loss: 0.3866 - val_acc: 0.8799\n",
      "Epoch 16/1000\n",
      " - 12s - loss: 0.3439 - acc: 0.9094 - val_loss: 0.3786 - val_acc: 0.8816\n",
      "Epoch 17/1000\n",
      " - 12s - loss: 0.3345 - acc: 0.9111 - val_loss: 0.3713 - val_acc: 0.8828\n",
      "Epoch 18/1000\n",
      " - 12s - loss: 0.3275 - acc: 0.9118 - val_loss: 0.3644 - val_acc: 0.8842\n",
      "Epoch 19/1000\n",
      " - 12s - loss: 0.3194 - acc: 0.9126 - val_loss: 0.3580 - val_acc: 0.8854\n",
      "Epoch 20/1000\n",
      " - 12s - loss: 0.3120 - acc: 0.9133 - val_loss: 0.3521 - val_acc: 0.8864\n",
      "Epoch 21/1000\n",
      " - 12s - loss: 0.3049 - acc: 0.9164 - val_loss: 0.3465 - val_acc: 0.8871\n",
      "Epoch 22/1000\n",
      " - 11s - loss: 0.2988 - acc: 0.9183 - val_loss: 0.3413 - val_acc: 0.8881\n",
      "Epoch 23/1000\n",
      " - 12s - loss: 0.2932 - acc: 0.9181 - val_loss: 0.3364 - val_acc: 0.8889\n",
      "Epoch 24/1000\n",
      " - 11s - loss: 0.2879 - acc: 0.9181 - val_loss: 0.3320 - val_acc: 0.8901\n",
      "Epoch 25/1000\n",
      " - 12s - loss: 0.2819 - acc: 0.9209 - val_loss: 0.3276 - val_acc: 0.8907\n",
      "Epoch 26/1000\n",
      " - 12s - loss: 0.2764 - acc: 0.9210 - val_loss: 0.3235 - val_acc: 0.8912\n",
      "Epoch 27/1000\n",
      " - 12s - loss: 0.2711 - acc: 0.9246 - val_loss: 0.3196 - val_acc: 0.8920\n",
      "Epoch 28/1000\n",
      " - 12s - loss: 0.2666 - acc: 0.9248 - val_loss: 0.3159 - val_acc: 0.8930\n",
      "Epoch 29/1000\n",
      " - 12s - loss: 0.2618 - acc: 0.9241 - val_loss: 0.3124 - val_acc: 0.8938\n",
      "Epoch 30/1000\n",
      " - 12s - loss: 0.2584 - acc: 0.9266 - val_loss: 0.3093 - val_acc: 0.8941\n",
      "Epoch 31/1000\n",
      " - 12s - loss: 0.2537 - acc: 0.9276 - val_loss: 0.3062 - val_acc: 0.8945\n",
      "Epoch 32/1000\n",
      " - 12s - loss: 0.2502 - acc: 0.9280 - val_loss: 0.3032 - val_acc: 0.8952\n",
      "Epoch 33/1000\n",
      " - 12s - loss: 0.2467 - acc: 0.9297 - val_loss: 0.3004 - val_acc: 0.8958\n",
      "Epoch 34/1000\n",
      " - 11s - loss: 0.2430 - acc: 0.9290 - val_loss: 0.2978 - val_acc: 0.8961\n",
      "Epoch 35/1000\n",
      " - 11s - loss: 0.2400 - acc: 0.9310 - val_loss: 0.2952 - val_acc: 0.8967\n",
      "Epoch 36/1000\n",
      " - 12s - loss: 0.2357 - acc: 0.9328 - val_loss: 0.2928 - val_acc: 0.8975\n",
      "Epoch 37/1000\n",
      " - 12s - loss: 0.2331 - acc: 0.9333 - val_loss: 0.2905 - val_acc: 0.8978\n",
      "Epoch 38/1000\n",
      " - 13s - loss: 0.2290 - acc: 0.9334 - val_loss: 0.2883 - val_acc: 0.8980\n",
      "Epoch 39/1000\n",
      " - 14s - loss: 0.2261 - acc: 0.9349 - val_loss: 0.2864 - val_acc: 0.8984\n",
      "Epoch 40/1000\n",
      " - 12s - loss: 0.2243 - acc: 0.9354 - val_loss: 0.2842 - val_acc: 0.8984\n",
      "Epoch 41/1000\n",
      " - 12s - loss: 0.2216 - acc: 0.9354 - val_loss: 0.2822 - val_acc: 0.8987\n",
      "Epoch 42/1000\n",
      " - 12s - loss: 0.2175 - acc: 0.9374 - val_loss: 0.2805 - val_acc: 0.8989\n",
      "Epoch 43/1000\n",
      " - 12s - loss: 0.2159 - acc: 0.9373 - val_loss: 0.2786 - val_acc: 0.8991\n",
      "Epoch 44/1000\n",
      " - 12s - loss: 0.2125 - acc: 0.9387 - val_loss: 0.2771 - val_acc: 0.8994\n",
      "Epoch 45/1000\n",
      " - 12s - loss: 0.2111 - acc: 0.9391 - val_loss: 0.2756 - val_acc: 0.9000\n",
      "Epoch 46/1000\n",
      " - 12s - loss: 0.2082 - acc: 0.9396 - val_loss: 0.2741 - val_acc: 0.9005\n",
      "Epoch 47/1000\n",
      " - 12s - loss: 0.2058 - acc: 0.9398 - val_loss: 0.2726 - val_acc: 0.9007\n",
      "Epoch 48/1000\n",
      " - 12s - loss: 0.2044 - acc: 0.9405 - val_loss: 0.2712 - val_acc: 0.9010\n",
      "Epoch 49/1000\n",
      " - 12s - loss: 0.2020 - acc: 0.9406 - val_loss: 0.2696 - val_acc: 0.9014\n",
      "Epoch 50/1000\n",
      " - 12s - loss: 0.2003 - acc: 0.9417 - val_loss: 0.2683 - val_acc: 0.9013\n",
      "Epoch 51/1000\n",
      " - 12s - loss: 0.1981 - acc: 0.9422 - val_loss: 0.2671 - val_acc: 0.9016\n",
      "Epoch 52/1000\n",
      " - 12s - loss: 0.1958 - acc: 0.9432 - val_loss: 0.2659 - val_acc: 0.9017\n",
      "Epoch 53/1000\n",
      " - 12s - loss: 0.1935 - acc: 0.9445 - val_loss: 0.2648 - val_acc: 0.9019\n",
      "Epoch 54/1000\n",
      " - 12s - loss: 0.1924 - acc: 0.9450 - val_loss: 0.2637 - val_acc: 0.9017\n",
      "Epoch 55/1000\n",
      " - 12s - loss: 0.1899 - acc: 0.9447 - val_loss: 0.2626 - val_acc: 0.9022\n",
      "Epoch 56/1000\n",
      " - 12s - loss: 0.1895 - acc: 0.9432 - val_loss: 0.2616 - val_acc: 0.9026\n",
      "Epoch 57/1000\n",
      " - 12s - loss: 0.1864 - acc: 0.9458 - val_loss: 0.2605 - val_acc: 0.9026\n",
      "Epoch 58/1000\n",
      " - 12s - loss: 0.1856 - acc: 0.9451 - val_loss: 0.2597 - val_acc: 0.9030\n",
      "Epoch 59/1000\n",
      " - 12s - loss: 0.1834 - acc: 0.9460 - val_loss: 0.2587 - val_acc: 0.9030\n",
      "Epoch 60/1000\n",
      " - 12s - loss: 0.1823 - acc: 0.9471 - val_loss: 0.2579 - val_acc: 0.9032\n",
      "Epoch 61/1000\n",
      " - 12s - loss: 0.1808 - acc: 0.9472 - val_loss: 0.2570 - val_acc: 0.9032\n",
      "Epoch 62/1000\n",
      " - 12s - loss: 0.1791 - acc: 0.9493 - val_loss: 0.2561 - val_acc: 0.9033\n",
      "Epoch 63/1000\n",
      " - 12s - loss: 0.1777 - acc: 0.9478 - val_loss: 0.2554 - val_acc: 0.9034\n",
      "Epoch 64/1000\n",
      " - 12s - loss: 0.1764 - acc: 0.9484 - val_loss: 0.2546 - val_acc: 0.9039\n",
      "Epoch 65/1000\n",
      " - 12s - loss: 0.1749 - acc: 0.9486 - val_loss: 0.2538 - val_acc: 0.9042\n",
      "Epoch 66/1000\n",
      " - 12s - loss: 0.1729 - acc: 0.9497 - val_loss: 0.2532 - val_acc: 0.9042\n",
      "Epoch 67/1000\n",
      " - 12s - loss: 0.1715 - acc: 0.9512 - val_loss: 0.2525 - val_acc: 0.9044\n",
      "Epoch 68/1000\n",
      " - 12s - loss: 0.1701 - acc: 0.9510 - val_loss: 0.2518 - val_acc: 0.9047\n",
      "Epoch 69/1000\n",
      " - 12s - loss: 0.1700 - acc: 0.9505 - val_loss: 0.2511 - val_acc: 0.9051\n",
      "Epoch 70/1000\n",
      " - 12s - loss: 0.1688 - acc: 0.9516 - val_loss: 0.2506 - val_acc: 0.9052\n",
      "Epoch 71/1000\n",
      " - 12s - loss: 0.1667 - acc: 0.9514 - val_loss: 0.2499 - val_acc: 0.9050\n",
      "Epoch 72/1000\n",
      " - 12s - loss: 0.1654 - acc: 0.9532 - val_loss: 0.2493 - val_acc: 0.9050\n",
      "Epoch 73/1000\n",
      " - 12s - loss: 0.1640 - acc: 0.9522 - val_loss: 0.2488 - val_acc: 0.9052\n",
      "Epoch 74/1000\n",
      " - 12s - loss: 0.1628 - acc: 0.9533 - val_loss: 0.2482 - val_acc: 0.9050\n",
      "Epoch 75/1000\n",
      " - 12s - loss: 0.1617 - acc: 0.9531 - val_loss: 0.2476 - val_acc: 0.9054\n",
      "Epoch 76/1000\n",
      " - 11s - loss: 0.1608 - acc: 0.9540 - val_loss: 0.2471 - val_acc: 0.9054\n",
      "Epoch 77/1000\n",
      " - 12s - loss: 0.1602 - acc: 0.9528 - val_loss: 0.2467 - val_acc: 0.9054\n",
      "Epoch 78/1000\n",
      " - 12s - loss: 0.1584 - acc: 0.9540 - val_loss: 0.2462 - val_acc: 0.9057\n",
      "Epoch 79/1000\n",
      " - 12s - loss: 0.1565 - acc: 0.9556 - val_loss: 0.2457 - val_acc: 0.9059\n",
      "Epoch 80/1000\n",
      " - 12s - loss: 0.1566 - acc: 0.9542 - val_loss: 0.2453 - val_acc: 0.9059\n",
      "Epoch 81/1000\n",
      " - 12s - loss: 0.1567 - acc: 0.9540 - val_loss: 0.2449 - val_acc: 0.9059\n",
      "Epoch 82/1000\n",
      " - 12s - loss: 0.1543 - acc: 0.9551 - val_loss: 0.2444 - val_acc: 0.9060\n",
      "Epoch 83/1000\n",
      " - 12s - loss: 0.1539 - acc: 0.9560 - val_loss: 0.2440 - val_acc: 0.9061\n",
      "Epoch 84/1000\n",
      " - 12s - loss: 0.1523 - acc: 0.9569 - val_loss: 0.2437 - val_acc: 0.9057\n",
      "Epoch 85/1000\n",
      " - 12s - loss: 0.1512 - acc: 0.9566 - val_loss: 0.2433 - val_acc: 0.9057\n",
      "Epoch 86/1000\n",
      " - 12s - loss: 0.1499 - acc: 0.9565 - val_loss: 0.2429 - val_acc: 0.9056\n",
      "Epoch 87/1000\n",
      " - 12s - loss: 0.1489 - acc: 0.9583 - val_loss: 0.2427 - val_acc: 0.9060\n",
      "Epoch 88/1000\n",
      " - 12s - loss: 0.1487 - acc: 0.9572 - val_loss: 0.2422 - val_acc: 0.9060\n",
      "Epoch 89/1000\n",
      " - 12s - loss: 0.1477 - acc: 0.9564 - val_loss: 0.2418 - val_acc: 0.9062\n",
      "Epoch 90/1000\n",
      " - 13s - loss: 0.1470 - acc: 0.9577 - val_loss: 0.2415 - val_acc: 0.9064\n",
      "Epoch 91/1000\n",
      " - 12s - loss: 0.1455 - acc: 0.9592 - val_loss: 0.2412 - val_acc: 0.9065\n",
      "Epoch 92/1000\n",
      " - 12s - loss: 0.1454 - acc: 0.9586 - val_loss: 0.2410 - val_acc: 0.9061\n",
      "Epoch 93/1000\n",
      " - 12s - loss: 0.1436 - acc: 0.9602 - val_loss: 0.2407 - val_acc: 0.9062\n",
      "Epoch 94/1000\n",
      " - 11s - loss: 0.1433 - acc: 0.9581 - val_loss: 0.2405 - val_acc: 0.9064\n",
      "Epoch 95/1000\n",
      " - 12s - loss: 0.1422 - acc: 0.9596 - val_loss: 0.2403 - val_acc: 0.9062\n",
      "Epoch 96/1000\n",
      " - 11s - loss: 0.1419 - acc: 0.9590 - val_loss: 0.2399 - val_acc: 0.9062\n",
      "Epoch 97/1000\n",
      " - 12s - loss: 0.1405 - acc: 0.9596 - val_loss: 0.2397 - val_acc: 0.9061\n",
      "Epoch 98/1000\n",
      " - 12s - loss: 0.1404 - acc: 0.9592 - val_loss: 0.2393 - val_acc: 0.9066\n",
      "Epoch 99/1000\n",
      " - 12s - loss: 0.1394 - acc: 0.9604 - val_loss: 0.2391 - val_acc: 0.9064\n",
      "Epoch 100/1000\n",
      " - 12s - loss: 0.1384 - acc: 0.9611 - val_loss: 0.2389 - val_acc: 0.9064\n",
      "Epoch 101/1000\n",
      " - 12s - loss: 0.1389 - acc: 0.9605 - val_loss: 0.2386 - val_acc: 0.9068\n",
      "Epoch 102/1000\n",
      " - 12s - loss: 0.1372 - acc: 0.9610 - val_loss: 0.2384 - val_acc: 0.9065\n",
      "Epoch 103/1000\n",
      " - 12s - loss: 0.1363 - acc: 0.9610 - val_loss: 0.2383 - val_acc: 0.9064\n",
      "Epoch 104/1000\n",
      " - 12s - loss: 0.1360 - acc: 0.9622 - val_loss: 0.2381 - val_acc: 0.9065\n",
      "Epoch 105/1000\n",
      " - 12s - loss: 0.1337 - acc: 0.9634 - val_loss: 0.2380 - val_acc: 0.9065\n",
      "Epoch 106/1000\n",
      " - 12s - loss: 0.1341 - acc: 0.9615 - val_loss: 0.2377 - val_acc: 0.9066\n",
      "Epoch 107/1000\n",
      " - 11s - loss: 0.1343 - acc: 0.9619 - val_loss: 0.2375 - val_acc: 0.9066\n",
      "Epoch 108/1000\n",
      " - 12s - loss: 0.1320 - acc: 0.9637 - val_loss: 0.2372 - val_acc: 0.9068\n",
      "Epoch 109/1000\n",
      " - 12s - loss: 0.1318 - acc: 0.9627 - val_loss: 0.2371 - val_acc: 0.9068\n",
      "Epoch 110/1000\n",
      " - 12s - loss: 0.1315 - acc: 0.9626 - val_loss: 0.2372 - val_acc: 0.9066\n",
      "Epoch 111/1000\n",
      " - 12s - loss: 0.1303 - acc: 0.9633 - val_loss: 0.2371 - val_acc: 0.9065\n",
      "Epoch 112/1000\n",
      " - 13s - loss: 0.1294 - acc: 0.9647 - val_loss: 0.2366 - val_acc: 0.9068\n",
      "Epoch 113/1000\n",
      " - 12s - loss: 0.1280 - acc: 0.9658 - val_loss: 0.2367 - val_acc: 0.9068\n",
      "Epoch 114/1000\n",
      " - 12s - loss: 0.1277 - acc: 0.9652 - val_loss: 0.2363 - val_acc: 0.9068\n",
      "Epoch 115/1000\n",
      " - 12s - loss: 0.1271 - acc: 0.9643 - val_loss: 0.2363 - val_acc: 0.9066\n",
      "Epoch 116/1000\n",
      " - 12s - loss: 0.1269 - acc: 0.9657 - val_loss: 0.2361 - val_acc: 0.9068\n",
      "Epoch 117/1000\n",
      " - 12s - loss: 0.1267 - acc: 0.9650 - val_loss: 0.2361 - val_acc: 0.9064\n",
      "Epoch 118/1000\n",
      " - 12s - loss: 0.1248 - acc: 0.9663 - val_loss: 0.2360 - val_acc: 0.9065\n",
      "Epoch 119/1000\n",
      " - 12s - loss: 0.1240 - acc: 0.9662 - val_loss: 0.2357 - val_acc: 0.9064\n",
      "Epoch 120/1000\n",
      " - 12s - loss: 0.1251 - acc: 0.9663 - val_loss: 0.2357 - val_acc: 0.9064\n",
      "Epoch 121/1000\n",
      " - 12s - loss: 0.1242 - acc: 0.9657 - val_loss: 0.2356 - val_acc: 0.9062\n",
      "Epoch 122/1000\n",
      " - 11s - loss: 0.1240 - acc: 0.9667 - val_loss: 0.2355 - val_acc: 0.9061\n",
      "Epoch 123/1000\n",
      " - 12s - loss: 0.1220 - acc: 0.9675 - val_loss: 0.2352 - val_acc: 0.9062\n",
      "Epoch 124/1000\n",
      " - 12s - loss: 0.1213 - acc: 0.9666 - val_loss: 0.2352 - val_acc: 0.9060\n",
      "Epoch 125/1000\n",
      " - 12s - loss: 0.1215 - acc: 0.9658 - val_loss: 0.2352 - val_acc: 0.9059\n",
      "Epoch 126/1000\n",
      " - 12s - loss: 0.1199 - acc: 0.9679 - val_loss: 0.2351 - val_acc: 0.9058\n",
      "Epoch 127/1000\n",
      " - 12s - loss: 0.1197 - acc: 0.9672 - val_loss: 0.2350 - val_acc: 0.9060\n",
      "Epoch 128/1000\n",
      " - 12s - loss: 0.1186 - acc: 0.9682 - val_loss: 0.2349 - val_acc: 0.9059\n",
      "Epoch 129/1000\n",
      " - 12s - loss: 0.1181 - acc: 0.9690 - val_loss: 0.2349 - val_acc: 0.9058\n",
      "Epoch 130/1000\n",
      " - 12s - loss: 0.1179 - acc: 0.9685 - val_loss: 0.2348 - val_acc: 0.9058\n",
      "Epoch 131/1000\n",
      " - 12s - loss: 0.1176 - acc: 0.9692 - val_loss: 0.2348 - val_acc: 0.9057\n",
      "Epoch 132/1000\n",
      " - 12s - loss: 0.1168 - acc: 0.9673 - val_loss: 0.2345 - val_acc: 0.9057\n",
      "Epoch 133/1000\n",
      " - 13s - loss: 0.1165 - acc: 0.9683 - val_loss: 0.2348 - val_acc: 0.9057\n",
      "Epoch 134/1000\n",
      " - 12s - loss: 0.1154 - acc: 0.9690 - val_loss: 0.2345 - val_acc: 0.9056\n",
      "Epoch 135/1000\n",
      " - 12s - loss: 0.1151 - acc: 0.9693 - val_loss: 0.2344 - val_acc: 0.9056\n",
      "Epoch 136/1000\n",
      " - 12s - loss: 0.1159 - acc: 0.9696 - val_loss: 0.2344 - val_acc: 0.9056\n",
      "Epoch 137/1000\n",
      " - 12s - loss: 0.1146 - acc: 0.9694 - val_loss: 0.2344 - val_acc: 0.9054\n",
      "Epoch 138/1000\n",
      " - 12s - loss: 0.1151 - acc: 0.9689 - val_loss: 0.2343 - val_acc: 0.9055\n",
      "Epoch 139/1000\n",
      " - 12s - loss: 0.1136 - acc: 0.9702 - val_loss: 0.2342 - val_acc: 0.9055\n",
      "Epoch 140/1000\n",
      " - 12s - loss: 0.1128 - acc: 0.9701 - val_loss: 0.2342 - val_acc: 0.9054\n",
      "Epoch 141/1000\n",
      " - 12s - loss: 0.1118 - acc: 0.9708 - val_loss: 0.2343 - val_acc: 0.9055\n",
      "Epoch 142/1000\n",
      " - 12s - loss: 0.1123 - acc: 0.9702 - val_loss: 0.2342 - val_acc: 0.9054\n",
      "Validation accuracy: 0.9054399999809265, loss: 0.23421662689208986\n",
      "Accuracy: 0.9054399999809265, Parameters: (layers=1, units=128)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 16s - loss: 0.6065 - acc: 0.8168 - val_loss: 0.5173 - val_acc: 0.8637\n",
      "Epoch 2/1000\n",
      " - 14s - loss: 0.4286 - acc: 0.8742 - val_loss: 0.3836 - val_acc: 0.8813\n",
      "Epoch 3/1000\n",
      " - 14s - loss: 0.3244 - acc: 0.8955 - val_loss: 0.3177 - val_acc: 0.8908\n",
      "Epoch 4/1000\n",
      " - 14s - loss: 0.2682 - acc: 0.9109 - val_loss: 0.2826 - val_acc: 0.8970\n",
      "Epoch 5/1000\n",
      " - 16s - loss: 0.2315 - acc: 0.9232 - val_loss: 0.2625 - val_acc: 0.9004\n",
      "Epoch 6/1000\n",
      " - 14s - loss: 0.2056 - acc: 0.9310 - val_loss: 0.2485 - val_acc: 0.9040\n",
      "Epoch 7/1000\n",
      " - 14s - loss: 0.1851 - acc: 0.9378 - val_loss: 0.2398 - val_acc: 0.9054\n",
      "Epoch 8/1000\n",
      " - 14s - loss: 0.1693 - acc: 0.9422 - val_loss: 0.2346 - val_acc: 0.9065\n",
      "Epoch 9/1000\n",
      " - 14s - loss: 0.1564 - acc: 0.9487 - val_loss: 0.2310 - val_acc: 0.9068\n",
      "Epoch 10/1000\n",
      " - 14s - loss: 0.1457 - acc: 0.9508 - val_loss: 0.2292 - val_acc: 0.9067\n",
      "Epoch 11/1000\n",
      " - 14s - loss: 0.1349 - acc: 0.9550 - val_loss: 0.2295 - val_acc: 0.9065\n",
      "Epoch 12/1000\n",
      " - 14s - loss: 0.1253 - acc: 0.9584 - val_loss: 0.2288 - val_acc: 0.9062\n",
      "Epoch 13/1000\n",
      " - 14s - loss: 0.1170 - acc: 0.9603 - val_loss: 0.2306 - val_acc: 0.9053\n",
      "Epoch 14/1000\n",
      " - 18s - loss: 0.1095 - acc: 0.9650 - val_loss: 0.2309 - val_acc: 0.9048\n",
      "Validation accuracy: 0.9048399999809266, loss: 0.23089948148727418\n",
      "Accuracy: 0.9048399999809266, Parameters: (layers=2, units=8)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 17s - loss: 0.5684 - acc: 0.8469 - val_loss: 0.4496 - val_acc: 0.8720\n",
      "Epoch 2/1000\n",
      " - 15s - loss: 0.3520 - acc: 0.8955 - val_loss: 0.3242 - val_acc: 0.8881\n",
      "Epoch 3/1000\n",
      " - 15s - loss: 0.2593 - acc: 0.9154 - val_loss: 0.2748 - val_acc: 0.8978\n",
      "Epoch 4/1000\n",
      " - 15s - loss: 0.2109 - acc: 0.9274 - val_loss: 0.2531 - val_acc: 0.9014\n",
      "Epoch 5/1000\n",
      " - 15s - loss: 0.1793 - acc: 0.9402 - val_loss: 0.2390 - val_acc: 0.9053\n",
      "Epoch 6/1000\n",
      " - 15s - loss: 0.1604 - acc: 0.9454 - val_loss: 0.2351 - val_acc: 0.9057\n",
      "Epoch 7/1000\n",
      " - 15s - loss: 0.1405 - acc: 0.9537 - val_loss: 0.2316 - val_acc: 0.9060\n",
      "Epoch 8/1000\n",
      " - 15s - loss: 0.1264 - acc: 0.9580 - val_loss: 0.2287 - val_acc: 0.9071\n",
      "Epoch 9/1000\n",
      " - 15s - loss: 0.1131 - acc: 0.9647 - val_loss: 0.2306 - val_acc: 0.9055\n",
      "Epoch 10/1000\n",
      " - 15s - loss: 0.1050 - acc: 0.9665 - val_loss: 0.2341 - val_acc: 0.9040\n",
      "Validation accuracy: 0.9039599999809265, loss: 0.2341313259124756\n",
      "Accuracy: 0.9039599999809265, Parameters: (layers=2, units=16)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 20s - loss: 0.5170 - acc: 0.8526 - val_loss: 0.3786 - val_acc: 0.8804\n",
      "Epoch 2/1000\n",
      " - 19s - loss: 0.2822 - acc: 0.9110 - val_loss: 0.2784 - val_acc: 0.8964\n",
      "Epoch 3/1000\n",
      " - 18s - loss: 0.2080 - acc: 0.9293 - val_loss: 0.2461 - val_acc: 0.9034\n",
      "Epoch 4/1000\n",
      " - 18s - loss: 0.1679 - acc: 0.9431 - val_loss: 0.2342 - val_acc: 0.9066\n",
      "Epoch 5/1000\n",
      " - 18s - loss: 0.1401 - acc: 0.9540 - val_loss: 0.2297 - val_acc: 0.9072\n",
      "Epoch 6/1000\n",
      " - 18s - loss: 0.1223 - acc: 0.9598 - val_loss: 0.2296 - val_acc: 0.9066\n",
      "Epoch 7/1000\n",
      " - 18s - loss: 0.1072 - acc: 0.9652 - val_loss: 0.2324 - val_acc: 0.9042\n",
      "Epoch 8/1000\n",
      " - 19s - loss: 0.0927 - acc: 0.9715 - val_loss: 0.2384 - val_acc: 0.9023\n",
      "Validation accuracy: 0.9023199999809265, loss: 0.2384400800704956\n",
      "Accuracy: 0.9023199999809265, Parameters: (layers=2, units=32)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 23s - loss: 0.4686 - acc: 0.8591 - val_loss: 0.3215 - val_acc: 0.8876\n",
      "Epoch 2/1000\n",
      " - 21s - loss: 0.2325 - acc: 0.9210 - val_loss: 0.2506 - val_acc: 0.9012\n",
      "Epoch 3/1000\n",
      " - 21s - loss: 0.1692 - acc: 0.9421 - val_loss: 0.2342 - val_acc: 0.9063\n",
      "Epoch 4/1000\n",
      " - 22s - loss: 0.1351 - acc: 0.9547 - val_loss: 0.2308 - val_acc: 0.9056\n",
      "Epoch 5/1000\n",
      " - 22s - loss: 0.1116 - acc: 0.9626 - val_loss: 0.2314 - val_acc: 0.9048\n",
      "Epoch 6/1000\n",
      " - 21s - loss: 0.0931 - acc: 0.9711 - val_loss: 0.2392 - val_acc: 0.9028\n",
      "Validation accuracy: 0.9027599999809265, loss: 0.23924059827804564\n",
      "Accuracy: 0.9027599999809265, Parameters: (layers=2, units=64)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 34s - loss: 0.4199 - acc: 0.8774 - val_loss: 0.2803 - val_acc: 0.8953\n",
      "Epoch 2/1000\n",
      " - 30s - loss: 0.1948 - acc: 0.9308 - val_loss: 0.2376 - val_acc: 0.9040\n",
      "Epoch 3/1000\n",
      " - 30s - loss: 0.1402 - acc: 0.9499 - val_loss: 0.2331 - val_acc: 0.9046\n",
      "Epoch 4/1000\n",
      " - 29s - loss: 0.1103 - acc: 0.9626 - val_loss: 0.2360 - val_acc: 0.9027\n",
      "Epoch 5/1000\n",
      " - 30s - loss: 0.0873 - acc: 0.9730 - val_loss: 0.2464 - val_acc: 0.9004\n",
      "Validation accuracy: 0.9003599999809265, loss: 0.24635980993270873\n",
      "Accuracy: 0.9003599999809265, Parameters: (layers=2, units=128)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 15s - loss: 0.5936 - acc: 0.7653 - val_loss: 0.4395 - val_acc: 0.8756\n",
      "Epoch 2/1000\n",
      " - 13s - loss: 0.3500 - acc: 0.8783 - val_loss: 0.2755 - val_acc: 0.8985\n",
      "Epoch 3/1000\n",
      " - 13s - loss: 0.2364 - acc: 0.9180 - val_loss: 0.2375 - val_acc: 0.9054\n",
      "Epoch 4/1000\n",
      " - 13s - loss: 0.1829 - acc: 0.9389 - val_loss: 0.2294 - val_acc: 0.9075\n",
      "Epoch 5/1000\n",
      " - 13s - loss: 0.1514 - acc: 0.9504 - val_loss: 0.2389 - val_acc: 0.9040\n",
      "Epoch 6/1000\n",
      " - 12s - loss: 0.1233 - acc: 0.9604 - val_loss: 0.2464 - val_acc: 0.9027\n",
      "Validation accuracy: 0.902680000038147, loss: 0.2463792050075531\n",
      "Accuracy: 0.902680000038147, Parameters: (layers=3, units=8)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 19s - loss: 0.5163 - acc: 0.8095 - val_loss: 0.3230 - val_acc: 0.8903\n",
      "Epoch 2/1000\n",
      " - 15s - loss: 0.2401 - acc: 0.9157 - val_loss: 0.2392 - val_acc: 0.9048\n",
      "Epoch 3/1000\n",
      " - 15s - loss: 0.1657 - acc: 0.9417 - val_loss: 0.2349 - val_acc: 0.9066\n",
      "Epoch 4/1000\n",
      " - 15s - loss: 0.1240 - acc: 0.9591 - val_loss: 0.2498 - val_acc: 0.9009\n",
      "Epoch 5/1000\n",
      " - 15s - loss: 0.1006 - acc: 0.9665 - val_loss: 0.2642 - val_acc: 0.8998\n",
      "Validation accuracy: 0.89984, loss: 0.26419126218795774\n",
      "Accuracy: 0.89984, Parameters: (layers=3, units=16)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 20s - loss: 0.4380 - acc: 0.8515 - val_loss: 0.2487 - val_acc: 0.8999\n",
      "Epoch 2/1000\n",
      " - 18s - loss: 0.1761 - acc: 0.9346 - val_loss: 0.2372 - val_acc: 0.9012\n",
      "Epoch 3/1000\n",
      " - 19s - loss: 0.1176 - acc: 0.9587 - val_loss: 0.2525 - val_acc: 0.9012\n",
      "Epoch 4/1000\n",
      " - 18s - loss: 0.0845 - acc: 0.9707 - val_loss: 0.2976 - val_acc: 0.8915\n",
      "Validation accuracy: 0.891520000038147, loss: 0.2976181831455231\n",
      "Accuracy: 0.891520000038147, Parameters: (layers=3, units=32)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 25s - loss: 0.3538 - acc: 0.8752 - val_loss: 0.2401 - val_acc: 0.9008\n",
      "Epoch 2/1000\n",
      " - 23s - loss: 0.1408 - acc: 0.9463 - val_loss: 0.2478 - val_acc: 0.9015\n",
      "Epoch 3/1000\n",
      " - 21s - loss: 0.0942 - acc: 0.9661 - val_loss: 0.2854 - val_acc: 0.8948\n",
      "Validation accuracy: 0.8948399999809266, loss: 0.28544124004364013\n",
      "Accuracy: 0.8948399999809266, Parameters: (layers=3, units=64)\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 31s - loss: 0.3067 - acc: 0.8865 - val_loss: 0.2415 - val_acc: 0.9004\n",
      "Epoch 2/1000\n",
      " - 30s - loss: 0.1267 - acc: 0.9531 - val_loss: 0.2696 - val_acc: 0.8962\n",
      "Epoch 3/1000\n",
      " - 30s - loss: 0.0857 - acc: 0.9695 - val_loss: 0.3043 - val_acc: 0.8890\n",
      "Validation accuracy: 0.8890400000190735, loss: 0.304262776222229\n",
      "Accuracy: 0.8890400000190735, Parameters: (layers=3, units=128)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsnXl8JHWZ/z99d5LOPclM7qPTyWQymXuSGXRlEIWdWRhXf4DCLseOiD/lGHBwFxEQWRBcHVBB3PVYURBkV1hXXysIsj/dl+vc95Wrk859dJK+76N+f8RvUdWp6q6qrk46PfV+veY1M0l3dVV39aeeer7P83lUFEVBQUFBQWH5US/3DigoKCgoLKAIsoKCgkKWoAiygoKCQpagCLKCgoJClqAIsoKCgkKWoAiygoKCQpagCLKCgoJClqAIsoKCgkKWoAiygoKCQpagFfl4pa1PQUFBQTwqIQ9SImQFBQWFLEERZAUFBYUsQRFkBQUFhSxBEWQFBQWFLEERZAUFBYUsQRFkBQUFhSxBEWQFBQWFLEERZAUFBYUsQRFkBQUFhSxBEWQFBQWFLEERZAUFBYUsQRFkBQUFhSxBrLmQgkJSKIpCLBYDAGg0GqhUgjxVFBQUoAiygkzE43HEYjFEo1GEQiH65yqVChqNhv6jVquhVquhUqkUsVZQSEARZIW0iMfjiEajdFSsUqlowaWoBbdWItSJzwsEAigtLYVWq1WEWkEBiiArSICiKFAUhUgkgng8DgC0kBIRJj9j/s0kHA7DZrOhoKAA4XCY9Ry1Wg2NRqMItcJlhyLICoKhKIqOiBOFWCzkORqNhvM1YrEYS6jJYxPTH4pQK+QSiiArpCRRiIkIpiOEidE08+dc22UKNUVR9GMmJydRW1tLC3RinlpBYSWhCLICL6RiYnJyEiaTCXl5ebJFpHyCnOzxXK87MTGB6urqRUINgBZorgVFBYVsRBFkhUUQIY5Go6AoCrOzs9DpdMjPz5ftNcQKcrLtqNWLy+mZee5wOKwItcKKQBFkBRqKouiKCRJtErEiOWMhZIOwJUt9AEA0GkUkEgEAzMzMID8/H0VFRYpQKywriiAr0EJMStMSo850olnXw5+m/138tR/Jss104Kr88Pl80Ov1ANhCzXxOYtWH0vSikAkUQb6MYTZzAPxRpdgIGWALMfNnRJSXS5D54FuoTKylJncOFEUlTX0oYq0gBUWQL0O4mjmSCYgY8Zz94m1Jf0+E2vSP388qQeaDr5aaq+mlt7cXbW1t9B2GVqtVhFpBFIogXyYka+ZIhZAI2f7graL2x/voXbAAwLYfpXpoxkmszhACX+qDpHri8TirhZw8Vml6UUiGIsg5DrOGuK+vD6tXr0ZRUZEoAUgWIYsV4kSYaYyVDHl/knUnKk0vCqlQBDlH4WrmYP5bDFwRcrpCzISkMVayMAuJsoU0vTidTrjdbtTX1wMAZ45aqfzIXRRBzjESa4iZi1VSFucAdoQspxAn4nr409A8/G3k5+dz1hZnCikpCzm3kSjU8XgcGo2GtzsRUGqpcxVFkHMELiFOFDWpgqxWq2H41pdgl2tnkxD72n54APT+9WeRl5eHgoIC+s9SC7UY5BB1YEGMyTEmi6iVppfcRBHkFQ5fMwcXUgTZ/uCtMMixoyJp++W/AADCDzwDn8+H2dlZ+P1+UBTFEupYLMYSMaksZ4QsdjtCml7GxsYQj8dRVVUFQBHqlYIiyCuUVM0cXIgR5EymJsSgf+4hVDByy/F4HMFgED6fDz6fD+FwGCdOnGAJtclkQkFBAfLy8gQJdTaV36VzcWEuKBJhJ6kPgLvphQgzV4mewtKjCPIKI7GcSsxKfCbK15YC5qKfWq1Gfn4+8vPzUVFRAbvdju3bt7OE2uv1YmZmBoFAAAAWpT6ECrUYSKOIHNuRK/Wh1S58vYXWUitNL8uPIsgrBGYzx+HDh7Fz505ZqiUI2SjEiSQrkUsUagKZTEIiai6hDgQCCAaDKCwslCyqckXZcqRfhG5HiFBPTU3B5/OhoaGBfqzS9JI5FEHOYviaOZh/i0GtVtPdeYSVIMRMxJbIqdVqOjJmwhTq2dlZTExMYGRkBIC0iFrOHPJSCTIfiakPIr5EqJWml8yhCHIWIudkDiZqtRqRSGTFiTAXtFfGx/+vpOczhXp+fh7V1dUoKioSFFFzCbWcqYbEKSpStyO3sKfT9JIo1opQc6MIchYhZjKHFAHQPfsP0Mm1s1lC63/8M7B9e1rbYL6XQiLqRKHOz8+HTqdDOByG3++njfyl7styR8hMYrEYnYtORqqml2AwiEuXLmHDhg2sBUel6YWNIshZQLJmDi6Yq+hCyIWIOBlL0emXSqjn5ubgcDhgtVoRDAYBLAh1YkSd6jNbyhzyUmyHeR4T8QX4R3IBl3eJniLIy4iQZg4uNBqNoC9KrgtxIsvRgk2EmqIoeL1erFu3DsCCkPn9fvh8Png8HkxNTSEQCEClUiUV6kw0mKRDLBaTJYWSuB2l6YUbRZCXAYqi4Pf7AYA+ocR8ecjiHN+t5OUmxImINSzKROu0Wq2GyWSCyWRiPS6VUJPFsvz8/LRSH9kSIROi0WjaqQ+ynXPnzqG+vp4eKZZLQq0I8hLCbObo6+vDmjVrUFZWJno7Go1mUbUEoAgxk+WIloVamSYTapLysFqtiyJqk8kkWKizPUIWC3NBMRqNQq/XC2p6+fa3v42HHnoIOt3KWD1RBHkJ4JrMQdIOUkisJ1aEmJ+lsvdMN8omQp2Xl4fKykqUlJQAeF+ovV4v3G43JicnEQgE6LprZurDaDTS+yBXtUa2CDITZrSdqpb6jTfewJe//GVZXncpUAQ5gySbzMEX5QqBPFcRYmGkipaX2+2NSWJkyxdRx2IxOvXhcrkwMTGBYDBIC7XP58P8/DyKi4tZQp3u/khFTkEW2vSS6FG9ElAEWWaETuZIR5BXv/R1hFM/TCGBTKYx5OrUEyrsGo0GhYWFKCwsZP2cCLXL5aJbyJlCzRdR85GNETIgTmQVQb4MEdvMIUWQz12zCwDQdsMH4eobTmt/L2cykcbIluoIItRarRZms5nep1QRNZ9Qx2Ix2SJkMtl7qYhGo7JeBJYCRZDTREwzBxONRrNoEYILIsJMen/xR/rfbTd8kP63ItLCIdFy9MA/0bn9dFhK+02hMLeTKqLmE+pAIACn04nCwkIYDAbJ+xaNRpGXl5fW8QDi7kRcLheKi4vTfs2lRBFkiXDZX4ppB9VoNHQDARdcQswFEee2Gz6I4tYGzscoQs2P9uDfw7Pn73Du3DnodDo6SiQWnkKjOjkj5KW8xU4l1A6HAy6XC5OTk7RQM6PpgoICQUIttOMvFWLuINxuN704ulJQBFkkzGaOS5cupVW6lhiZCRVhLviiZgCKUKeg8zc/BgAUPPEvdGv07OwsbDYbIpEItFotLdDkT2IZVbaZC6Wb0yZCrdPp0NLSQv88FovR75HD4cDY2BhCoVBKoZYrhyy0nhkAnE6nEiHnKlyTObRabVqVEiTXnI4Qc8GMmpPBJdSXs0j7HvssAKA6Ib8ciUTg9Xrh8/kwPT0Nn89H18IS8YlEIrIs7MlV1ZApNBoNioqKUFRUxPo5n1BrNBrk5+fD4/GgsLAQer0+7dSHUEFWUhY5SLLJHOlUSox/6hMAgHPy7CYnyaJmPpRoevGin06nQ2lpKUpLS+mfURSFcDhMi9D8/Dz8fj/m5uZgNBoXzQIUGh3KFWkvdWVBKqHu6emBx+PB7OwsS6iZKSK9Xi9r6sPlcikpi1yBq5kj8WTRarWiFoTkjoTFIDRq5uNyi6ZTlcipVCoYDAYYDAaUlZXBYDDA7/ejoaEBoVCIjqiJUMfjcUEjpuRc1MsGiFDrdDqYzWY61RONRunFRK6ImpkiYgq1mMoJRZBzgGTNHIlw5YG5WE4hTiRdYWZyOUTTQmuXSapBpVLBaDTCaDRi1apV9O8pikpq30nEJxqNpi3Ici0MyrnAmJhD1mq1nBE1EWqv14u5uTmMjo7SQl1QUECn+UKhUMqI2ul0oq6uTpb9XyoUQYbwZo5EtFrtoskJhGwSYS6kpDOEkovRdLq1y8STItmIKY/Hg2AwiBMnTtCPZ0aKQnOvcgryUnbXAcmF2ufzYXx8HKFQCJcuXUI4HKaFmvmHCLVSZbHCSHcyB1eEfGHP1YhHpeWVlws5o2Y+ciGaThYtS001MKsTKisrMTc3h+3bt7Pqg51OJ+uWnik+JpMJOp2O9drZ2O6cLlqtFsXFxXC73SgtLUVVVRWA94Xa5/Nhbm4OIyMjCIVCeOSRR6BSqRAMBmEymdDR0YE1a9Ys2u7bb7+N/fv3IxaL4c4778RDDz3E+v3w8DD27dsHu92OsrIyvPLKK6itrQUA/OQnP8GTTz4JAHjkkUdw++23s567d+9eDA4O4vz588KPU9S7kiOQ0rVYLIZz585h/fr1kkbKMKssLuy5mv65WrtwEldtqqd/Nn58SIY9zyyZjJr5WIlCzRUty5375asPTrylHxkZQTgchlar5RVoqchZ8SHXexONRmE0Gun/E6FOrKZ4/fXXcd9996G0tBT/9V//he9///t4/fXXWY+JxWK4++678e6776K2thbbt2/H3r17aU9rAHjwwQdx22234fbbb8d///d/40tf+hJefvllzM/P46tf/SqOHz8OlUqFrVu3Yu/evfTC75tvvrnIf0QIl5UgcxnCezweySeLVqsF9eC9uMDz+8nTC0MzqzbVo2ZbE+t32S7QSxE1JyPb0x6J0fJSLcbx3dJHIhE6UrTb7fB4PDh69Ch0Ot2iGmqhVQrZFCEThJa9lZWVIRKJYN++ffTE7ESOHj2KlpYWNDc3AwA+9alP4T//8z9Zgnzx4kU8++yzAICrrroKf/3Xfw0A+O1vf4uPfvSjdA/CRz/6Ubz99tu4+eab4fV68eyzz+L73/8+brrpJlHHd1kIcqrJHFK/TCOf/GtBj2MKMyFRoIHsFOnliJr5yMZomggz9fmvLmv9sE6nQ0lJCUpKSlBcXAyVSoV169YhHA7TFR+Tk5Pw+XyIxWIwGAysig+u0jw5B67KGSGLKXtjliomMj4+zlr0q62txZEjR1iP2bhxI958803s378f//Ef/wGPx4O5uTnO546PjwMAHn30URw4cIA20BdDTgtyshpiAmnQEHPiMdMTYuASZibZLtLLHTXzkQ3RdOGLX4H/3ieX9DX5YKYa9Ho9ysrKWN2kFEUhFAotqqFmluYVFBQgFovJIqRyRtpituXz+RbNQBTLN7/5Tdxzzz146aWX8KEPfQg1NTVJX//06dOwWq147rnnYLPZRL9eTgoyn88EF1qtFpFIRNCHLFWIE0klzEyyUaSzKWrmYzmi6fznH4ELSzulhItUuV9maV55eTn9c4qiEAwGF9VQu1wuzhpqoWItJqqVa1ukazLZ+1BTU4PR0VH6/2NjY6ipqWE9prq6Gm+++SYAwOv14o033kBJSQlqamrw+9//nvXcXbt24dChQzh+/DgaGxsRjUYxMzODXbt2sR6bDJXIdk95TF8zhJBmjkTOnDkDs9mcNAEvlxDzIUSYU7HcIp2twiyETIm0FGE+evQourq60nrd+fl5zM/PszwopDAzMwO/34/6+npWDbXX60UwGOQc2Mrlsez1ejEyMsLKzUrl+PHj2LJlS8r0EEVRuPLKK3Hq1Cnex0SjUbS2tuK9995DTU0Ntm/fjldffRUdHR30Y2ZnZ1FWVga1Wo0vf/nL0Gg0eOKJJzA/P4+tW7fi5MmTAIAtW7bgxIkTrDsRm82G6667jlRZCLp65USELKaZI5FU3XaZFmNAXMTMx3JH0ishauYjU9H0Uo2PSkTusjdmaV7i65CKD6Z1Z2K3nZzVGkLNl4TkrbVaLV544QVce+21iMVi2LdvHzo6OvDYY49h27Zt2Lt3L37/+9/jS1/6ElQqFT70oQ/hu9/9LoCFRcNHH30U27dvBwA89thjkkzGElnRETLxE5BSQ0zo7e1FeXk5q6sKAH5n2gAAKN34/mq2sdiQ5h4LQ46ImYuljqJXmjALQapICxFmiqJw/Phx+ksuFRLZNjY2prUdskiVeBufCqbREGmNDgaDixo4SImeGI4dOybo/XE4HLj11lvxhz/8QdT2M0juR8jMAYfplK4xI2QixATHGTeABWEOut7vysukOMsRMXOx1FH0So6a+ZAaTbse/jTm7vwyLURcPhZyRZJyRshSpnwkGg3Z7XZ4vV7U1dXRKQ+73U7bm6ZTmseH0+lccV16wAoXZGDxBGaxEEFOFOJEiDADi8UZyIxAE2EGMhc1L5VIZ2uFhlwIEeryHz4FABi65YFFPhYmk4nV8JAO2dapR7bD18TBdM2bmpqC1+tlleYxL2JCWYnGQkAOCHK6aLVaXGrbLeo5ieIMIOPRc6aiZi4yKdK5GDUng1OoX30OAFD45A9YOdixsTF4vV6cOHGCVdEgZnIJkH2RdiqHNr1eD71ez2lvSio+yHvj9/tx9uxZ1nuTn5+/aD9XohcyoAgytFotGk+/iZaWFoRCIdhsNoxuvVHw85niDGQ+tbGUwswkEyKd61FzIr3XPLzwj2sW/vrU357GW69uoSt8QqEQenp60NHRweq647q1J39zCV08HpelzEzOCNlgEPc9YNqbktK8YDCI3t5eWCwW1mQXv98PAHRp3tDQEAYGBhZ1M64EVrwgp1u4rtVqEQwGcenSJTgcDjQ2NuLD7tOsK26qdAaTZKkNOcV5uYSZiVzt4LkozLT4JuFb39yA3becxFuvbgHwfkSa7NaeRIzj4+Pw+XyIx+MwGo0soZZrUrScqQ85LhDRaBQ6nQ55eXnIy8tjLcTH43G6hvrMmTN46623MDY2ht/+9rdYu3YtDh48iMrKStb25DYW8vv9uPHGG2G1WqHRaHD99dfjmWeeEXWMK7rKAgCr3E0sfr8fPT09cDgc6OjowOrVqwUJvBiBJjCrNQhyCvRyCnMypIr0ShJnIeLLx/0PnqUF2e/3Y3BwEOvXrxf8/MRmDq/XC6fTSS+sMaNprhrhZFy4cAGNjY1pd7v19/dj1apVSduYheB0OjEzM4PW1taUj33hhRewevVq/M3f/A36+vrQ2trKitJjsRhaW1tZxkKvvfYaq1b6xhtvxHXXXUcbC/34xz+mjYW2bdvGMhY6ceIEDAYDjhw5gquuugrhcBhXX301Hn74YezevRu4HKosAGkRstfrxeDgIAKBAKqqqqBSqTit+fj4iPcs/e/e3l5BKY5M552zIWLmQmqqI1uj5nTElwtmlCzF80GlUtERI/FZHhgYQElJCYxGI2eNsNDJ2tm2OChmWojb7aZFuLOzc9HvM2UsdNVVVwFYyItv2bIFY2Njoo5xxQuyGNxuNwYHBxEOh2E2m2lHqJmZGcnb1Gg06LS+i9WrV9M/E1uxAcgnztkqzEzEiPRyLgLKLb7J2H3LSfzbv1hkSzWQSdmJHahM/+BU+Wm5qyzk2E62GwsRnE4nfv3rX2P//v2C9pew4gVZSEThdDphtVpBURTMZjPrgyJeFlJheiLHYjGMjo7C+PaLqK6uRn19PX0C8Yl0psR5JQgzEyEincmoeSnFN5FvfXMD7n/wLG76bD+ee0y8Q1giySJbMflpr9eLS5cuobCwMGn9dCrkzCEv5YBTscZCzP28+eabcd9999ERuFBWvCAnY35+HlarFVqtFi0tLZxlMGq1Oq3x7RqNBuFwGENDQxgfH0dNTQ127Nix6MRhpjkAboFOJc6AeIFeacLMhE+k042al1N8U/HAE3689Wp625CSauByhTt69CirosFut3PWT6fKT4tJNSQj0Zw+Gaki5EwYCxHuuusuWCwW3H///YL2lUnOCTJFUZibm8Pg4CAMBgPWrl27aOqCXESjUczPz2Nubg7Nzc3YuXOn4BPvI96zcDgcmJycxLp163g7BAH2gqDU6HklCzOTRJFOFjVns/Amg1l5IQW5cr/J5gDyeVhw5aflqvoQm0NOVoe8fft29Pf3Y2hoCDU1Nfj5z3+OV19lXwmZxkJPP/009u3bBwC49tpr8fDDD8PhcAAA3nnnHTz99NMAFiouXC4XfvjDH0o5xJUvyOSqTFEUZmZmMDQ0hIKCAnR0dKS9OsxHNBrF8PAwpqamUFxcjJqaGkm+ARqNhk53JIuguWqdAWnivBTdf0sJEWivbSGH5/v8U3BGV16HFklbENIRZTnNfLhQq9Wi8tOBQABWqzVl/XQqxKQ+PB5P0jrkTBgLjY2N4amnnsLatWuxZcvCZ3fPPffgzjvvFHyMK16QKYrC5OQkbDYbioqKsGHDBklO/UKmhkQiEQwPD2N6ehq1tbXYsWMHXC4XpqenJe07U5CZ+zEzM4OCd/8FxcXFaG5uhtFo5BVoOcQ5F4SZSYnWuSJFWS4yLch88OWnjxw5gvLy8pT106ny02JyyBRFpRT9PXv2YM+ePayfPfHEE/S/b7jhBtxwww2cz923bx8dMRNqa2vTSn8COSLIXq8XmzdvluwFQPws+JynwuEwhoeHMTMzg7q6OuzcuZM+cbhEVSjM51IUBbvdDqvViqKiImzatInVu88XQcshzrkgzL7PP8X6/0oUZbmi5OUSZD7UajXn1BJm/bSQ/LRYc/qVyIoXZI1GA4vFktY2+AQ5HA7DZrPBbrejvr6eJcSJz5WCRqOhpwoMDg7CZDItEmI+LBd+DY1GQ3cO/c60Ie1FwZUqzIliTFiJopzI7ltO4r9e2SRKYOUQZLlEjaIozm1x1U8DyfPTgUCAThOmqp9OxwFyOVnxggwsfLjpnECJokqqJmZnZ9HQ0MApxASpETJFUZifn4fb7cbMzIzoVEuiy12yHLTYRcGVIsx8QswkF0T5r/72NJ54IIa8vDw6d5usskGOCdjLZVCULD99/Phx6HS6pPXT+fn58Pl8GVvIzzTZc1+zjBBBJuYux48fh8lkws6dO1FbW5v0hBIbIZPUxJEjRzAzM4O8vDysX79edN472YXA5/Oh4n9fRtF//xBbR/+Arf/7R2z93z8uWhwMukL0Hy4mT4+wFgGzCSFiTCjROjO4J/LyrW9y16s/9pwGTU1NyMvLg8vlQn9/P44dO4YTJ06gp6cHY2NjcDqddE19tgiyXDXIWq0WGo0GNTU1aG1txebNm9HV1YWOjg6Ul5cjFothfHwczzzzDK688krYbDY8+uijeP311znXeN5++220tbWhpaWF029ieHgYV199NTZs2IBdu3axOu5+8pOfwGKxwGKx4Cc/+Qn98xMnTqCzsxMtLS247777JAWJSoT8Z4aGhhAMBtHY2Ii2tjbBJ7TQCJmiKMzOzmJwcBD5+fl0RPynP/1J0v5y+UD7/X5YrVb4/X60tLSgrKyMdRwf8Z6F3+9Hb28v1vzwaUycnqB/lyxyzqaIWYwQM8mFSPmGz/TirVe3sLpCSWWD1+vF9PQ0XeVw5syZRZGjGIHNti49PhLrpx955BFcd911OHjwILq6unDu3DmYzWbWexaLxXD33XezfCz27t3Lapt+8MEHcdttt9E+Fl/60pdoH4uvfvWrLB+LvXv3orS0FJ/73Ofwgx/8AN3d3dizZw/efvtt4mMhmJwQZKkEg0EMDQ1henoaq1evxubNmyV5CSSD1EVbrVbk5+dj/fr1spTjMS8EwWAQVqsVHo8HZrMZq1at4t0vIuRV3/03VDF+Pnn3TbRA8+Wdl1uYpYoxgUTK2S7MiYt7yeCqbDh69Cja2troBbO5uTnaopI5685kMsFgMHCeK9nmYyEGj8eDmpoaXH/99bj++usX/T4TPha7du2C2+3Gjh07AAC33XYbfvnLX16egixWRAOBAIaGhuByudDU1ERHD3IuApAc8cDAAJ2WkLMuWq1WIxwO49KlS3A6nWhubsa6detSHgPfhBWmQDPFGXhfoDfdez16fvrOkgtzukKcyEqOloVUXqhUKhiNRhiNxkUWlcwFs/HxcYRCIWi1Wlqgs9XHQozxUqq26Uz4WIyPj9ML7MyfiyUnBFkogUAAg4ODcLvdaG5uRnt7O1QqFSYmJhAKcedRpUAiYqPRKEiIxS7ChMNhjI2NYXZ2FuvWrcPatWtFpViSjbyKx+OIPnQQwZERVFVVIe/gQyxxXnvbgrv6Ugmz3GJMyHVR5oJvwSwSidBpDzJCKRwOg6IoWK1W+jlSfSzkapteCT4W6XJZCDLxmfV6vZyRpE6ng8/nS+s1SERstVphMBiwbt26RSc+FyRiFfJhRyIRDA0NwW63097NVVVVKZ/H9Xpc+z81NYWhoSFUVFSgq6troQzwz5Hz/Pw8Zh79v5g4PYFN915PCzOwIM6AvMKcKSFmks2inCptwSfKUtZSdDodSkpKWCI2NzcHu92O4uJieigpqRNm1gibTCbo9XregGC5BJlMGeEiEz4WNTU1rIU/rm0KIScEme9k8Pl8GBwchM/ng9lsRkdHB+dj06klBhaiymPHjokSYgLJBSc7aaPRKGw2G6anp+kyvEAggP7+ftH7mrgAymxIKSkpwdatWznH7ahUKjjufhxb/5xni/386/Tv5BbnpRBjQjaLciq4RFmOkjdg4Zw2GAxYtWrVorQHiaYdDgdGR0cRDodZdp9EqEmd/VKPk/J4PEld1jLhY1FWVoaioiIcPnwY3d3d+OlPf4p7771X9HHmhCAn4vP5YLVaEQgE0NzcnHSRC5BuwelwODAwMIBIJIL29vakV2U+klVpRKNRjIyMYHJyclGHYLrTtoGFKGhgYAAFBQUpG1ISX0/zqX8AwBZmYLE4ixHmpRRiJtkqykIW9xJFOdP1w2q1GoWFhYvqfCORCLxeL7xeLyYnJ+H1eunzhczGI2kPKRcMOVMWmfCxAIAXX3wRd9xxBwKBAHbv3i16QQ/IgRFOwMLJQ04Iq9WKUChEG9AL+fADgQB6enqwefNmQa9HhFin08FsNsNqtcJisUhatDtz5gzMZjMrqia+ymNjY6itrUVdXd2i6CAcDuPMmTP0iSGG//mf/0FeXh70ej1aWloE7bfH48HQ0BA2bOCuk00UZiZChHm5xJhJNoqy0GoLIspYgegNAAAgAElEQVThcBgXL17Epk2b0nrdiYkJxONx1kKVWEgOmqIoaLVaeL1eBAIBqNVqliscSXskw263w+v1oqlpsSVrInfffTfuvfdeSd+NDHJ5jHACFsq+zp07h0gkgubmZtGRqtAI2eFw0AMMmbae6bZPkwg5Ho9jbGwMo6OjqKqq4vRVJkiJkD0eD/r7+xEOh7F582ZR3UzJar2j0Shs2/4PpqensWP0j4t+z4yaXWd7WL/LBiEmZGNZnJgSOGD5Ouy4UKlUUKvVKCoqYqU9YrEYXS89NzeH4eHhlFO1xUbI6c7vWy5yQpC1Wi3q6+tZ5iVin5+sucPpdGJgYAAajQZtbW2LhCxdg6FoNIqxsTEMDw9j9erV6O7uTnnyiXlNn8+HgYEBhMNhWCwWBINB0a2lXEb+8Xgco6OjGB0dfT+l8oEPgKIoxF//J87tFG9YS/974oO3itqHpSJbUxjJYM7lk6t+mM9sS+x2Eu/uyADWRHtMvqnaeXl5tDuc3+9PmfaQo8piucgJQdbpdJLFGOCP/sjoJ5VKhdbWVl5/VakRMkVR8Pv9OH/+PKqqqrB9+/aUt27MfU5FMBjEwMAAfD4fWlpaJOW4ma9HInKKojAxMQGbzYY1a9ZwRvLRj98PlUoFzZvP8W6z+o8vA8hOYV6povzv32/NqoYOMabyXFNLKIqiy1XD4TC9NkTSHiTlYTKZ6AuIx+NJak6fzSheFhy4XC6cOHECg4ODsFgs2LJlS1Kza7ERMikxO3ToECKRCMxmM1pbWwWLcSpIw8ipU6dQWVmJrq6utMQYeD9FMjMzg0OHDsHj8WD79u0wm8280TxFUYh94gHEPvFA0m1X//Fl+k82kS0eGHz+FlzceFdf1qQsgPS9LMjUEqPRiJqaGnR2dqKrqwubN2+ma4PtdjvOnTuHf/u3f8NHPvIR+P1+vPbaazh9+jRnf0EqH4uRkRFcddVV2Lx5MzZs2IDf/OY3ABa+V3/3d3+Hzs5ObNy4kVX+9tprr6GzsxMbNmzAX/7lX2J2dlbS8eZEhCxXh53L5cLAwABUKhXvDD4uhEbIxHx+cHAQxcXF2LJlC6ampmTb/0gkQtuFNjY28jaMkDsCMa/rcrngdDphNBqxefPmpBUZxPqQeddBRDlZxAyAJcrZEDmvxEj5gSf8+NoXT7Gix4KCghXtZZEo7Fxpj02bNqG7uxs333wzpqen8dxzz2HDhg04cOAAazupfCyefPJJ3HTTTfjc5z6HixcvYs+ePbDZbPjBD34AADh37hxmZmawe/duHDt2DPF4HPv378fFixexatUq/P3f/z1eeOEFPP7446KPMycEGUjPYMjtdsPv96O/vx8Wi0X07Y5Go0m6KEiMhaxWKwoLC1klZunknwmxWAzDw8OYnJxEfX09duzYkfTLJ6YZxePxoK+vj45U1q9fn96+ChRmIHtSGtkgymIX9x7+BoWfvVAOr9eL0dFRuvGJeFkwqxtWgpeFkNSHWq2mK5K++MUvcj5GiI+FSqWC273gjOhyuVBdXQ1gwd/iwx/+MACgsrISJSUlOH78ODZv3gyKouDz+VBeXg63242WlhZJx5kzgiwFt9uNgYEBUBRFO7BJSRtotVq6iykRUuvLdHhjotFoJLdtUxSF4eFhuitox44dgk5+tVqd8otCLlBkIbCwsBDHjh2TtJ9cxD7xgCBRBrIjas7GCoxU/M09tkU1ysTLwul0YmxsjPaySIymsy1CFlpl4ff7k5ZxCvGxePzxx3HNNdfg+eefh8/nw+9+9zsAC/4Wv/rVr3DzzTdjdHQUJ06cwOjoKLq6uvC9730PnZ2dKCgogMVioeuWxZIzgiwmQvZ4PBgYGEAsFkNLSwtKSkpw6tQpRKNRSYLMFeWSWmW9Xo+Ojg7e7j0pEXI8HsfExAR8Ph8ikYigqgwmyUrmQqEQrFYrXC4XLBYLysvL6fc23UaUROy7bsf8/Dzazv5a8HOWW5yzIVqWSjIvC9LUQaobfD4fYrEYiouLUxrip0KOlJxQQXY6nWkv6L322mu44447cODAARw6dAi33norzp8/j3379uHSpUvYtm0bGhoacMUVV9B3x9/73vdw6tQpNDc3495778XTTz+NRx55RPRr54wgC4EpxGazmVWrqNPpJNcSM3PIxDw8sVaZDzGCTFEUpqenMTg4iPLycphMJjQ1NYmOQLgEmZl/ZhovETIxDocIfewTD+AXxwvxyZEnUj+JwXKlNJZLlMWmLQBhRkQ6nQ6lpaWs78PJkyfR2NiIUCgEt9vNGqWUGE3L0RqdilgsJiiFkqrkTYiPxY9+9CO8/fbbAICdO3ciGAxidnYWlZWVeO659+/qrrjiCrS2tuL06dMAALPZDAC46aabOBcLhXBZCLLH44HVakUkEkFLSwtn0TipB5aCVqtFMBjEyZMnQVFU0hI5rtdNJcgkBz0wMEAvBhqNRhw7dkzSLSGzhC0Wi2FkZAQTExOC8s9CicViKeeaMe9qbtjmwet4DAAkCzOwdOK8kiJlKe5w8XgchYWFKC4uRmVlJf3zaDTKapEmkXTieClSKyznwFEhQYHL5UoaIQvxsaivr8d7772HO+64A5cuXUIwGERFRQX8fj8oikJBQQHeffddaLVarFu3DhMTE7h48SLsdjsqKirw7rvvor29XdIx5owgc31YXq+X9prgE2KC1AjZ6/Wit7cXLpcLW7ZsEV2QnkqQmZ7KGzduZOWgpfpZkOeRZhTSFShHri8cDmNwcBB2ux0qlYrOTxYWFi5a7U/8zG7Y5sEvjhfi9XppwgwsrTgvhyhLiZIB8aLMV4Wj1WoXOcORWmGmhSepFQ4GgxgbG6PFOtPRdCpBFuJjcfDgQXzmM5/Bc889B5VKhZdeegkqlQozMzO49tproVarUVNTg5dfXjjXqqur8ZWvfAUf+tCHoNPp0NDQgJdeeknS/ueElwWwcOUmwsYUYuJpkQqbzQa9Xk+vqKaCGBiRsU/Dw8OSeud9Ph/6+/sXeQ+Q1IdWq0VLSwtnDvr06dOiPTQoisLx48cRDAaxevVqNDU1Ce7I+tOf/oQrrriC83fMSo/Gxka6VZYZUXk8Hnq1n9zqBoNBtLe3s/bhF8fZaR4pwpxIJsV5qUVZiiAThIrysWPH0vaCCIfDOHXqFGpra+kOvGg0CqPRyEp7pOq8I+eskP35+c9/jrm5OTz00ENp7XsGuHy8LICFaItpLkRmyglFqJ8Fc26d2WxGeXk5baIihcQI2ev1or+/H/F4PGXqQ2yEPDc3h/7+fkSjUbS2trLmjEmFoiiMj49jeHgY1dXVdKQdiUQQj8c585PEwpF47J47d471Rf1QvQn/M2KmH59OxEzIZL4519MX6WAwGFg5WoqiEAwG6Wh6ZmYGgUAAKpWKt/NOaP4YWNlt00AOCfLU1BQGBwdpkRSLVqtNWn6WbG5dOrkyIsh+vx8DAwMIBoOwWCyCzFGELgi6XC709fVBp9Ohs7MTY2Njad86Eh/lgYEBlJeXv29oLwBi4QgsVHW0t7fTX1QSTbcZ/oTeEDsal1OYAXnFeSnL4qSmLQhLJcpc6xsqlQp5eXnIy8vjNBwiZvhDQ0OIRqMwGAwwGo3071NNLXG5XKyytpVGzgjymjVr6BItKfB124VCIQwODoqaWycGckt/9uzZlANKE0kVIZN0SDQaRVtbGx1tp+ul7HA40N/fj/z8fHqBMV2YX9SKigoAQCc8i9IXwIIwy5HGyIQ4r6RoOdOIWXDm6ryjKArhcBizs7NwuVyw2Wz0sNbEaJqUq670CDlnvCxSreinIlGQw+Ewent7ceLECZSUlGDHjh302CQ5INs/deoUNBoNuru7UVFRIWr7fBFyMBjE+fPncf78edTV1WHbtm2sE500hoglFovh1KlTGBoaQnt7O9avX5+WGKdzZ/F6/WN0xCwHcnppLIUHhhh/Cy5233KS93dyVUbI4WNBjO2Li4vR0dGB7du3Y+vWrairq4NOp8Pc3BwuXLiAI0eO4LrrrsPJkydx6NAhnDlzBuFweNE2M+FjEQ6Hcdddd6G1tRVr167FG2+8IfmYcyZCThciyKQed2ZmBo2NjbBYLILzV0L8IZjjmBobG9Ha2opDhw5JEvrESDccDmNoaAhzc3NJR1aJjZBJuiYYDKKjoyMtZz0mQgSZVF7wIUcag4lcUfNKiJT5UhfL4fQmZjt8U0v++Z//GQ888AD8fj8OHjyIaDTKKmnLhI+FWq3GU089hcrKSvT19SEej2N+fl7yseaMIMsRubpcLhw9ehT19fWscUlCSOUPwaz3TRzHJBUSITMrHBoaGtDa2pqy/leIIEejUXqoqtlshtPplN34W0g0lkqUAfmFGUhfnDMtyunmkgFuURbqc5KKTBkL8VFbW4tIJIIDBw5wLlhnwseiq6sL//qv/4qenoXBC2q1mpUbF0vOpCykEovFMDQ0hNOnT4OiKOzcuRN1dXWixZIvfRCPxzEyMoLDhw8DAHbs2IH6+npZmi9UKhVmZ2dx+PBhqNVq7NixA7W1tSkvTqki5Hg8DpvNhiNHjsBoNNLpGi6T+nT3Xyg3bPMIepzcqQyC1JRGtlh4JmP3LSdx9uxZDA4OYmZmBj6fT5YAZ7kmTvPlkLl8LMbHx1mPefzxx/HKK6+gtrYWe/bswfPPPw/gfR8LEqQQHwunc+HzffTRR7FlyxbceOONmJ6elnKYAHJIkMWeQLFYDDabDYcOHQKwIJQ6nU6yUCbmoImJ+6FDhxAKhdDV1ZW0zVmM0FEUhcnJSdhsNoTDYXR3d6OxsVHUAgqXIDP3ORaLobu7m3VxEpPqEPJ5iM0hCxVlIPPCLEacS7TOrBfmf3gmisLCQvh8PthsNjidTpw4cQK9vb0YHx+Hy+USve6wHKkPqX40BOJjMTY2ht/85je49dZbEY/HsW/fPtTW1mLbtm24//77aR8LMu3niiuuwMmTJ7Fz5048+OCDkl8/Z1IWgLAvuJi5dWIgETLxPLZarSgrKxM0BUSoHSazhbqkpATNzc2SFk64It3Z2Vn09/ejpKSEd58zESGL3Z6Q9AWTTKQyCGJTGplIYciRtiBUVFSgoqKC9q5oaWnhnCRN2qRJ96XBYOC8AMs1BioajSb13yakOpcy4WNRXl6O/Px8fOITnwAA3HjjjfjRj34k+NgSySlBTgZxSCNz68TUzQpBo9HQK76FhYWiysGImCcTZKfTib6+PhiNRrqFenp6Gh6P8KiRwIx03W43XaOc2JqdiNDcM2luoSiK/uIWFhYuEnmpt8ViRRnIrDADwsU5mxf7SD6ZnIvJ2qS9Xi9cLhfGx8d5LTzlWtQTG3TwnVeZ8LEAgOuvvx6///3v8eEPfxjvvfceKyctlpwXZObt/apVq1JGrGInaQALdbl2ux0+n4/T8zgVyRo8vF4v+vr6AADt7e2sleV0vCxCoRBdGtTa2irIsjBVRBsKhTAwMACv14vm5mZoNBp4vV44HA6MjIwgEonQZUxEoKXWQ0sRZUC+GuZkpOoKlFuU5YySd99yEq8838CbuiODCvLz81mmQ1wWnoFAAG63Gz6fL6UhfjKE5pCDwWDSSDoTPhYA8PWvfx233nor7r//flRUVODHP/6xqONjkjNeFgDodl2AbVVZVlaGpqYmGAyGpM8/cuQItm3bJviq7na70d/fD7VaDYPBgLKyMqxZs0b0fp85cwZms5nlVxEIBDAwMIBAIMDbuTc/P4/p6WlRzlLhcBgXLlyA0+lEZ2enqBXhU6dOoa2tbdEFh+Tjp6amYDabsXr1asRiMUSjUdYXmxT6ezweeDweuN1uOJ1O1u1vYWGhqHFDUkSZkGlhBpJHy3KKslyCTPje18rQ2NiY1jYuXrxIWwsQsQ6Hw9DpdLRAFxYWIj8/P+nnfe7cOZjN5pSBzuTkJO6991789re/TWu/M8Tl5WVBIC29VquVZVUpBOJnkUqQmR1wZOSTzWaTPIqJGSGTybpOpxMtLS1JO/fERMhM0aysrIRerxddnpOYsmD6WNTU1KQs5SOF/gaDAatWrUIkEsH58+exfv16+gvLHDdUUFBApzv4nMLa8w7jUmCHqOMgZDqNkYpsTl987uF5vPVqY1rbIBaeiUJKLsperxfDw8Mpu++ERsgrvUsPyDFBnp+fR09PD0wmE2tunVBSDSsNBAKwWq3w+XxoaWlheWYIHXTKhUajQTgcxsDAAKanp9HU1MQ7oDTxeakuAvF4HOPj4xgZGaFF0+PxsBY3hEIW9ZiLi2VlZWnn47kMiIi/h8fjwfT0NKxWK+27y4ym5VhkzPTC31LklOVMWxDS9bzgWxfR6/UoLy9nfX+I4ZTX68Xc3BxsNhsikQiMRiN8Ph/m5uZQVFSE/Px83u9FKuvNlUBOCTIASTlcghA/C7PZzNninMqciI94PE67XjU1NYlqGEkWITOrPVatWsUSzXRyzx6PBz09PTAYDJIuekyS5aQ1Gs2ibiyyoOTxeOByuTA2Nga3242mgt9jiNoleT8IyxUxZ/O8vnREWUzZG1f3HUVRCIVCOHnyJEKhEIaGhuD3+6FWqzmd4ZxOpxIhZxPl5eVpTXBOFORIJIKhoSHMzs6mjFrFzsYjNb/Eh7mlpWVRCU4q+F7T4XCgr68PBQUFnCkbKYIcCAQwPz8Pj8eDjo6OlBNRMlGHzFxQIp1YFy9exJo1a9BI2fD/hhoFbysZyynM6YhyJqLkdEi3DlmlUsFoNEKr1bLy2cT5zePxYGZmBoODg3jxxRcxNDSEoqIivPnmm9i4cSOamppYwc3bb7+N/fv3IxaL4c4771zkmTwyMoLbb78dTqcTsVgMzzzzDPbs2YNwOIzPfvazOH78ONRqNb797W9j165drOfu3bsXg4ODOH/+vOTjBXJMkNPtLiKCHI1GMTw8jKmpKTQ0NAgaayQ0ZcHMcZeWlmL79u2YmJiQtL+JwkoqMlQqVdLBqmLqiSORCAYHBzE/Pw+TyYTGxsaUYkxRFL2op9FooFKpeN8/OVIOer0eJpMJN5RLq7zgQy5hTpW2YJKNeeV0ouRMzGLkcob7wQ9+gGeffRZTU1M4d+4cXnnlFbz00kv0YzLlYwEAb775Ju93TSw506knB2q1GtPT0zhy5Ai0Wi127tyJ2tpaQSkEIRHy/Pw8jh49ipmZGWzatAlr166FXq+XNHma+ZrE3e3ChQtoamrC5s2bk54gQiJk0j599OhRFBQUYMeOHcjPz08qoCS/TBpkNBoN/f9IJIJIJIJwOIxYLIZ4PC5rkwlBTDefUDLV9cdHOl196brA8ZHMHS6TCC1DVavVUKlUuPLKK/GVr3wFb775JkuwmT4Wer2e9rFgItbHAlgIgp599llJE6Y5j0OWrWQJUq/GzPly0WgU3d3daGjgr8XkIlmE7PF4cOLECQwPD6OjowPr169n5V6lCjJZ+Dp58iQqKyvR1dUlyPwnVe55amoKhw8fptuniT9GqufF43H6PdBoNNDr9TAYDNDr9fSFhynSbrebLoWLRqOIx+Np+TQTMiHKwNIKcza2Wi+HKGezjwWw4GFx4MAByetWieSUIIuFNI0cPnwYfr+fLmGT0krNJap+vx9nzpxBT08PWlpaeCNXsYJMoldy27Rjxw5UVlaKMrbnyz0fPXoUc3Nz2Lp1K8xmM+u94Mr5MtMT8Xic9qVm7otarYZarYZOp4NerwdFUejv78fIyAjWrl1LvwZxriPRNJmTKEWkMyXKACSJcq4YEwkVZbnufsR0+6Vb9ibWx+L06dOwWq34+Mc/Lvk1E7ksc8jMPG5JSQm2bt0Kg8GAubk5Sa3IADtCDoVCsFqtcLvddC1xMoQKMrmADA0N0T4cR44cEW2IlBjp+nw+9PX1gaKolLlnZuMNM6IVMiCAeEHPzc2hubmZs8aapDKYfwOg0yDk1pSZl+b78kvt5hOClPzyN/69GF+80SXqdaRUYGR6cW/3LSfxzD9oWA09iYNK4/G4LI6GYtqmkwlyJnws/vCHP+D48eNobGxENBrFzMwMdu3axTKvF0tOCbIQ5ubmMDAwgIKCgkVlW+nUEpOos6+vD7Ozs2hubkZ7e7ugi0QqQWbW/ZKFwHQcrcg+kdpnt9uN1tbWlMbzpDGEiCPJ76U6RlJRMjIyQi+o8H1Zyc+ZURGfSEejUTrdEYvFOBcPMynKgHhh/sa/L9TJShHmbFrse+jrMbz8nWK6TjwQCECj0dBlaEajURZBFpuy4EvZZcLHYt26dfjc5z4HYGFq/XXXXZeWGAOXkSA7nU709/dDp9Nh/fr1KCgoWPQYqYJMzOd9Ph/q6+sFVWUwSSbIZECpwWBIaf4jZn9DoRCOHTsm+MJBxNftdqOoqEjwF25ubo6uKNm2bZukBhIukSYX1uLiYrpBhFwsyP6SCo9PbHHhzZOZbRgQIszPVb2ABybvASBNmMWI8lKUwK1atYp190fmQ3q9XkxMTMDj8eDYsWPIz89ntUqLCSbECLLb7eaNkDPlYyE3OeVlAWBRc4bH46GdxywWS9KSrXA4jLNnz2Lbtm2CXovpIFddXY2JiQl84AMfEL3PpBV706ZN9M/8fj/6+voQjUbR2trKu99/+tOfcMUVV3D+LhFmq3M4HMaVV14pSFSJ2Pn9foyNjcHj8SAcDsNoNKKwsBBFRUUoLCyE0WikhZ2kQTQaDSwWS1oNJEwCgQBttmSxWFgXKBI5k1QKieIB4D/PyDN2Sih8wkwEORExwixUlJeiJpmvHM7j8WBsbAxtbW3w+/20UHs8HkQiEej1ejrdYTKZeDvwpqamEAqF0NDQkHJf/uIv/gKnTp3KSKmdDFyeXhZk4cnv96O/vx/hcBgtLS2Cqg+Il0UqmMZFzC64yclJSfvMjJCZ+WeLxcJqL022P6lOQuJ3TFqdmXWUybbLzBPn5eWhtbWV/l0oFILb7YbH48H4+DiCwSC0Wi29OEeMhuT4gpAV7vn5eVgsFs70CtNIn0k8Hsf/2erGGyeS10/LSSZTGdmUvkg1l0+tVtOiS2CaTHm9XtjtdroDj5mXNplMgnPImSihXA5yTpCDwSAGBgbg8Xhovwkx1QepPti5uTn09/dzeh6THKuU8U/RaJT2spCSf+Y7aZl+x0JbnYUs2JEuKqPRiMrKSnpU1fj4OCoqKqDVajE9PQ2bzUYX8pPWWDFubsz8c11dHbq6ukQLPKny+GS3H68fkac8SSiJwsxMW3AhVJiFiPJSde5xiXKyLr1EkylCNBqlO/CIIX4wGITRaEQkEqGFOpmFZ5ZGx4LJOUEmUeu6detk/XBILlev16Ozs5MzB03EUYwgk7SHy+XCmjVrJA9XTSQYDKKvrw+hUEiw3zFz4UzMgt3MzAyGhoawevVq7NixY9EXMRKJ0JabNpsNPp+PjoaIUJtMpkXPczgc9BQTqfnnRJZDlIHMRMzZHClLaZvWarUoLi5mnasDAwPIy8uDRqOBw+HA6OgowuEw3Z1JXACNRmNaC93ZQs4JckdHhyzNBQSS343FYmhra0uagya360KEg5n2qKioQEFBgaA8WSKJC4LRaBSDg4OYnZ1FS0sLpxES3/6Q6olkrc5MXC4X+vv7kZ+fj82bN/P6Tet0OpSVlbHSDLFYjBbpsbExeL1eAKBX6B0OBzQaDdavXy9b0T1pA29WuzEYv1KWbYrl9frHgEnhY+JTCXOqsril9LdgirJc8/Ti8TgKCgoWLdaFQiE6J/3uu+/ixRdfhMvlwl133YWNGzfiYx/7GGpra+nHy+1j4ff7ceONN8JqtUKj0eD666/HM888k/bx5pwgywHJjw4MDMDn8/HmLBMhqYdUzM/Po7+/HyaTia6BttvtkvaVRMjxeByjo6MYGxsTVOlB0ivkbzH1xCQtFA6HsXbtWkl9/BqNZtF4oHA4jP7+fkxMTMBkMiESieDs2bMsX+SioiLRkTJZzBwdHUVDQwNaW1uxXbU8kbJUhAhzNkTLiWOg0oWvyoKkPMrLy/GZz3wGV155JZ588kl8+tOfxunTp1n9BJnwsQCABx98EFdddRXC4TCuvvpqvPXWW9i9e3dax5tzgpxumkKtVqOnpwcOhwNmsxkdHR2Ct0kiZD48Hg/6+vqgVqt5S++k7K/dbsfExAQqKyvR3d0taBFEpVIhGo3Sxya2scNsNos2uOeDNLwMDw+jrq4O7e3t9MWEVHd4PB7Mzs5iaGgIkUgE+fn5LJHmi87JLEJSv818b5YrfbGjuwyHjwiPkpkkE+ZsEWVg4VyR485GaNmb2+1GWVkZuru70d3dzfod08cCAO1jwRRksT4WXV1duOqqqwAsmFtt2bIFY2NjaR9vzgmyVGKxGIaHh+H1erF69Wrs3LlTtLjzRcjMcUytra2yebY6HA7Mzc0hGo3SkXYqSJ64sLAQx44do/O4JJfLFdWIaeyQcgzJ8sTMVfqqqip6f4gvstPpxOjoKEKhEKsMT6/XY2RkBLFYLGnaY7lEOV34hJlLlJfalnP3LSfxwj8WZzRCTiSZFzKXj8WRI0dYj3n88cdxzTXX4Pnnn4fP58Pvfvc7AO/7WNx8880YHR2lfSy6urpYr/3rX/8a+/fvl3KILHJOkMWKKDEWGh0dRU1NDSoqKpKOTUpGYoTMtK7kM7ZnItTZitnqXF5ejpqampRinLhg19bWRs86c7vdmJycRF9fH52zIwJNys3KyspkW1gDFi5S/f39iMfj6OjoEHW3wOWLTNJMTqcTIyMj8Hg80Ol0yMvLw9jYGC3UXPWuK1WUAW5hzoZI+Z5HXXjtxdQlm6kQmvqQy8fiwIEDOHToEG699VacP38e+/btw6VLl7Bt2zY0NDTQPhaEaDSKm2++Gffddx8dgadDzgmyUIir2dDQECoqKuhb/YsXL6Y1iom08Y6MjGBiYoLOWaYSWpILTnbyMVudSY0yEdFUx8q1YKdSqRb5yt8IErEAACAASURBVJJROna7HRcuXEAsFoNer0cgEMD4+Dgt1FKFmZn2SByDlS5utxs2mw1VVVXYsmUL1Go1Xe/qdrvpelcyjYQcS0FBwYoWZWCxMCeK8nKY19/8+cG0RkABwoOUpfaxINx1112wWCy4//77RR0XHzknyELKtIgvBNNYiJDubDyS56yuruYsAUv2XL5ogKRTJicn0dTUxKpRTtZ2LdUAaHx8HB6PB52dnSgpKQFFUfD5fLSoWa1WOkfITHckKzti5onlTnsQY36DwbCo2oNrfhuzDI+kqRb2ZZcs+yOEdPLIyWAKczaMhtp9y0n85mebM14f7HK5eCPUTPhYAMAjjzwCl8uFH/7wh7IdR84JcjLIAo/RaOT1hZAiyETkh4aGYDQaJZn/cAkrc8xTdXU1Z40yVx2yFCEmjR1E9Nva2lgLflzdVn6/H263G3Nzc/RiW15eHi3QZLGNvO/FxcWypj0ikQisVis8Ho/gWmuAvwyv1TOB3/ZWy7Jvy02iMDujJcs24mnP35zCkwcoVhdeQUGBLDlmQrIIORM+FmNjY3jqqaewdu1abNmycBdwzz334M4770zrOHLOyyIejy9qfyYRFLDgf8AcpJjIyMgIVCoVaxEgGURs8vLyUFpaikAgAIvFInq/z5w5A7PZTIseaXUuLS2lpxyk2l8pQpzY2FFfXy/5i0IW20g7tdPppKPPyspKrFq1CoWFhTAYDGlFTMwytsbGRqxZs0a2CMzr9eK/LlTKsq1UZCJC5uOLN7rgjJYs28y9X/90A1037PV64fP56MiTKdTMi3U8HsfJkycFect84QtfwO23344PfvCDmTyMdLh8vSwIzOoGi8Ui2M9CyPRoZsNIe3s7CgsLMTc3Rzc4iIVEyB6PB729vdDpdILc3dRqNZ23FtNhB7zf2EGGoabb6UQW2wwGA7xeL+LxON3V6Ha74XK56IoIg8HASncwjYmSQaoyuMrY0iEajdIeIn+5Noa3e6pk2W62sBAxL188df1tZ/HWq1tYUSxZryB+FkNDQ4hGo8jLy2NZeArJI7tcLsF3SNlMzgkysLD4ZbVa4XQ6aYN4oRGUTqeDz+fj/T3T/CfRQzid/DOZohGPx9HW1ib45FKr1XC73fB6vcjPzxeUlyUXqkgkIrmxg4tkeeK8vLxFFRGJxkR6vZ4l0kzTc9IKHo/HZe3eY+5zfX09vQC70hf5+Oj4QCcu/O+5ZXntxPZqtVpN15InljR6vV44HA74/X4cO3YMOp2OjqILCwsXVcsk80JeSeRcyiIWi+GPf/wj6uvrUVVVJfpW1uFwYHJyklU0DrzvNma329Hc3MzpYub1emG1WrFx40bBr0dancfGxlBXV4eWlhbBrc7xeByhUIi2xAwGg9DpdKwcLlPUMtXYAbzvN11UVITm5mZJeWKmSLvdbgQCAeh0Ovo4m5qaUFNTI1t6gtyNmEwmmM1mzn3OtCgvZdoCAJ67b8EM65pPHlvS12UitPLC7XZjfHwc7e3tLHc4j8dDu8PpdDq89957eOedd/DLX/4SFRUVi7Yjd9s0AJw4cQJ33HEHAoEA9uzZg29/+9upzktBJ23OCTKw2BNZDB6PB0NDQ9iwYWF6L7Mlua6uLukU6mAwiAsXLmDr1q0pX4dZ/1xXV4dwOAyTyYQ1a9YkfV6qPHE4HIbb7aaFze/30+5YXq8XtbW1aGhokG1BJRgMor+/n/ZtlqP7EHg/t03GbBmNRvp4tFotK5IuKCgQJdJkMdDr9aKtrS3pmgKQWVFeLkFm4vF40NPTgy9/c+n2Q4goOxwOzM7O8q7JkLFJP/vZz/Dqq69i1apVCIfDuPPOO/H5z38ewJ8Xa1tbWW3Tr732Givguuuuu7B58+ZFbdPf/e53cfz4cfz4xz9mtU2r1Wp0dXXhO9/5Drq7u7Fnzx7cd999qdqmL88cMsA9jFMoJO3ArFMW2pKcqnUaYAsNs/7ZZrOlHOMkZMFOr9ezJjnMzc2hr68P+fn5qK6uhsfjwdGjR+n0AFckLYRYLAabzYbZ2VnZo22v14ve3l7k5eVh27Zti3LbkUiEvuAkqy1OvHAmLgYyK0mSkSvpi0QxjsViGBwchNPpxNq1a/HO6+wLUyajaD4fZSapuvS0Wi2qq6vx4IMP4le/+hUOHTqEaDTK8rHIRNt0XV0d3G43duzYAQC47bbb8Mtf/jJtHwsgRwU5HbRaLQKBAI4cOYKioiLBLclAanMhUpGRn5+/yEuZr55YiiUmsCBq/f390Gq1nD7IzEh6enqajqRTiTS5UNlsNtTU1MhaT8wsY0vmrKfT6ZLWFjMtPkmOUq1WY3R0FGVlZZIWA1e6KCeKMfH1rq6uxrZt2zjPqXde3876v9wCnUqUxYxvAhaElZQ0EjLRNq1Wq1lOcrW1tRgfHxe8n8nISUGWGiGTnGIgEMDOnTtF337zCSUZx8SsyEhEo9EsSrVIscQkC5perxcWi4W3NjMxkibPTSbSKpUKw8PDKCoqkrWemKIojI2NYWxsTFTkyoSrtjgajcLhcGBwcBChUAharRbz8/OIRCJJfZj5yIQoZ6pBhI9wOIze3l7E43Fs2rSJFRSkIlGggfRFmtzxcZ3b0WhU0GcjVrgTkdo2nQlyUpDFQnwViJn7hQsXZMmFEnF0uVwpxzExI2Qp9cSxWAyjo6N0Y8fatWtFixqfSJOml3A4DK1WC5/PB5vNJjndwYSUsUmNXPkgxv8TExNobm6mfUSS+TAzp5pk+ou3lDx918L7MTk5iZGREZjNZlRWylNr/c7r2xGNRtHf349AIICHvp48ZZfIX/3tafzqJ530uU/OdeJGKOTu1OVy8d5NZaJturS0lOXsxrVNqeSkIAsVh3A4jMHBQTgcDtHlcclIbHUWIo5MHwyxjR3T09MYGhpCVVUVurq6ZBMTIvJ2ux0Wi4UWaqnpDiaZKmMDFkS+r68P5eXl2L59O+v94PJhjsVi9Or9xMQEXUPNJdIfqOnD/463cr1sVnL3njGcO+egzZYqKyvpLst0LqQEu92OgYEBNDQ0/DkPvXh7qaLovbefw1uvbqFTcyQYcbvdKCgooG1i+e4Sk3XpZaptuqioCIcPH0Z3dzd++tOf4t577xX6liUlJwU5FUzBlHqLzAUZ3nj48GFRXhZkZP3s7CyrFjdVioKMlSJG93KNsEnME3d1dbH2RUq6g4h0PB6nFwPlNhdiVnx0dnYKFnmNRrNodFA8HqdFempqCj09PQgEAgut8WvCOGZfL9t+ZxKSvtu6dSuMRiO9GDo5OUmXFSYuhgr5LoTDYfT09AAAtmzZkjSSFZKL3n3LSfpxHo8HFy9exKpVq1BWVkYPUQDAWmdRq9VQqVRJm0Iy0TYNAC+++CJd9rZ7925ZFvSAHC17i8VinItrZJV9eHgY1dXVvG3Chw4dQnd3t6jFKlLNEAwG0d3dLUgMEhfsiBUmafSgKIrlV0zynaSxIxqNwmKxyNbYAbwv8oWFhUlbtoXAFGkiBJFIBMXFxaiurkZxcbEsURrx4ZiamqJtTuUiEolgYGAAfr+fdvkiddJnXOk5mREylUP+ZNdprFmzBvX19bznMqnvJcdE6nuZdwYmk4l+PrORhowIk4NrPnkMb7+2lb5jbW9vX3ReMyNo8p2hKApf//rXcerUKbzzzjuy7EuGuHzrkBMFmaIo+taqvLw8ZePCsWPHsHHjRkFiRKaAaDQatLa24uLFi+js7BTkT8xcsOMSpXg8Tn9RiKCFQiFQFIWqqipUVVWxvizpwBzL1NraKqvIk/coLy8P9fX1CAaDi+qkpZbgkWqBdH04EmEKTzK/DLkW+eQW5Vs/eBHt7e2CpownQkrHyLnn9XqhUqmQl5dHd4S2t7cLrj4SgsvlQk9PD30BEfL5z8zM4MCBA1Cr1XjiiSfQ3t4u2/5kgMu3DpkJWTTiKjXjg9QiJxNkImAkciI5LPJcvpNVzIKdWq1GcXExXeHgdrvR1NSEoqIiuN1ujIyM0OY9RMyKioo4a3D5IOmbmZkZup5Yrk44ZgNGa2srvfBiMpkkpTsSfUr6+vqgUqmwceNGScLDB6m2KSwsTFlNkq3lcJs3S7e81Gq1KC0tpVuRKYqCzWbDxMQEKioqEI1GcebMGTrPTiJpKT7ZsViMtiIQOtaMoii88cYb+MY3voGvfvWr+PjHP55xe8+lIicj5Hg8TgsxRVEpHd4SOXfuHBoaGjhXbpkt1GS1mnky8D1XSuUEsOD6ZrVaUV5ejsbGRs4qBFI5wEx3MBsliEgzX4+5GFhTU5O0A1EsiWVsUtzY+DoOTSYTPXG4tbVV1oYUpsHQ2rVrRZ0z6YqynBEyVzeeVDweDy5duoSysjI0NTWx7kCIORA59zweD2Kx2KJ5h3yBjcPhQG9vL33+CTlHpqen8YUvfAEFBQX41re+Jevnn2Eu35SFy+XCmTNnBE+LTuTSpUtYvXo167mJrc58Apb4XKlCTCxDdTodWlpaREeA0WiUlb8l3WxFRUXQarWw2+0oKipCS0uLbIuBwPsTtcvLyxd9gdNlcnISVquVNlEKBAJpdxwC7EXM+vp6VFdXS4q40hFluQRZLjGOxWIYGhqCw+EQdXEiFRxMkQ6Hw8jLy6NFOj8/H6OjowgEAoLTKvF4HL/4xS9w8OBB/OM//iM+9rGPrbSo+PIV5Hg8jnA4LPkD6+/vR3FxMV0iRPLPFRUVaGpqSlorSyYcr1q1irUAIVSImY0dYkzXhUByucFgEEajEeFwmDYjIn+kLrKRWm5yRyJnGZvf76ctSS0WCysdxBdJCxVp0qadn5+PlpaWtJtdlluU5RBkErlWV1ejrq4ubeGjKArBYBAejwfT09Ow2+3QarX07EYudz8mU1NTeOCBB1BUVIRvfetbslbmLCGXryCT8jOp2Gw26PV6FBQUsL6sQvLPg4ODyMvLQ0VFhSghJnP4pqen0dTUtCgVkg7J8sSJlRDMqJP8SeZVzPS0kLuMjURp8/Pzgv2sE4+JT6R1Oh1sNhscDocou1MhSBXldAX52XvTM/4nFSViIlcx2+7r66MtX41GI8vdj3xOpAxvZmYGRqMRfX19+M53voOnnnoK119//UqLipkogiyVwcFBTE1NwWAwoLW1VdTt2vj4OEZGRrBq1SoUFRWhuLg45aw5ZmNHshIlsTC3TaIdIdsmXxTyJxgMsgzlSV7QbrdjcHAwIznomZkZDA4Oora2VnB+MRlMkbbb7fB6vTAajaioqEj77oC533a7Hf9vqFHS89MR5L/b1cdKSzGnbAv5XIjhVUNDgyTb2mSQO0wh6wnEOOoXv/gFXn75ZYyOjqKpqQnbtm3DV77yFdpTewVy+QoyIM2Ck6QL7HY7SktL0dnZKeh5zDwxMV/3eDxwuVxwu92IRCKs2zOSxyUewnLU/CbidrvR19eHgoICmM3mtLdNStXcbjccjvc7v1avXo3S0lJ6fl66MJ3e5M5v+3w+9Pb2wmAw0JaO6aQ7mAQCAfT09ECv18NiseA/TokfLJqOIJNUBTFZIsdETJaYx8SswgmFQujt7YVKpUJbW5us7zfxzaAoCmvXrhW07Xg8jtdeew3PP/88vva1r+Gv/uqv4PP5cObMGWzevFnWVNgSc3kLcjgcFmwwRNIFExMTaGxshF6vx/z8PNra2pI+T+iCHVnoIALtcrnoaKaqqgoVFRWy+SeEQiEMDAzQvhxy1hOTC5bP54PFYoFer2dF0mTxJjGSFgIx6ne5XLLnzpmpD2aJIhdic9LxeJxOByUuIotNXUgV5FR5Y1JXTI6JlEqq1Wr4/X40NDQIvnsSAvPOTIxvxsTEBPbv34/Vq1fj4MGDOTEBhIEiyKmOjRT/k3QBMW53Op0YHx9HR0cH7/OkLNiRkrn5+XmYzWYYDAZapImHK7NUTUzTBzMHzTTTkYN4PI7x8XGMjY2hqamJc1oKwB5ySv5EIhHk5+ezRJq5cCZXhQMfdrsdVqsV1dXVktMqfCKt1+vhcrlQWVkJs9nMeUFdClEWu5Dn9/tx8eJF6HQ6FBcX03PtALDK1cQ44RFCoRB6enqg0WjQ1tYmaJE0Ho/jZz/7Gb773e/imWeewe7du1dyrpiPy1uQI5EIHblyQTq8iouLF93SJxvFJKTDLhGmoNXV1fGOISImN0SkhdYTk6nRqdpkpZBuGRu5O0hsny4oKIDBYMD8/DyKi4thsVhks/MEFlIIvb29dAelnF1lxMchEAigpKQEwWAwaSQtRpTFCrIYMSYt5tPT02hra1t0p0DOP+ZCGwC6+YMcG9c5wOxsZBpRpWJ8fBz33Xcfampq8M1vfjPp3csKRxFkLkEmOUq1Ws07cohrFFMmGzuSwVVPTEYY6XQ6zMzMwGQy0SkEuSCdcADQ2toq64o7yS263W4UFRUhGAwiFostKoOSYsVJzIuIQ52UOnQ+KIrCxMQERkZG0NzcvKgShi+SLiwsxCnHJkGvIUaQxYgxafAgF1ahF21issRsOmJ26JHzsL+/n14EF/K5xeNxvPzyy/je976Hf/qnf8K1116bi1ExE0WQmYJMWp19Ph/tacpHNBrFiRMn0N3dnXZjh16vF1wyJxRyUSHuY5FIZFEVhNTXy2QZG1PQElfcSdcXU9CYFpjJojMCuevJxJ2C1+tFT08PCgsLYTabBV8smCItxCFObkFmjmniMuyRAvOzmpychNvthl6vp9v8/397Zx4WVb3/8ffBYd9BQQEF2YZNtgFB7VpZcd0ey+W6ZJnXfDQ1xTLNrmnZopJWerUsc638WWq36xJSLpjplUFUXAFBQEAQlYFhZ7bv7w/8Hs/ADAxwAIHzeh6fR2bG4/fMMJ/zPZ/P+/P+0M9K3x1PQUEBFi5cCA8PD3z22We81gueYHp2QKbews21OuuCEMI6vrU0ENfV1eH27duorq6Gj48P78UpfXlirgqivLwcdXV1LSqwtWcrNfDYRc7W1haenp4G76K4u7OKigoQQrRSONbW1lAoFLh16xYIIRCLxbxe/LgBramxUoZiSPrCkKBsSDCWyWS4desWbw0eXKqrq5GWlgYrKyt2UjpNTdECokqlgqWlJasrHjBgABISErBt2zZs2LABzz//fHffFXPp2QFZoVAgLy8PeXl5cHNzM7iKTAt2Z8+ehZOTE+uT21wOsj0bO1qTJ6bdUTQfTb8gDQtsIpGI7eCzsLDgRSLHRaFQICsrC7W1tbyoPrh5Trlczo5ksre3h5OTU4uLoU1BC4It8VowhOaCcnMBublgrFQq2Qk4fn5+vKabCCHIz89HYWEh/Pz8msz5cusHmzZtQkJCAmQyGYYOHYqoqCgsXbqU19+1J5yeHZDv3LmD8vJyg3djgHbBjnurKZfLoVAo2GBGb82MjY21VAJ8N3YA2taVbdXlEkK00gJyuRw1NTVgGAYuLi5wcnJqVWVdF9xCpq58a1uhxcY+ffqgf//+WufVVge82tparToDnwVBjUaDnJwcpDzUr3FvLiDPG5mnt5mFNni01tSpKaqqqnDz5k3Y2dnB09PToN8TjUaDXbt2Yfv27fj888/x7LPPIjc3F6mpqZgwYYKwQ274ou4akPWZ1OvCkDxxQ7UA1d1SxcDAgQPh4ODAWzCmqY+ampoWdQsaAjVKunv3Ljw8PGBhYaGlUwUey+9sbW1bFMwA7Tl5fBsM1dXVsVNBxGKx3t0fV3tbXl6OqqqqZhUrGo2GnUvY3AzE1kBTCH379oW0OEDv65oKyHFvGOksHFpYWKC8vBympqa8exVztdb+/v4Gp23u3LmDN998E35+foiLi+NVE98F6dkBWaPRQKlUNvma1hbsqJGOSqWCm5sbFAoF5HI5a+RtbW0NW1tbnV96Q9ZNp1/wrScGHu8se/fuDQ8PD53Bsjk7T1tbW1hYWDRaFw2WSqUSYrGY164qehEpLCxs9VQQbhcbDdJUsSISiXD//n3WQIrPiwj1cVAoFFopBH2pC30BWVeqghCCvLw85Ofnw87ODmq1Wkvd0da28NaoMzQaDXbs2IFdu3bhyy+/xDPPPNOTdsL6EAKyvoDc2kCsVCqRm5sLmUymV4GgVqu1UgJcmRoN0rrMeqgPQnZ2druoBNoqY2tKfmdtbY3KykpWmcHnCCUAKCsrw61bt9plx11dXa01L49PBzxuOktfQ01LgnLDgEwLa5aWlvD29tZKzTUlwTPkvGhqpaSkBP7+/gbfoeXk5GDhwoUIDAzEunXreJnezmXWrFk4evQonJyccP36dQDA0qVLceTIEZiYmMDLywu7du1ic9tr167Fjh070KtXL/z73//G3//+d17X0wKEgNwwILe2w46bD21NN1nDfDS1v6RB2sjICDk5OTA3N2c7+PiCtg2XlJTwrstVKBSsR7RIJALDMFrn1VZ/C4VCwRanxGIxr19ubrBsmG9tqwMe8DjQm5mZNdv0oisoNwzI3GDMTSHoavDQh6FBury8HOnp6exYLEM2Bmq1Gtu3b8f333+PjRs3Yvjw4e2yKz5z5gysrKwwY8YMNiD/8ccfGDFiBEQiEd59910AQFxcHG7evIlp06YhOTkZhYWFeP7559lxa51Azx7hpGsHyu2wM1RxQRs7evfujcjIyFY1KzSc0kwVEDR9UFtbCxMTExgbG6OoqIgNZm35xeHK2Nzc3BAZGdluO+7BgwezO24qv5PL5cjPz2fld/ScGrZO61v73bt3kZ+f3y4FwaqqKqSnp8PCwkLniCZdU7W5DniFhYU6HfDMzMy0gmVzendKc2OguMGYBktHR8cWf6bNTQu/d+8eysrKQAiBk5MTzMzMUFtb2+wdQnZ2NhYuXIiQkBCcPXuW910xl+HDhyM3N1frsZiYGPbv0dHROHjwIADg0KFDmDp1KkxNTTFw4EB4e3sjOTkZQ4YMabf1tZVuG5AprU1PVFRUIDMzEyYmJggNDeVV20qDJc0TU/MVqhS4d+8ea/ZuZWXFBjND5Vx0JpylpSUkEgmv0iKut7KuwpeZmRnMzMzYc+L6W5SUlCAnJ0dLfkcVK/RCV15ejoyMDNjZ2bX6AtjU2nNzc1FSUtJiD2RTU1P06dOHTcdQVz8azAoKClBdXQ2lUglra2sMHDiwRTn05oIyVw8dEBDAW4GMBmmaQ/f09ISzszOba+fOOOQ2fZibm4MQgm3btmHv3r3srriz2blzJ6ZMmQKgvi07Ojqafc7NzQ13797trKUZRLcNyNT+0s7Ojg3Cnd3Ywc0TOzs7IzIyUmsXbGVlBSsrK7i4uADQnjpNB5rS4hoN0tziGtX81tTUQCwW86rMAB7rcvv27YvBgwcbdHFgGAYWFhawsLBA37592feBXnyKi4uRmZkJtVrNXjRpUOBzR0+7+FxcXBAREdHmY9P0jJmZGRwcHJCVlQWNRgN/f3+oVCr2DqElDnjcoBwd5YAkqQxfLjJj1Rmurq6IiIjg9W5BpVKxHazcYbGmpqZ6d9Kpqal45513oFQq4ezsjMWLF8PDw4O3NbWWTz/9FCKRCNOnT+/spbSabhuQ09PTsWTJEsjlcvj5+UEikSAyMlLvhOKGjR3+/v68/uJTPbGZmRnCwsIMyq3SqdO2trbo378/gMfFNblcjvv377O7F6A+b+nh4cH72rkjlAxde1MwDMNefPr164eioiJWx21sbAyZTIa8vDwAaKRYaWkgpX6/hBDe73QAbWN3sVjMvu/USJ17hyCTyZCbm9ukAx43KH82rxdu3LgBhULRLmunFyk3NzetteuC7qTt7e0RHx8POzs7rF69GsbGxrh48SJOnjyJf/7zn7yuryXs3r0bR48excmTJ9nzcHV1RX5+PvuagoICuLq6dtYSDaLbFvUoSqUSN27cQFJSEi5cuIDU1FQYGRkhLCwM4eHhCA8Px9mzZ+Hs7Izw8HBefWEBbQ9hX1/fNrfeNqSkpIQ1ojc3N0dFRUWr8ra6aO0IJUOhqRVq0N9wjfrkd9xApkt+B2h3lLVWJtcUtbW1SE9Ph0gkgq+vb4vSQro07XRaMz2v45luGKA52aTdaWvhdvL5+/sbHOhv3bqFRYsWYfDgwfj444957QBsCbm5uRg7dixb1EtISMDbb7+NP//8U+tzvnHjBl5++WW2qPfcc88hMzPziS7qdfuA3BBCCCorK3Hx4kX89NNPOHjwINzc3ODo6Ijw8HBIJBIMHjy4zV8Crp6Y71Zq4HFRjWEY+Pj4aH056K6M2zatVqsNNuppOELJ1dWV14uUSqXC7du3UV5e3qKJxoC2lpjKChsqIKibXHvI5LiBns/mEZrGKSkpQX5+PlQqFUxNTbU+s7YWeoGWjVOiqFQqfP311zhw4AA2b96MoUOHtmkNbWHatGk4ffo0Hj58CGdnZ6xevRpr165FXV0d+1lER0fjm2++AVCfxti5cydEIhE2btyIUaNGddbShYDcFHV1dZg7dy7ee+89+Pr6oqioCMnJyexO+v79+/D29oZEIkFERATCwsJgZWVlkDERN088YMAAXgNCa2Vs1KGroSE+/aLTjjyanjAzM+N9hFJ7mdHT/GZpaSnu3bvHFtYcHBzY8+NDSkibJNor0FNpJQ30fDjgUagBk1qthp+fn8HvR3p6OhYtWoRhw4Zh9erVvKdNdOmKZTIZpkyZwkoS9+/fD3t7exBCEBsbi/j4eFhYWGD37t0IDw/ndT3tiBCQ24JarUZGRgakUimkUikuX74MpVKJ4OBgNkgHBARo3WZTZYapqSm8vb151RNzgxlfu1aaEpDL5SgrK0NpaSk0Gg2rJrC1tW1Wb2so1DKUTvDm04yeK/Fzd3dH37592SBNL0BcL5KWpnHUajVu374NuVzOm4UlFyrDs7KyatbaU5cDHoBGQZr7u1FcXIzs7Gy2WGoIKpUKW7ZswX/+8x989dVXiIqKattJ6kGXrnjZsmVwcHDA8uXLsW7dOpSWliIuLg7xq0ukeQAAG6hJREFU8fHYvHkz4uPjIZVKERsbC6lU2i7rageEgMw31dXVuHz5MpKTk5GcnIybN2/C2toa/v7+uHPnDkJCQvDWW2/xPvWA74GlXBruWvv06cMG6YZNLHQn3ZL/n9qflpaWtlhqZgjcBoymdvT60jgNh8823G0+fPgQWVlZvDu+AdoNHn5+fq1+b7gOeDTXTtUtlZWVrL+FobvbtLQ0LFy4EE8//TQ++OAD3nfFDWmYExaLxTh9+jRb8H3mmWeQkZGBuXPn4plnnsG0adMava4L0LMbQ9oDCwsLDBs2DMOGDQNQ/yWPi4vDtm3bEBUVhWvXriEmJgYDBgxAZGQkJBIJJBIJK71rKe0tY6NFNSsrK60GCUdHRzYfR/W2dBedl5cHhULBBrKGOmJKwzx0ZGQk78GMTgYxpFuNK7+jX2Cu/I5qvzUaDaytrWFhYQGZTIZevXq1i8JBLpcjIyODbThqy91Or169WDUO8HgQQE5ODhwcHKDRaNhidlMOeCqVCps2bcLhw4fx9ddfIzIyss3n2RqKi4vZz6hv374oLi4GUK8rpmoj4LGuuIsEZIMQAnIbYBgGERERiI2NZYtqGo0G2dnZkEqlOHHiBNatW4eqqioEBAQgIiICERERCA4ObjKdwXVja49ONaVSidu3b6OysrLZQM/V23KlXHSKNtUR00BGdbYFBQUwMzPjvTEF0HZNa0sw48rvqPabNmDk5+fD0tISdXV1uHr1aqutPBtC0x/l5eUIDAzkvauttrYWaWlpMDU1RVRUlFZahuuAl5ubyzrgHThwAHZ2djh69ChGjx6Ns2fP8ppuawuG9g90F4SA3Eaef/55rZ+NjIzg7e0Nb29vVqCuUChw9epVSKVS7NixA9euXYOJiQnCwsLYIO3t7Q0jIyPcuHEDlZWV6N27NwYPHsx74Yg7Qqk57ak+GIaBpaUlLC0ttZpY5HI5cnJyUF5eDmNjYza46WpiaQ20MKVSqfTqydsCd0zT0KFD2V0/N9fODWSGyO+4UN2vq6srfHx8eA003HZzX19fneoPkUgEe3t7LfliTU0Nfv75Z/z+++9wcnJCQkICrl27hiNHjvC2tpbi7OyMoqIiNmVBuz67oq64pQgBuQMwMTFhA++CBQtACKmfsXbhAqRSKT744AOkpaWxdp6LFi1q0SBKQ+COUOK7JRmoDza3b9+Gi4sLQkNDYWRkpOUQl5WVpSVR4zrfNQc32NAxXHzC1Vv7+fk10or36tULdnZ2WmkRpVLJnhu3QUeXARG131Qqle2S/qipqUFaWhosLCxa9Nlev34dixYtQkxMjNauuLKyktf1tZRx48Zhz549WL58Ofbs2YMXX3yRfXzLli2YOnUqpFIpbG1tu1W6AhCKek8Ev/32Gz766CMsWbIEDMOwRUOZTAZfX182mIeGhrZ4l9kwD823QqCmpgYZGRno1auXQdM1qHc0VUA018RSUVGB9PT0Fs3iawk0/dGvX782NwVxvS3Ky8tRW1sLhmFQW1sLFxcXuLu78668oZposVhscOOOUqnEF198gYSEBGzdurVTpWO6dMUvvfQSJk+ejLy8PLi7u2P//v1wcHAAIQRvvvkmEhISYGFhgV27diEiIqLT1t5CBJVFV4HurhoGG5VKhbS0NFYbffnyZRBCEBISwgZpsVisM0gRQlBQUNBuI5RoUY26mrXW1pPbWkwDNe1aoxNZAgICeFdnUGvPhqbxfEFzuUZGRujTpw9bPGyL/I5LVVUV0tLS2AuVoamta9euYdGiRRg1ahT+9a9/tftMuy+//BLbt28HwzAYNGgQdu3ahaKiIkydOhUlJSWQSCT44YcfesJsPSEgdzeoKuDixYvsLjojIwP29vasNjoyMhI3btxAeXk5QkNDeW9gAB7nQtvDSJ+qMzIzM2FnZwcjIyOtsVJc57u2msa3x4WKXgjv3r2rs5NPX9t0c/I7Cu0ALS4ubpFUTqFQYMOGDThx4gS++eYbhIaGtvlcm+Pu3bt46qmncPPmTZibm2Py5MkYPXo04uPjMWHCBEydOhVvvPEGQkJCMG/evHZfTycjBOSeAA1gUqkUJ0+exIEDB2Bubo6goCCEh4cjMjISYWFhsLGxaXPgqa2txa1bt0AIgVgsbpdcaEZGhk5/CK6vhVwu1yqs0SDdnG8v1SzTgbF8NqcAj3etNjY28PLyMvhCqK8jj6vssLa2ZieE0E5BQy+EV65cQWxsLMaOHYvly5d32G6U2l9euXIFNjY2eOmll7Bw4UJMnz4d9+7dg0gkwvnz5/Hhhx/i999/75A1dSJCQO5pvPzyy5g2bRrGjBmDzMxMJCUlITk5GZcuXUJtbS2CgoJY17vAwECDv5hcXw5vb28tW0Y+4B6/JekPbmGNTtA2NTVlAzRtYmnthI2WrJ9qotvS4NHwmNwLkEwmg0qlQu/eveHo6Mi2ujd1Aaqrq8P69euRmJiIb7/9FsHBwW1eV0vZtGkTVqxYAXNzc8TExGDTpk2Ijo5GVlYWACA/Px+jRo1im0K6MUJAFnhMXV0dUlNT2Xz09evXYWFhgfDwcDYf7eHh0WjXRaea9OnTR+fzbaW0tBS3bt2Ck5MT3N3d23x87sQSOn5JqVTCxsYGHh4esLW15bUwKJfLkZ6eztv6G0InhPTp0weurq5aO+mm5HepqamIjY3FSy+9hGXLlvF+N2AIpaWlmDhxIn7++WfY2dnhH//4ByZNmoQPP/xQCMh66NKyN13GJFyaMiPZs2cPPvnkEwDA+++/j9dee61D197R0EYB6klACEFpaSkuXLiApKQkHDx4kPXJiIiIgJeXF/773/9iwYIFkEgkvBe9uEW1QYMG8Tahmjax2NvbIysrC4QQ+Pn5QalUsgb7utIBLQ2karUaWVlZqKioQFBQEO8NHvomhJiYmGipKRrK7xYsWMBelJYsWYJJkybxrkwxlBMnTmDgwIGsJeaECRNw7tw5lJWVQaVSQSQSdUstcVvo0jtkXcYkXPSZkchkMkRERCAlJQUMw0AikeDixYu8+/12NTQaDW7fvo1PP/0U8fHxCAwMZLW5NNURHBzcpuBMCEFRURHu3LnTbkU12rLt7u6Ofv36NTo+NejhOt8ZGRlp+XU0JS/kGru7urry3klWVlaG9PR09OvXDwMGDDD4+JcuXcLixYsxcuRIREZG4vLly7h16xb27t3bKd1uUqkUs2bNwoULF2Bubo6ZM2ciIiICZ86cwcSJE9miXnBwMObPn9/h6+tguv8OWdfAQy6HDh3CjBkzwDAMoqOjUVZWhqKiIpw+fRovvPACm6t84YUXkJCQwJqW9FSMjIzg6OgILy8v5ObmwsLCAkqlEtevX0dSUhK+//57XL16Fb169WIN/iMjI+Hj42NQAYvbCdcezSncomBTLdvc4EuhbcVyuZwdKMBt9LC1tQXDMMjMzIRKpWqXBg+6666srERwcLDBdw21tbVYu3Ytzp8/jz179iAwMBAA2IaKziIqKgqTJk1CeHg4RCIRwsLCMGfOHIwZMwZTp07F+++/j7CwMLz++uudus4niS4dkJtDnxmJvscFAAcHB6xcuZL9mY5tCgsLw7x580AIQUVFBS5evIikpCR88sknbI6ZK73jGvxzHd90dcK1FUII8vLyUFRU1GpNtK62YtrEQlvCq6qqYGVlhd69e7P5W75ys7RBxc3NDb6+vgbvaFNSUvDWW29hypQpOH36dKelJ/SxevVqrF69WusxT09PJCcnd9KKnmyerE+vE5g1axb2798PKysrvPPOO42e37t3L+Li4kAIgbW1NbZu3YqQkBAAgIeHB2sQLhKJkJKS0tHL73AYhoGNjQ2effZZPPvsswAee2RQg/9vv/0WDx48YMc+paSksF1V7VX0cnBwaDQ0tq3QScsFBQWwtrZGeHg41Go15HK51gTths53LVmDSqVCZmYmampqWuTPUVtbizVr1kAqleLHH3+Ev79/a0/TYMrKyjB79mxcv34dDMNg586dEIvFOs3kBVpHtw7I+sxIXF1dcfr0aQDAzJkzIZfL9V6xBw4ciD///BP29vY4duwY5syZo2WKnZiYyLsMrKvBMAxcXV0xfvx4jB8/HgBQVFSE1157DYWFhYiMjMQbb7wBtVrdyOC/tTs67hgobtGLL5pq8DA3N280QVsul6OoqIjVadOiIZWn6boQPXz4EJmZmXB3d4efn5/Bu+Lk5GQsWbIE06ZNQ2JiYoftimNjYzFy5EgcPHgQCoUC1dXVWLNmDZ577jnWTH7dunWIi4vrkPV0R7p0UQ9obG7N5bfffsOWLVvYot6iRYtYjwiJRIJLly4BAAYNGgRra2ukpaU1+X+VlpYiKCiITW94eHggJSWlxwdkXZSVlSElJUXLDa+6uhqXLl1iuwxpEwU31WHIJBQ6F65///7tUlSjue6WNnhQmhvOamlpiby8PKhUKvj7+xvsb1FTU4NPPvkEly5dwrfffgs/P7/WnF6rkMvlCA0NRXZ2ttb7rc9MXqAR3V+HrMuYRKlUAgDeeOONJs1Idu7ciTVr1gAA5s6diz179jSrhdywYQPS09Oxfft2APW7Z3t7e9YzODMzU68E7/Tp03jxxRcxcOBAAPUSoFWrVgGon5obGxsLtVqN2bNnY/ny5fy8QU84hBA8fPgQycnJkEqlSE5ORkFBAdzd3VlttEQiYQtqpaWl7B2PWCzm3bOXNng8fPiQ91w3lacVFhbiwYMHMDY21kp1NDf3LykpCe+88w5eeeUVxMbGdvjk5NTUVMyZMwcBAQG4cuUKJBIJNm3aBFdXV5SVlQGo/zzt7e3ZnwW06P4BmS+a2mVTEhMTMX/+fJw9e5a9fb179y5cXV1x//59DBkyBO+99x42btyoNyBv2LABR48e1XpcrVbD19cXx48fZydr7Nu3DwEBAfyeZBeBSu9ogE5JSUFVVRVsbGxQWFiIzZs3Y8iQIbwH4/Zu8FAoFEhPTwfDMBCLxTAxMWH1wrSRhRoP2drawtTUFKampjAzM8PHH3+M1NRUfPfdd/D19eV1XYaSkpKC6OhonDt3DlFRUYiNjYWNjQ02b96sFYDt7e1RWlraKWt8wun+sreO4urVq5g9ezaOHTumlUukgnYnJye8+uqryM7ObvGxk5OT4e3tDU9PTwDA1KlTcejQoR4bkI2MjODj4wMfHx+88sorKCkpwYQJE+Dm5obx48dj3759WLFiBUxNTbUM/r28vFoVRGkuurKysl0aPLgDWBt6OdMmFvoY13jo6tWrWLFiBR48eAAPDw/MmDGDvfvrDNzc3ODm5sY2Fk2aNAnr1q3TayYv0DqEgNwMeXl5mDBhAn744Qet3UlVVRXb8VVVVYU//vgDc+fObfJY58+fR0hICFxcXLBhwwYEBgbqlOB1oUm67Y69vT22bt2qdYEihEAul7MG/ytXrkR2djZcXFxYbXRERAR69+7dZH6ZFtX69+/fIqmZodTW1iI9PR3GxsZaMwv1QSexAMCpU6fg7OyMgwcPora2lrVfpRrjjqZv377o378/MjIyIBaLcfLkSQQEBCAgIECnmbxA6+jxKYvm8tCzZ8/GL7/8And3dwBg5W3Z2dmsokClUuHll1/G9OnT9aY+ysvLYWRkBCsrK8THxyM2NhaZmZk4ePAgEhISsH37dsyaNQsHDx6EsbExSkpKGh1j/fr12Lt3L/t/pqWl4cGDB3BwcOiREjwuVItMUx0XLlxAaWlpI4N/c3Nz3Lt3D4WFhRCJRO3iWscdlaVvnJK+f3fu3Dm8++67mDVrFubPn9/hueKmSE1NxezZs6FQKODp6Yldu3ZBo9HoNJMXaISQQ+5oDMlFU6hCIzMzk7UfPHPmDPbv348DBw6wk3b1ceTIEXz55Zc4deqU1vEExcdjVCoVbty4AalUigsXLuDSpUsoKyuDQqHA3LlzMXLkSIjFYl6DHh2nZG5uDh8fH4MlaVVVVfjwww+Rnp6Obdu2wcvLi7c1CTwRGBSQ+a1cCOjl3r17oBe/5ORkaDQaODo6IjIyEpmZmcjJyUF0dDROnjxpUHV/3759Pb7VuzlEIhFCQkIwZ84cfPfddxCLxRg+fDi+/vprmJiYIC4uDsOGDcPo0aOxcuVKHDp0CIWFhWjhJgXA43FKV65cgYeHB/z9/Q0KxoQQnDlzBi+88AICAgJw/PjxDgvGarUaYWFhGDt2LAAgJycHUVFR8Pb2xpQpU6BQKDpkHQKPEXbIPNFc6mPLli3YunUrRCIRzM3N8cUXX2Do0KEA6k2QFi9eDLVajfHjxyMhIaHJXXZ1dTXc3NyQlZXF3h5yJXhz587FnDlz2v+kuxjFxcVwdnbWeowW3aRSKbuTvnfvHjw9PVlDpbCwMFhbW+vNMVPjeGtr6xbplisrK7Fq1SpkZWXhu+++YyWRHcUXX3yBlJQUlJeX4+jRo5g8eXJPnOTRURhWoCCEtOSPQDuTk5NDAgMDm3zNTz/9RMaOHav1WEFBASGEkOLiYuLn50dCQ0OJv78/CQgIIBs3bmx0DI1GQxYuXEi8vLzIoEGDyMWLF9nndu/eTby9vYm3tzfZvXs3D2fVtVCr1SQtLY3s2rWLzJs3j0RFRZGwsDDy6quvko0bN5Jz586RsrIyIpfLyeHDh8mpU6fI3bt3SVVVlUF/KisrSXx8PAkODiZbt24larW6w88xPz+fjBgxgpw8eZKMGTOGaDQa4ujoSJRKJSGEkP/9738kJiamw9fVjTEoxgoqiy7ITz/91ChdwZXgjR49Gmq1Ghs3bkRFRQUkEgl7S0w5duwYMjMzkZmZCalUinnz5rHWpKtXr9ayJh03blyP8icwMjKCn58f/Pz8MHPmTAD1iglq8P/VV1/h4sWLKC8vh0QiwaRJk+Dk5AQbG5tmpXcVFRVYuXIlcnNzcejQIXh4eLT/Celg8eLF+Oyzz9h5hSUlJbCzs2PTLILhVucg5JC7GHK5HH/++aeWvKiqqor9YlVVVeH8+fMYOXIkgPrBoP7+/o2+XPqsSX///XfWmtTe3p61Ju3pmJmZITo6GosXL8bMmTNhb2+PvXv3YsGCBcjJycGyZcsQHR2NiRMnYu3atTh+/DhkMhmbjyaEIDExETExMYiIiEBCQkKnBWM61EEikXTK/y+gH2GH/ATBzUO7ubk1ykMDwK+//oqYmBitBobi4uJGEjwakHNzc3H58mVW0E8RrElbz1NPPYUzZ86wumL6XtPW66SkJCQmJmL9+vWoqKiAr68v7t+/D3Nzcxw5cgQDBgzozOXj3LlzOHz4MOLj49luwdjYWGGSx5OAobkNIuSQuxwVFRUkPDyc/PLLL42eGzNmDPnrr7/Yn0eMGEEuXLhA1q9fTz7++GNCCCF5eXnEw8ODODk56c1F//jjj2TQoEEkKCiIDBkyhKSmprLPubu7k6CgIBISEkIkEkk7nOGTj0KhICkpKeSDDz7olFxxcyQmJpIxY8YQQgiZNGkS2bdvHyGEkLlz55KvvvqqM5fW3RByyD0ZpVKJiRMnYvr06ZgwYUKj5w2xJhWJRAgPD8ekSZMwduxYnblowZ60aYyNjSGRSLpEeiAuLk6Y5NHZGBq5ibBD7jJoNBry6quvktjYWL2vOXr0KBk5ciTRaDTk/PnzJDIykhBCSElJCfHw8CAymYzIZDLi4eFBSkpKCCGEjBs3jvzxxx96jymTyYiLiwv7s7u7O3nw4AFPZyUg0KURdsg9lXPnzuGHH37AoEGDEBoaCgBYs2YN8vLyANTno0ePHo34+Hh4e3uz1qTA4xFOkZGRAIBVq1bBwcFBby6ay44dOzBq1Cj2Z4ZhEBMTI2ij25H8/HzMmDEDxcXFYBgGc+bMQWxsLGQymTDJoytiaOQmwg65x9JULppy6tQp4ufnRx4+fMg+RrXRly5dIpaWlsTd3V1vLjoxMZHY2NiQkJAQEhISQlavXs0+d+zYMeLr60u8vLzI2rVreTyzrk9hYSGrIS8vLyc+Pj7kxo0bZOnSpex7tXbtWrJs2bLOXKaAgTFWCMgCTaJQKEhMTAz5/PPP9b7mypUrxNPTk2RkZOh8vrCwkMyZM4esX79eK2hw4RaXuKhUKuLp6Ulu375N6urqSHBwcKN/K/AYmlby9fUlhYWFhJD699/X17eTV9bjMSjGCjpkAb0QQvD666/D398fb7/9ts7XNGVPSrXRNjY2uHbtGoKCgvTqovXB9Ys2MTFh/aIFGsNNKxUXF6Nfv34A6q0zmzOrEngyEHLIAnoxJBf90UcfoaSkBPPnzwfw2J5Unza6qVy04BfdeiorKzFx4kRs3LixkTkVwzC8ez0LtBOGbqWJkLIQaCNN5aLlcjmpqKgghBDy22+/EW9vb0IIIQcOHCCvv/46IaReF+3v70/s7e315qI/++wzNg8dGBhIjIyMWJVId9VF60orCSmLJw4hZSHw5NCcLtrGxgZWVlYAgNGjR0OpVOLhw4daemmRSIQRI0Zg6dKlrKfEzZs3tY6zdOlSpKamIjU1FWvXrsXTTz+tZZiemJiI1NTUbmPgT/SklcaNG4c9e/YAgDDJowshBGSBdkdf0OBiiF+0o6Mj/vrrL4wbN86gXHRP8IymaaVTp04hNDQUoaGhiI+Px/Lly3H8+HH4+PjgxIkTPWaSeVdH8EMWaHfOnj2Lv/3tbxg0aBDrhtYwF22oX/SsWbOwYsUK5ObmYvjw4bh+/bpOQ3/BM1rgCUPwQxbonhiii27OMzooKIj4+fmR4OBgEhAQQFatWtXoGLW1tWTy5MnEy8uLDB48mOTk5LDPrVmzhnh5eRFfX1+SkJDAz4kJdGcMirEt3SELCHQqDMMYAzgK4HdCyBdNvO5XAAcIIf+n5/kPASgIIWseHfMsgFhCSBLnNfMBBBNC3mAYZiqA8YSQKQzDBADYB2AwABcAJwD4EkLU/JylQE9FyCELdBmYeu3WDgBpzQRjWwBPAzjEecySYRhr+ncAMQAuPXra+NGfhruTFwHsefT3gwCee7SGFwH8RAipI4TkAMhCfXAWEGgTgg5ZoCsxDMCrAK4xDJP66LF/ARgAAISQbx49Nh7AH4SQKs6/dQbw6yM9rgjA/wE4/ug43gC+IoQ0FDi7Ash/dGwVwzByAI6PHk/ivK7g0WMCAm1CCMgCXQZCyFkYUBwhhOwGsLvBY9kAQnS8PJRhGDvUB+sgQoj+6bICAu2MkLIQ6PEQQsoAJAIY2eCpuwD6AwDDMCIAtgBKuI8/wu3RYwICbUIIyAI9EoZh+jzaGYNhGHMALwBIb/CywwBee/T3SQBOkfoq+GEAUxmGMWUYZiAAHwDJHbNyge6MkLIQ6Kn0A7CHYZheqN+Y7CeEHGUY5iMAKYSQw6gvIP7AMEwWABmAqQBACLnBMMx+ADcBqAAsEBQWAnwgyN4EBAQEnhCElIWAgIDAE4IQkAUEBASeEISALCAgIPCE8P/d425bX8UprwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ad5af2780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def tune_ngram_model(data):\n",
    "    \"\"\"Tunes n-gram model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "    \"\"\"\n",
    "    # Select parameter values to try.\n",
    "    num_layers = [1, 2, 3]\n",
    "    num_units = [8, 16, 32, 64, 128]\n",
    "\n",
    "    # Save parameter combination and results.\n",
    "    params = {\n",
    "        'layers': [],\n",
    "        'units': [],\n",
    "        'accuracy': [],\n",
    "    }\n",
    "\n",
    "    # Iterate over all parameter combinations.\n",
    "    for layers in num_layers:\n",
    "        for units in num_units:\n",
    "                params['layers'].append(layers)\n",
    "                params['units'].append(units)\n",
    "\n",
    "                accuracy, _ = train_ngram_model(\n",
    "                    data=data,\n",
    "                    layers=layers,\n",
    "                    units=units)\n",
    "                print(('Accuracy: {accuracy}, Parameters: (layers={layers}, '\n",
    "                       'units={units})').format(accuracy=accuracy,\n",
    "                                                layers=layers,\n",
    "                                                units=units))\n",
    "                params['accuracy'].append(accuracy)\n",
    "    _plot_parameters(params)\n",
    "\n",
    "\n",
    "def _plot_parameters(params):\n",
    "    \"\"\"Creates a 3D surface plot of given parameters.\n",
    "    # Arguments\n",
    "        params: dict, contains layers, units and accuracy value combinations.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_trisurf(params['layers'],\n",
    "                    params['units'],\n",
    "                    params['accuracy'],\n",
    "                    cmap=cm.coolwarm,\n",
    "                    antialiased=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the IMDb movie reviews dataset to demonstrate training n-gram model\n",
    "    data = load_imdb_sentiment_analysis_dataset(FLAGS.data_dir)\n",
    "    tune_ngram_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
