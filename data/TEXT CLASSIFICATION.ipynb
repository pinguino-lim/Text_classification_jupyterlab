{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based by \"https://github.com/google/eng-edu/tree/master/ml/guides/text_classification\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_imdb_sentiment_analysis_dataset(data_path, seed=321):\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    # Arguments\n",
    "        data_path: string, path to the data directory.\n",
    "        seed: int, seed for randomizer.\n",
    "\n",
    "    # Returns\n",
    "        A tuple of training and validation data.\n",
    "        Number of training samples: 25000\n",
    "        Number of test samples: 25000\n",
    "        Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    # References\n",
    "        Mass et al., http://www.aclweb.org/anthology/P11-1015\n",
    "\n",
    "        Download and uncompress archive from:\n",
    "        http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname)) as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data.\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname)) as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels.\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_texts, train_labels), (test_texts, test_labels)) = load_imdb_sentiment_analysis_dataset(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def get_num_classes(labels):\n",
    "    \"\"\"Gets the total number of classes.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    # Returns\n",
    "        int, total number of classes.\n",
    "    # Raises\n",
    "        ValueError: if any label value in the range(0, num_classes - 1)\n",
    "            is missing or if number of classes is <= 1.\n",
    "    \"\"\"\n",
    "    num_classes = max(labels) + 1\n",
    "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
    "    if len(missing_classes):\n",
    "        raise ValueError('Missing samples with label value(s) '\n",
    "                         '{missing_classes}. Please make sure you have '\n",
    "                         'at least one sample for every label value '\n",
    "                         'in the range(0, {max_class})'.format(\n",
    "                            missing_classes=missing_classes,\n",
    "                            max_class=num_classes - 1))\n",
    "\n",
    "    if num_classes <= 1:\n",
    "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
    "                         'Please make sure there are at least two classes '\n",
    "                         'of samples'.format(num_classes=num_classes))\n",
    "    return num_classes\n",
    "\n",
    "\n",
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "    # Arguments\n",
    "        sample_texts: list, sample texts.\n",
    "    # Returns\n",
    "        int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)\n",
    "\n",
    "\n",
    "def plot_frequency_distribution_of_ngrams(sample_texts,\n",
    "                                          ngram_range=(1, 2),\n",
    "                                          num_ngrams=50):\n",
    "    \"\"\"Plots the frequency distribution of n-grams.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n",
    "            Min and mplt are the lower and upper bound values for the range.\n",
    "        num_ngrams: int, number of n-grams to plot.\n",
    "            Top `num_ngrams` frequent n-grams will be plotted.\n",
    "    \"\"\"\n",
    "    # Create args required for vectorizing.\n",
    "    kwargs = {\n",
    "            'ngram_range': (1, 1),\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': 'word',  # Split text into word tokens.\n",
    "    }\n",
    "    vectorizer = CountVectorizer(**kwargs)\n",
    "\n",
    "    # This creates a vocabulary (dict, where keys are n-grams and values are\n",
    "    # idxices). This also converts every text to an array the length of\n",
    "    # vocabulary, where every element idxicates the count of the n-gram\n",
    "    # corresponding at that idxex in vocabulary.\n",
    "    vectorized_texts = vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "    # This is the list of all n-grams in the index order from the vocabulary.\n",
    "    all_ngrams = list(vectorizer.get_feature_names())\n",
    "    num_ngrams = min(num_ngrams, len(all_ngrams))\n",
    "    # ngrams = all_ngrams[:num_ngrams]\n",
    "\n",
    "    # Add up the counts per n-gram ie. column-wise\n",
    "    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n",
    "\n",
    "    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n",
    "    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n",
    "        zip(all_counts, all_ngrams), reverse=True)])\n",
    "    ngrams = list(all_ngrams)[:num_ngrams]\n",
    "    counts = list(all_counts)[:num_ngrams]\n",
    "\n",
    "    idx = np.arange(num_ngrams)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('N-grams')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title('Frequency distribution of n-grams')\n",
    "    plt.xticks(idx, ngrams, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_sample_length_distribution(sample_texts):\n",
    "    \"\"\"Plots the sample length distribution.\n",
    "    # Arguments\n",
    "        samples_texts: list, sample texts.\n",
    "    \"\"\"\n",
    "    plt.hist([len(s) for s in sample_texts], 50)\n",
    "    plt.xlabel('Length of a sample')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Sample length distribution')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution(labels):\n",
    "    \"\"\"Plots the class distribution.\n",
    "    # Arguments\n",
    "        labels: list, label values.\n",
    "            There should be at lease one sample for values in the\n",
    "            range (0, num_classes -1)\n",
    "    \"\"\"\n",
    "    num_classes = get_num_classes(labels)\n",
    "    count_map = Counter(labels)\n",
    "    counts = [count_map[i] for i in range(num_classes)]\n",
    "    idx = np.arange(num_classes)\n",
    "    plt.bar(idx, counts, width=0.8, color='b')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title('Class distribution')\n",
    "    plt.xticks(idx, idx)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of the training sample: 174.0\n",
      "Number of the test sample: 172.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of the training sample:\",get_num_words_per_sample(train_texts))\n",
    "print(\"Number of the test sample:\",get_num_words_per_sample(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e0407a400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e06212dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sample_length_distribution(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0e040c7a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequency_distribution_of_ngrams(train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as ngram vectors.\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train)\n",
    "    x_val = selector.transform(x_val)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_val = x_val.astype('float32')\n",
    "    return x_train, x_val\n",
    "\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.67816091954023\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts)/get_num_words_per_sample(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val = ngram_vectorize(train_texts, train_labels, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0dc1759a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.spy(x_train, precision=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import initializers\n",
    "from tensorflow.python.keras import regularizers\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "    # Arguments\n",
    "        num_classes: int, number of classes.\n",
    "    # Returns\n",
    "        units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "    # Arguments\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of the layers.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "    # Returns\n",
    "        An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model\n",
    "\n",
    "\n",
    "def sepcnn_model(blocks,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 embedding_dim,\n",
    "                 dropout_rate,\n",
    "                 pool_size,\n",
    "                 input_shape,\n",
    "                 num_classes,\n",
    "                 num_features,\n",
    "                 use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False,\n",
    "                 embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "    # Arguments\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of the layers.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "        input_shape: tuple, shape of input to the model.\n",
    "        num_classes: int, number of output classes.\n",
    "        num_features: int, number of words (embedding input dimension).\n",
    "        use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "        is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "        embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "    # Returns\n",
    "        A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 21s - loss: 0.4715 - acc: 0.8538 - val_loss: 0.3228 - val_acc: 0.8874\n",
      "Epoch 2/1000\n",
      " - 19s - loss: 0.2330 - acc: 0.9192 - val_loss: 0.2515 - val_acc: 0.9021\n",
      "Epoch 3/1000\n",
      " - 19s - loss: 0.1688 - acc: 0.9412 - val_loss: 0.2325 - val_acc: 0.9063\n",
      "Epoch 4/1000\n",
      " - 20s - loss: 0.1357 - acc: 0.9537 - val_loss: 0.2310 - val_acc: 0.9052\n",
      "Epoch 5/1000\n",
      " - 19s - loss: 0.1108 - acc: 0.9638 - val_loss: 0.2326 - val_acc: 0.9047\n",
      "Epoch 6/1000\n",
      " - 20s - loss: 0.0929 - acc: 0.9698 - val_loss: 0.2395 - val_acc: 0.9018\n",
      "Validation accuracy: 0.90176, loss: 0.23947998819351196\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def train_ngram_model(data,\n",
    "                      learning_rate=1e-3,\n",
    "                      epochs=1000,\n",
    "                      batch_size=128,\n",
    "                      layers=2,\n",
    "                      units=64,\n",
    "                      dropout_rate=0.2):\n",
    "    \"\"\"Trains n-gram model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        layers: int, number of `Dense` layers in the model.\n",
    "        units: int, output dimension of Dense layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val = ngram_vectorize(\n",
    "        train_texts, train_labels, val_texts)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = mlp_model(layers=layers,\n",
    "                                  units=units,\n",
    "                                  dropout_rate=dropout_rate,\n",
    "                                  input_shape=x_train.shape[1:],\n",
    "                                  num_classes=num_classes)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('imdb_mlp_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the IMDb movie reviews dataset to demonstrate training n-gram model\n",
    "    data = load_imdb_sentiment_analysis_dataset(FLAGS.data_dir)\n",
    "    train_ngram_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def train_sequence_model(data,\n",
    "                         learning_rate=1e-3,\n",
    "                         epochs=1000,\n",
    "                         batch_size=128,\n",
    "                         blocks=2,\n",
    "                         filters=64,\n",
    "                         dropout_rate=0.2,\n",
    "                         embedding_dim=200,\n",
    "                         kernel_size=3,\n",
    "                         pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(\n",
    "            x_train,\n",
    "            train_labels,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(x_val, val_labels),\n",
    "            verbose=2,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('rotten_tomatoes_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the Rotten tomatoes movie reviews dataset to demonstrate\n",
    "    # training sequence model.\n",
    "    data = load_rotten_tomatoes_sentiment_analysis_dataset(\n",
    "            FLAGS.data_dir)\n",
    "    train_sequence_model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train fine tuned sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def _get_embedding_matrix(word_index, embedding_data_dir, embedding_dim):\n",
    "    \"\"\"Gets embedding matrix from the embedding index data.\n",
    "    # Arguments\n",
    "        word_index: dict, word to index map that was generated from the data.\n",
    "        embedding_data_dir: string, path to the pre-training embeddings.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "    # Returns\n",
    "        dict, word vectors for words in word_index from pre-trained embedding.\n",
    "    # References:\n",
    "        https://nlp.stanford.edu/projects/glove/\n",
    "        Download and uncompress archive from:\n",
    "        http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "\n",
    "    # We are using 200d GloVe embeddings.\n",
    "    fname = os.path.join(embedding_data_dir, 'glove.6B.200d.txt')\n",
    "    with open(fname) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def train_fine_tuned_sequence_model(data,\n",
    "                                    embedding_data_dir,\n",
    "                                    learning_rate=1e-3,\n",
    "                                    epochs=1000,\n",
    "                                    batch_size=128,\n",
    "                                    blocks=2,\n",
    "                                    filters=64,\n",
    "                                    dropout_rate=0.2,\n",
    "                                    embedding_dim=200,\n",
    "                                    kernel_size=3,\n",
    "                                    pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        embedding_data_dir: string, path to the pre-training embeddings.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    embedding_matrix = _get_embedding_matrix(\n",
    "        word_index, embedding_data_dir, embedding_dim)\n",
    "\n",
    "    # Create model instance. First time we will train rest of network while\n",
    "    # keeping embedding layer weights frozen. So, we set\n",
    "    # is_embedding_trainable as False.\n",
    "    model = build_model.sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features,\n",
    "                                     use_pretrained_embedding=True,\n",
    "                                     is_embedding_trainable=False,\n",
    "                                     embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Train and validate model.\n",
    "    model.fit(x_train,\n",
    "              train_labels,\n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_val, val_labels),\n",
    "              verbose=2,  # Logs once per epoch.\n",
    "              batch_size=batch_size)\n",
    "\n",
    "    # Save the model.\n",
    "    model.save_weights('sequence_model_with_pre_trained_embedding.h5')\n",
    "\n",
    "    # Create another model instance. This time we will unfreeze the embedding\n",
    "    # layer and let it fine-tune to the given dataset.\n",
    "    model = sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features,\n",
    "                                     use_pretrained_embedding=True,\n",
    "                                     is_embedding_trainable=True,\n",
    "                                     embedding_matrix=embedding_matrix)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Load the weights that we had saved into this new model.\n",
    "    model.load_weights('sequence_model_with_pre_trained_embedding.h5')\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(x_train,\n",
    "                        train_labels,\n",
    "                        epochs=epochs,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=(x_val, val_labels),\n",
    "                        verbose=2,  # Logs once per epoch.\n",
    "                        batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('tweet_weather_sepcnn_fine_tuned_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='./data',\n",
    "                        help='input data directory')\n",
    "    parser.add_argument('--embedding_data_dir', type=str, default='./data',\n",
    "                        help='embedding input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the tweet weather topic classification dataset to demonstrate\n",
    "    # training sequence model with fine-tuned pre-trained embedding.\n",
    "    data = load_tweet_weather_topic_classification_dataset(\n",
    "            FLAGS.data_dir)\n",
    "    train_fine_tuned_sequence_model(data, FLAGS.embedding_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch train sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "\n",
    "def _data_generator(x, y, num_features, batch_size):\n",
    "    \"\"\"Generates batches of vectorized texts for training/validation.\n",
    "    # Arguments\n",
    "        x: np.matrix, feature matrix.\n",
    "        y: np.ndarray, labels.\n",
    "        num_features: int, number of features.\n",
    "        batch_size: int, number of samples per batch.\n",
    "    # Returns\n",
    "        Yields feature and label data in batches.\n",
    "    \"\"\"\n",
    "    num_samples = x.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size:\n",
    "        num_batches += 1\n",
    "\n",
    "    while 1:\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            if end_idx > num_samples:\n",
    "                end_idx = num_samples\n",
    "            x_batch = x[start_idx:end_idx]\n",
    "            y_batch = y[start_idx:end_idx]\n",
    "            yield x_batch, y_batch\n",
    "\n",
    "\n",
    "def batch_train_sequence_model(data,\n",
    "                               learning_rate=1e-3,\n",
    "                               epochs=1000,\n",
    "                               batch_size=128,\n",
    "                               blocks=2,\n",
    "                               filters=64,\n",
    "                               dropout_rate=0.2,\n",
    "                               embedding_dim=200,\n",
    "                               kernel_size=3,\n",
    "                               pool_size=3):\n",
    "    \"\"\"Trains sequence model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "        learning_rate: float, learning rate for training model.\n",
    "        epochs: int, number of epochs.\n",
    "        batch_size: int, number of samples per batch.\n",
    "        blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "        filters: int, output dimension of sepCNN layers in the model.\n",
    "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
    "        embedding_dim: int, dimension of the embedding vectors.\n",
    "        kernel_size: int, length of the convolution window.\n",
    "        pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    # Raises\n",
    "        ValueError: If validation data has label values which were not seen\n",
    "            in the training data.\n",
    "    \"\"\"\n",
    "    # Get the data.\n",
    "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
    "\n",
    "    # Verify that validation labels are in the same range as training labels.\n",
    "    num_classes = get_num_classes(train_labels)\n",
    "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
    "    if len(unexpected_labels):\n",
    "        raise ValueError('Unexpected label values found in the validation set:'\n",
    "                         ' {unexpected_labels}. Please make sure that the '\n",
    "                         'labels in the validation set are in the same range '\n",
    "                         'as training labels.'.format(\n",
    "                             unexpected_labels=unexpected_labels))\n",
    "\n",
    "    # Vectorize texts.\n",
    "    x_train, x_val, word_index = sequence_vectorize(\n",
    "            train_texts, val_texts)\n",
    "\n",
    "    # Number of features will be the embedding input dimension. Add 1 for the\n",
    "    # reserved index 0.\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Create model instance.\n",
    "    model = build_model.sepcnn_model(blocks=blocks,\n",
    "                                     filters=filters,\n",
    "                                     kernel_size=kernel_size,\n",
    "                                     embedding_dim=embedding_dim,\n",
    "                                     dropout_rate=dropout_rate,\n",
    "                                     pool_size=pool_size,\n",
    "                                     input_shape=x_train.shape[1:],\n",
    "                                     num_classes=num_classes,\n",
    "                                     num_features=num_features)\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    if num_classes == 2:\n",
    "        loss = 'binary_crossentropy'\n",
    "    else:\n",
    "        loss = 'sparse_categorical_crossentropy'\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "    # Create callback for early stopping on validation loss. If the loss does\n",
    "    # not decrease in two consecutive tries, stop training.\n",
    "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "    # Create training and validation generators.\n",
    "    training_generator = _data_generator(\n",
    "        x_train, train_labels, num_features, batch_size)\n",
    "    validation_generator = _data_generator(\n",
    "        x_val, val_labels, num_features, batch_size)\n",
    "\n",
    "    # Get number of training steps. This indicated the number of steps it takes\n",
    "    # to cover all samples in one epoch.\n",
    "    steps_per_epoch = x_train.shape[0] // batch_size\n",
    "    if x_train.shape[0] % batch_size:\n",
    "        steps_per_epoch += 1\n",
    "\n",
    "    # Get number of validation steps.\n",
    "    validation_steps = x_val.shape[0] // batch_size\n",
    "    if x_val.shape[0] % batch_size:\n",
    "        validation_steps += 1\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit_generator(\n",
    "            generator=training_generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=callbacks,\n",
    "            epochs=epochs,\n",
    "            verbose=2)  # Logs once per epoch.\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Save model.\n",
    "    model.save('amazon_reviews_sepcnn_model.h5')\n",
    "    return history['val_acc'][-1], history['val_loss'][-1]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the Amazon reviews dataset to demonstrate training of\n",
    "    # sequence model with batches of data.\n",
    "    data = load_amazon_reviews_sentiment_analysis_dataset(\n",
    "            FLAGS.data_dir)\n",
    "    batch_train_sequence_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1000\n",
      " - 13s - loss: 0.6688 - acc: 0.8324 - val_loss: 0.6483 - val_acc: 0.8429\n",
      "Epoch 2/1000\n",
      " - 12s - loss: 0.6224 - acc: 0.8758 - val_loss: 0.6100 - val_acc: 0.8517\n",
      "Epoch 3/1000\n",
      " - 13s - loss: 0.5826 - acc: 0.8791 - val_loss: 0.5771 - val_acc: 0.8560\n",
      "Epoch 4/1000\n",
      " - 12s - loss: 0.5484 - acc: 0.8835 - val_loss: 0.5488 - val_acc: 0.8593\n",
      "Epoch 5/1000\n",
      " - 14s - loss: 0.5187 - acc: 0.8858 - val_loss: 0.5242 - val_acc: 0.8614\n",
      "Epoch 6/1000\n",
      " - 13s - loss: 0.4923 - acc: 0.8873 - val_loss: 0.5026 - val_acc: 0.8639\n",
      "Epoch 7/1000\n",
      " - 13s - loss: 0.4699 - acc: 0.8901 - val_loss: 0.4836 - val_acc: 0.8658\n",
      "Epoch 8/1000\n",
      " - 13s - loss: 0.4500 - acc: 0.8917 - val_loss: 0.4668 - val_acc: 0.8674\n",
      "Epoch 9/1000\n",
      " - 15s - loss: 0.4318 - acc: 0.8964 - val_loss: 0.4517 - val_acc: 0.8697\n",
      "Epoch 10/1000\n",
      " - 15s - loss: 0.4154 - acc: 0.8989 - val_loss: 0.4381 - val_acc: 0.8712\n",
      "Epoch 11/1000\n",
      " - 14s - loss: 0.4015 - acc: 0.8992 - val_loss: 0.4259 - val_acc: 0.8728\n",
      "Epoch 12/1000\n",
      " - 15s - loss: 0.3880 - acc: 0.9012 - val_loss: 0.4147 - val_acc: 0.8752\n",
      "Epoch 13/1000\n",
      " - 13s - loss: 0.3757 - acc: 0.9043 - val_loss: 0.4046 - val_acc: 0.8767\n",
      "Epoch 14/1000\n",
      " - 14s - loss: 0.3637 - acc: 0.9078 - val_loss: 0.3952 - val_acc: 0.8780\n",
      "Epoch 15/1000\n",
      " - 15s - loss: 0.3534 - acc: 0.9076 - val_loss: 0.3867 - val_acc: 0.8797\n",
      "Epoch 16/1000\n",
      " - 14s - loss: 0.3438 - acc: 0.9077 - val_loss: 0.3787 - val_acc: 0.8815\n",
      "Epoch 17/1000\n",
      " - 14s - loss: 0.3355 - acc: 0.9096 - val_loss: 0.3714 - val_acc: 0.8826\n",
      "Epoch 18/1000\n",
      " - 14s - loss: 0.3265 - acc: 0.9129 - val_loss: 0.3644 - val_acc: 0.8835\n",
      "Epoch 19/1000\n",
      " - 13s - loss: 0.3191 - acc: 0.9116 - val_loss: 0.3582 - val_acc: 0.8851\n",
      "Epoch 20/1000\n",
      " - 15s - loss: 0.3115 - acc: 0.9164 - val_loss: 0.3521 - val_acc: 0.8863\n",
      "Epoch 21/1000\n",
      " - 15s - loss: 0.3056 - acc: 0.9155 - val_loss: 0.3466 - val_acc: 0.8875\n",
      "Epoch 22/1000\n",
      " - 15s - loss: 0.2984 - acc: 0.9170 - val_loss: 0.3414 - val_acc: 0.8880\n",
      "Epoch 23/1000\n",
      " - 14s - loss: 0.2923 - acc: 0.9185 - val_loss: 0.3366 - val_acc: 0.8890\n",
      "Epoch 24/1000\n",
      " - 14s - loss: 0.2862 - acc: 0.9205 - val_loss: 0.3318 - val_acc: 0.8900\n",
      "Epoch 25/1000\n",
      " - 13s - loss: 0.2811 - acc: 0.9220 - val_loss: 0.3275 - val_acc: 0.8908\n",
      "Epoch 26/1000\n",
      " - 14s - loss: 0.2753 - acc: 0.9226 - val_loss: 0.3236 - val_acc: 0.8914\n",
      "Epoch 27/1000\n",
      " - 13s - loss: 0.2714 - acc: 0.9237 - val_loss: 0.3195 - val_acc: 0.8920\n",
      "Epoch 28/1000\n",
      " - 14s - loss: 0.2660 - acc: 0.9252 - val_loss: 0.3160 - val_acc: 0.8926\n",
      "Epoch 29/1000\n",
      " - 14s - loss: 0.2620 - acc: 0.9248 - val_loss: 0.3125 - val_acc: 0.8930\n",
      "Epoch 30/1000\n",
      " - 13s - loss: 0.2580 - acc: 0.9250 - val_loss: 0.3093 - val_acc: 0.8938\n",
      "Epoch 31/1000\n",
      " - 15s - loss: 0.2533 - acc: 0.9285 - val_loss: 0.3061 - val_acc: 0.8943\n",
      "Epoch 32/1000\n",
      " - 14s - loss: 0.2496 - acc: 0.9286 - val_loss: 0.3032 - val_acc: 0.8948\n",
      "Epoch 33/1000\n",
      " - 15s - loss: 0.2466 - acc: 0.9291 - val_loss: 0.3004 - val_acc: 0.8954\n",
      "Epoch 34/1000\n",
      " - 13s - loss: 0.2429 - acc: 0.9300 - val_loss: 0.2978 - val_acc: 0.8962\n",
      "Epoch 35/1000\n",
      " - 18s - loss: 0.2384 - acc: 0.9316 - val_loss: 0.2954 - val_acc: 0.8958\n",
      "Epoch 36/1000\n",
      " - 14s - loss: 0.2357 - acc: 0.9329 - val_loss: 0.2930 - val_acc: 0.8965\n",
      "Epoch 37/1000\n",
      " - 17s - loss: 0.2337 - acc: 0.9322 - val_loss: 0.2907 - val_acc: 0.8974\n",
      "Epoch 38/1000\n",
      " - 14s - loss: 0.2294 - acc: 0.9337 - val_loss: 0.2884 - val_acc: 0.8980\n",
      "Epoch 39/1000\n",
      " - 13s - loss: 0.2266 - acc: 0.9343 - val_loss: 0.2863 - val_acc: 0.8982\n",
      "Epoch 40/1000\n",
      " - 12s - loss: 0.2242 - acc: 0.9350 - val_loss: 0.2842 - val_acc: 0.8982\n",
      "Epoch 41/1000\n",
      " - 12s - loss: 0.2219 - acc: 0.9348 - val_loss: 0.2824 - val_acc: 0.8985\n",
      "Epoch 42/1000\n",
      " - 13s - loss: 0.2189 - acc: 0.9360 - val_loss: 0.2805 - val_acc: 0.8990\n",
      "Epoch 43/1000\n",
      " - 15s - loss: 0.2161 - acc: 0.9376 - val_loss: 0.2788 - val_acc: 0.8993\n",
      "Epoch 44/1000\n",
      " - 15s - loss: 0.2140 - acc: 0.9366 - val_loss: 0.2771 - val_acc: 0.8996\n",
      "Epoch 45/1000\n",
      " - 13s - loss: 0.2113 - acc: 0.9384 - val_loss: 0.2754 - val_acc: 0.8998\n",
      "Epoch 46/1000\n",
      " - 14s - loss: 0.2094 - acc: 0.9388 - val_loss: 0.2740 - val_acc: 0.9002\n",
      "Epoch 47/1000\n",
      " - 13s - loss: 0.2061 - acc: 0.9398 - val_loss: 0.2726 - val_acc: 0.9005\n",
      "Epoch 48/1000\n",
      " - 12s - loss: 0.2047 - acc: 0.9399 - val_loss: 0.2712 - val_acc: 0.9009\n",
      "Epoch 49/1000\n",
      " - 12s - loss: 0.2010 - acc: 0.9422 - val_loss: 0.2698 - val_acc: 0.9012\n",
      "Epoch 50/1000\n",
      " - 12s - loss: 0.1995 - acc: 0.9420 - val_loss: 0.2685 - val_acc: 0.9014\n",
      "Epoch 51/1000\n",
      " - 11s - loss: 0.1974 - acc: 0.9430 - val_loss: 0.2673 - val_acc: 0.9012\n",
      "Epoch 52/1000\n",
      " - 11s - loss: 0.1952 - acc: 0.9428 - val_loss: 0.2661 - val_acc: 0.9017\n",
      "Epoch 53/1000\n",
      " - 12s - loss: 0.1952 - acc: 0.9430 - val_loss: 0.2649 - val_acc: 0.9018\n",
      "Epoch 54/1000\n",
      " - 11s - loss: 0.1922 - acc: 0.9436 - val_loss: 0.2637 - val_acc: 0.9023\n",
      "Epoch 55/1000\n",
      " - 11s - loss: 0.1907 - acc: 0.9434 - val_loss: 0.2628 - val_acc: 0.9026\n",
      "Epoch 56/1000\n",
      " - 11s - loss: 0.1881 - acc: 0.9448 - val_loss: 0.2618 - val_acc: 0.9027\n",
      "Epoch 57/1000\n",
      " - 12s - loss: 0.1870 - acc: 0.9456 - val_loss: 0.2607 - val_acc: 0.9031\n",
      "Epoch 58/1000\n",
      " - 12s - loss: 0.1851 - acc: 0.9461 - val_loss: 0.2597 - val_acc: 0.9032\n",
      "Epoch 59/1000\n",
      " - 11s - loss: 0.1838 - acc: 0.9456 - val_loss: 0.2590 - val_acc: 0.9031\n",
      "Epoch 60/1000\n",
      " - 11s - loss: 0.1821 - acc: 0.9473 - val_loss: 0.2580 - val_acc: 0.9034\n",
      "Epoch 61/1000\n",
      " - 12s - loss: 0.1808 - acc: 0.9475 - val_loss: 0.2571 - val_acc: 0.9035\n",
      "Epoch 62/1000\n",
      " - 12s - loss: 0.1789 - acc: 0.9474 - val_loss: 0.2563 - val_acc: 0.9036\n",
      "Epoch 63/1000\n",
      " - 12s - loss: 0.1772 - acc: 0.9486 - val_loss: 0.2555 - val_acc: 0.9040\n",
      "Epoch 64/1000\n",
      " - 12s - loss: 0.1764 - acc: 0.9479 - val_loss: 0.2546 - val_acc: 0.9042\n",
      "Epoch 65/1000\n",
      " - 11s - loss: 0.1755 - acc: 0.9489 - val_loss: 0.2538 - val_acc: 0.9043\n",
      "Epoch 66/1000\n",
      " - 12s - loss: 0.1728 - acc: 0.9508 - val_loss: 0.2532 - val_acc: 0.9043\n",
      "Epoch 67/1000\n",
      " - 12s - loss: 0.1725 - acc: 0.9498 - val_loss: 0.2525 - val_acc: 0.9046\n",
      "Epoch 68/1000\n",
      " - 12s - loss: 0.1709 - acc: 0.9495 - val_loss: 0.2518 - val_acc: 0.9049\n",
      "Epoch 69/1000\n",
      " - 11s - loss: 0.1691 - acc: 0.9498 - val_loss: 0.2514 - val_acc: 0.9043\n",
      "Epoch 70/1000\n",
      " - 12s - loss: 0.1679 - acc: 0.9516 - val_loss: 0.2506 - val_acc: 0.9046\n",
      "Epoch 71/1000\n",
      " - 12s - loss: 0.1669 - acc: 0.9520 - val_loss: 0.2500 - val_acc: 0.9049\n",
      "Epoch 72/1000\n",
      " - 12s - loss: 0.1665 - acc: 0.9515 - val_loss: 0.2496 - val_acc: 0.9048\n",
      "Epoch 73/1000\n",
      " - 11s - loss: 0.1646 - acc: 0.9526 - val_loss: 0.2488 - val_acc: 0.9054\n",
      "Epoch 74/1000\n",
      " - 12s - loss: 0.1632 - acc: 0.9528 - val_loss: 0.2484 - val_acc: 0.9054\n",
      "Epoch 75/1000\n",
      " - 12s - loss: 0.1619 - acc: 0.9529 - val_loss: 0.2478 - val_acc: 0.9056\n",
      "Epoch 76/1000\n",
      " - 12s - loss: 0.1606 - acc: 0.9556 - val_loss: 0.2471 - val_acc: 0.9057\n",
      "Epoch 77/1000\n",
      " - 12s - loss: 0.1605 - acc: 0.9532 - val_loss: 0.2468 - val_acc: 0.9058\n",
      "Epoch 78/1000\n",
      " - 12s - loss: 0.1587 - acc: 0.9551 - val_loss: 0.2464 - val_acc: 0.9055\n",
      "Epoch 79/1000\n",
      " - 12s - loss: 0.1581 - acc: 0.9547 - val_loss: 0.2458 - val_acc: 0.9058\n",
      "Epoch 80/1000\n",
      " - 11s - loss: 0.1566 - acc: 0.9560 - val_loss: 0.2454 - val_acc: 0.9061\n",
      "Epoch 81/1000\n",
      " - 11s - loss: 0.1552 - acc: 0.9546 - val_loss: 0.2449 - val_acc: 0.9063\n",
      "Epoch 82/1000\n",
      " - 11s - loss: 0.1543 - acc: 0.9551 - val_loss: 0.2446 - val_acc: 0.9063\n",
      "Epoch 83/1000\n",
      " - 11s - loss: 0.1540 - acc: 0.9557 - val_loss: 0.2441 - val_acc: 0.9064\n",
      "Epoch 84/1000\n",
      " - 12s - loss: 0.1526 - acc: 0.9565 - val_loss: 0.2438 - val_acc: 0.9062\n",
      "Epoch 85/1000\n",
      " - 11s - loss: 0.1513 - acc: 0.9568 - val_loss: 0.2434 - val_acc: 0.9062\n",
      "Epoch 86/1000\n",
      " - 11s - loss: 0.1507 - acc: 0.9574 - val_loss: 0.2429 - val_acc: 0.9064\n",
      "Epoch 87/1000\n",
      " - 11s - loss: 0.1501 - acc: 0.9576 - val_loss: 0.2426 - val_acc: 0.9061\n",
      "Epoch 88/1000\n",
      " - 13s - loss: 0.1485 - acc: 0.9576 - val_loss: 0.2423 - val_acc: 0.9059\n",
      "Epoch 89/1000\n",
      " - 11s - loss: 0.1481 - acc: 0.9573 - val_loss: 0.2421 - val_acc: 0.9059\n",
      "Epoch 90/1000\n",
      " - 11s - loss: 0.1475 - acc: 0.9587 - val_loss: 0.2418 - val_acc: 0.9059\n",
      "Epoch 91/1000\n",
      " - 11s - loss: 0.1456 - acc: 0.9584 - val_loss: 0.2414 - val_acc: 0.9060\n",
      "Epoch 92/1000\n",
      " - 11s - loss: 0.1449 - acc: 0.9599 - val_loss: 0.2410 - val_acc: 0.9061\n",
      "Epoch 93/1000\n",
      " - 11s - loss: 0.1443 - acc: 0.9594 - val_loss: 0.2408 - val_acc: 0.9061\n",
      "Epoch 94/1000\n",
      " - 11s - loss: 0.1438 - acc: 0.9587 - val_loss: 0.2405 - val_acc: 0.9061\n",
      "Epoch 95/1000\n",
      " - 12s - loss: 0.1424 - acc: 0.9596 - val_loss: 0.2402 - val_acc: 0.9064\n",
      "Epoch 96/1000\n",
      " - 11s - loss: 0.1412 - acc: 0.9597 - val_loss: 0.2401 - val_acc: 0.9057\n",
      "Epoch 97/1000\n",
      " - 11s - loss: 0.1400 - acc: 0.9610 - val_loss: 0.2397 - val_acc: 0.9062\n",
      "Epoch 98/1000\n",
      " - 12s - loss: 0.1408 - acc: 0.9598 - val_loss: 0.2394 - val_acc: 0.9065\n",
      "Epoch 99/1000\n",
      " - 12s - loss: 0.1390 - acc: 0.9609 - val_loss: 0.2393 - val_acc: 0.9060\n",
      "Epoch 100/1000\n",
      " - 12s - loss: 0.1387 - acc: 0.9610 - val_loss: 0.2390 - val_acc: 0.9062\n",
      "Epoch 101/1000\n",
      " - 11s - loss: 0.1376 - acc: 0.9608 - val_loss: 0.2386 - val_acc: 0.9066\n",
      "Epoch 102/1000\n",
      " - 11s - loss: 0.1376 - acc: 0.9619 - val_loss: 0.2385 - val_acc: 0.9064\n",
      "Epoch 103/1000\n",
      " - 12s - loss: 0.1361 - acc: 0.9623 - val_loss: 0.2383 - val_acc: 0.9064\n",
      "Epoch 104/1000\n",
      " - 12s - loss: 0.1355 - acc: 0.9625 - val_loss: 0.2381 - val_acc: 0.9065\n",
      "Epoch 105/1000\n",
      " - 12s - loss: 0.1339 - acc: 0.9625 - val_loss: 0.2380 - val_acc: 0.9064\n",
      "Epoch 106/1000\n",
      " - 12s - loss: 0.1334 - acc: 0.9621 - val_loss: 0.2377 - val_acc: 0.9066\n",
      "Epoch 107/1000\n",
      " - 12s - loss: 0.1328 - acc: 0.9630 - val_loss: 0.2375 - val_acc: 0.9068\n",
      "Epoch 108/1000\n",
      " - 12s - loss: 0.1311 - acc: 0.9631 - val_loss: 0.2374 - val_acc: 0.9068\n",
      "Epoch 109/1000\n",
      " - 12s - loss: 0.1322 - acc: 0.9630 - val_loss: 0.2373 - val_acc: 0.9069\n",
      "Epoch 110/1000\n",
      " - 12s - loss: 0.1304 - acc: 0.9629 - val_loss: 0.2369 - val_acc: 0.9066\n",
      "Epoch 111/1000\n",
      " - 12s - loss: 0.1296 - acc: 0.9644 - val_loss: 0.2368 - val_acc: 0.9066\n",
      "Epoch 112/1000\n",
      " - 11s - loss: 0.1287 - acc: 0.9654 - val_loss: 0.2366 - val_acc: 0.9067\n",
      "Epoch 113/1000\n",
      " - 11s - loss: 0.1295 - acc: 0.9646 - val_loss: 0.2365 - val_acc: 0.9066\n",
      "Epoch 114/1000\n",
      " - 12s - loss: 0.1282 - acc: 0.9652 - val_loss: 0.2364 - val_acc: 0.9068\n",
      "Epoch 115/1000\n",
      " - 12s - loss: 0.1275 - acc: 0.9645 - val_loss: 0.2363 - val_acc: 0.9066\n",
      "Epoch 116/1000\n",
      " - 12s - loss: 0.1272 - acc: 0.9643 - val_loss: 0.2362 - val_acc: 0.9065\n",
      "Epoch 117/1000\n",
      " - 12s - loss: 0.1266 - acc: 0.9649 - val_loss: 0.2361 - val_acc: 0.9067\n",
      "Epoch 118/1000\n",
      " - 12s - loss: 0.1257 - acc: 0.9650 - val_loss: 0.2361 - val_acc: 0.9065\n",
      "Epoch 119/1000\n",
      " - 11s - loss: 0.1249 - acc: 0.9655 - val_loss: 0.2358 - val_acc: 0.9062\n",
      "Epoch 120/1000\n",
      " - 11s - loss: 0.1235 - acc: 0.9673 - val_loss: 0.2357 - val_acc: 0.9062\n",
      "Epoch 121/1000\n",
      " - 11s - loss: 0.1228 - acc: 0.9659 - val_loss: 0.2357 - val_acc: 0.9060\n",
      "Epoch 122/1000\n",
      " - 12s - loss: 0.1242 - acc: 0.9654 - val_loss: 0.2355 - val_acc: 0.9062\n",
      "Epoch 123/1000\n",
      " - 12s - loss: 0.1222 - acc: 0.9660 - val_loss: 0.2355 - val_acc: 0.9059\n",
      "Epoch 124/1000\n",
      " - 12s - loss: 0.1221 - acc: 0.9668 - val_loss: 0.2355 - val_acc: 0.9060\n",
      "Epoch 125/1000\n",
      " - 11s - loss: 0.1203 - acc: 0.9670 - val_loss: 0.2353 - val_acc: 0.9060\n",
      "Epoch 126/1000\n",
      " - 12s - loss: 0.1204 - acc: 0.9679 - val_loss: 0.2351 - val_acc: 0.9058\n",
      "Epoch 127/1000\n",
      " - 12s - loss: 0.1198 - acc: 0.9671 - val_loss: 0.2350 - val_acc: 0.9058\n",
      "Epoch 128/1000\n",
      " - 12s - loss: 0.1190 - acc: 0.9680 - val_loss: 0.2350 - val_acc: 0.9055\n",
      "Epoch 129/1000\n",
      " - 12s - loss: 0.1186 - acc: 0.9683 - val_loss: 0.2349 - val_acc: 0.9055\n",
      "Epoch 130/1000\n",
      " - 13s - loss: 0.1178 - acc: 0.9681 - val_loss: 0.2348 - val_acc: 0.9056\n",
      "Epoch 131/1000\n",
      " - 12s - loss: 0.1170 - acc: 0.9679 - val_loss: 0.2347 - val_acc: 0.9054\n",
      "Epoch 132/1000\n",
      " - 14s - loss: 0.1178 - acc: 0.9688 - val_loss: 0.2348 - val_acc: 0.9057\n",
      "Epoch 133/1000\n",
      " - 12s - loss: 0.1176 - acc: 0.9678 - val_loss: 0.2347 - val_acc: 0.9055\n",
      "Validation accuracy: 0.9055199999809265, loss: 0.23472495356559753\n",
      "Accuracy: 0.9055199999809265, Parameters: (layers=1, units=8)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "\n",
    "def tune_ngram_model(data):\n",
    "    \"\"\"Tunes n-gram model on the given dataset.\n",
    "    # Arguments\n",
    "        data: tuples of training and test texts and labels.\n",
    "    \"\"\"\n",
    "    # Select parameter values to try.\n",
    "    num_layers = [1, 2, 3]\n",
    "    num_units = [8, 16, 32, 64, 128]\n",
    "\n",
    "    # Save parameter combination and results.\n",
    "    params = {\n",
    "        'layers': [],\n",
    "        'units': [],\n",
    "        'accuracy': [],\n",
    "    }\n",
    "\n",
    "    # Iterate over all parameter combinations.\n",
    "    for layers in num_layers:\n",
    "        for units in num_units:\n",
    "                params['layers'].append(layers)\n",
    "                params['units'].append(units)\n",
    "\n",
    "                accuracy, _ = train_ngram_model(\n",
    "                    data=data,\n",
    "                    layers=layers,\n",
    "                    units=units)\n",
    "                print(('Accuracy: {accuracy}, Parameters: (layers={layers}, '\n",
    "                       'units={units})').format(accuracy=accuracy,\n",
    "                                                layers=layers,\n",
    "                                                units=units))\n",
    "                params['accuracy'].append(accuracy)\n",
    "    _plot_parameters(params)\n",
    "\n",
    "\n",
    "def _plot_parameters(params):\n",
    "    \"\"\"Creates a 3D surface plot of given parameters.\n",
    "    # Arguments\n",
    "        params: dict, contains layers, units and accuracy value combinations.\n",
    "    \"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.plot_trisurf(params['layers'],\n",
    "                    params['units'],\n",
    "                    params['accuracy'],\n",
    "                    cmap=cm.coolwarm,\n",
    "                    antialiased=False)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_dir', type=str, default='',\n",
    "                        help='input data directory')\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "    # Using the IMDb movie reviews dataset to demonstrate training n-gram model\n",
    "    data = load_imdb_sentiment_analysis_dataset(FLAGS.data_dir)\n",
    "    tune_ngram_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
